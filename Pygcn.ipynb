{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pygcn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPZzDKSPKnwPidCrrwq2hgO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trishac97/pygcn/blob/master/Pygcn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2sD-dY86UnK",
        "outputId": "105d74e6-131a-472b-9b08-4fea189f82aa"
      },
      "source": [
        "!pip install --upgrade tensorflow==1.14.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==1.14.0 in /usr/local/lib/python3.7/dist-packages (1.14.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.1.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.0.8)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.12.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.8.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.19.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.15.0)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.14.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.12.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (3.17.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.37.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.4.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.41.0)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.14.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (3.1.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (4.8.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.14.0) (1.5.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "id": "qXgRcv6g6ZUC",
        "outputId": "defb107a-775d-4762-d9b6-29a5ab16d6b8"
      },
      "source": [
        "import tensorflow\n",
        "tensorflow.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.14.0'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JM7RBxjt6bhd",
        "outputId": "be8789b0-265d-413f-9df7-3c93df4b2532"
      },
      "source": [
        "#Used the implementation of original paper, fixed absolute path\n",
        "!git clone https://github.com/trishac97/pygcn.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pygcn'...\n",
            "remote: Enumerating objects: 98, done.\u001b[K\n",
            "remote: Counting objects: 100% (98/98), done.\u001b[K\n",
            "remote: Compressing objects: 100% (61/61), done.\u001b[K\n",
            "remote: Total 98 (delta 50), reused 74 (delta 35), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (98/98), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYnkY_us6gmC"
      },
      "source": [
        "!rm -rf pygcn/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBeDYODA7K88",
        "outputId": "2ad0eb89-1fe4-476d-a6ee-da449cd29e01"
      },
      "source": [
        "cd pygcn/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pygcn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMei5I7i6rc5",
        "outputId": "7efbf64b-ffd2-4999-b7e4-68a256d68867"
      },
      "source": [
        "#(1.1) Run GCN on the Cora dataset and show your results, including accuracy and runtime. \n",
        "%run pygcn/train.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cora dataset...\n",
            "Epoch: 0001 loss_train: 1.9617 acc_train: 0.1357 loss_val: 1.9663 acc_val: 0.1567 time: 0.2716s\n",
            "Epoch: 0002 loss_train: 1.9537 acc_train: 0.2071 loss_val: 1.9529 acc_val: 0.1567 time: 0.0135s\n",
            "Epoch: 0003 loss_train: 1.9415 acc_train: 0.2000 loss_val: 1.9404 acc_val: 0.1567 time: 0.0135s\n",
            "Epoch: 0004 loss_train: 1.9318 acc_train: 0.2000 loss_val: 1.9284 acc_val: 0.1567 time: 0.0154s\n",
            "Epoch: 0005 loss_train: 1.9173 acc_train: 0.2071 loss_val: 1.9169 acc_val: 0.1567 time: 0.0132s\n",
            "Epoch: 0006 loss_train: 1.8988 acc_train: 0.2071 loss_val: 1.9057 acc_val: 0.1567 time: 0.0229s\n",
            "Epoch: 0007 loss_train: 1.9027 acc_train: 0.2071 loss_val: 1.8953 acc_val: 0.1567 time: 0.0166s\n",
            "Epoch: 0008 loss_train: 1.8906 acc_train: 0.2000 loss_val: 1.8853 acc_val: 0.1567 time: 0.0162s\n",
            "Epoch: 0009 loss_train: 1.8804 acc_train: 0.2071 loss_val: 1.8756 acc_val: 0.1567 time: 0.0191s\n",
            "Epoch: 0010 loss_train: 1.8712 acc_train: 0.2143 loss_val: 1.8660 acc_val: 0.1567 time: 0.0181s\n",
            "Epoch: 0011 loss_train: 1.8524 acc_train: 0.2571 loss_val: 1.8563 acc_val: 0.1567 time: 0.0178s\n",
            "Epoch: 0012 loss_train: 1.8474 acc_train: 0.2643 loss_val: 1.8468 acc_val: 0.1633 time: 0.0218s\n",
            "Epoch: 0013 loss_train: 1.8542 acc_train: 0.2714 loss_val: 1.8374 acc_val: 0.3333 time: 0.0254s\n",
            "Epoch: 0014 loss_train: 1.8434 acc_train: 0.2357 loss_val: 1.8285 acc_val: 0.4467 time: 0.0191s\n",
            "Epoch: 0015 loss_train: 1.8222 acc_train: 0.3357 loss_val: 1.8198 acc_val: 0.4500 time: 0.0181s\n",
            "Epoch: 0016 loss_train: 1.8142 acc_train: 0.3357 loss_val: 1.8116 acc_val: 0.4367 time: 0.0195s\n",
            "Epoch: 0017 loss_train: 1.8063 acc_train: 0.3571 loss_val: 1.8036 acc_val: 0.3900 time: 0.0181s\n",
            "Epoch: 0018 loss_train: 1.8113 acc_train: 0.3214 loss_val: 1.7960 acc_val: 0.3700 time: 0.0180s\n",
            "Epoch: 0019 loss_train: 1.8047 acc_train: 0.2500 loss_val: 1.7885 acc_val: 0.3700 time: 0.0204s\n",
            "Epoch: 0020 loss_train: 1.7686 acc_train: 0.3286 loss_val: 1.7810 acc_val: 0.3600 time: 0.0177s\n",
            "Epoch: 0021 loss_train: 1.7858 acc_train: 0.3571 loss_val: 1.7734 acc_val: 0.3533 time: 0.0177s\n",
            "Epoch: 0022 loss_train: 1.7782 acc_train: 0.3714 loss_val: 1.7657 acc_val: 0.3533 time: 0.0173s\n",
            "Epoch: 0023 loss_train: 1.7512 acc_train: 0.3357 loss_val: 1.7579 acc_val: 0.3500 time: 0.0208s\n",
            "Epoch: 0024 loss_train: 1.7479 acc_train: 0.3214 loss_val: 1.7504 acc_val: 0.3500 time: 0.0194s\n",
            "Epoch: 0025 loss_train: 1.7712 acc_train: 0.3143 loss_val: 1.7434 acc_val: 0.3500 time: 0.0177s\n",
            "Epoch: 0026 loss_train: 1.7116 acc_train: 0.3286 loss_val: 1.7365 acc_val: 0.3500 time: 0.0176s\n",
            "Epoch: 0027 loss_train: 1.7173 acc_train: 0.3429 loss_val: 1.7296 acc_val: 0.3500 time: 0.0174s\n",
            "Epoch: 0028 loss_train: 1.7142 acc_train: 0.3357 loss_val: 1.7226 acc_val: 0.3500 time: 0.0194s\n",
            "Epoch: 0029 loss_train: 1.6769 acc_train: 0.3429 loss_val: 1.7156 acc_val: 0.3500 time: 0.0190s\n",
            "Epoch: 0030 loss_train: 1.7048 acc_train: 0.3214 loss_val: 1.7085 acc_val: 0.3500 time: 0.0178s\n",
            "Epoch: 0031 loss_train: 1.6803 acc_train: 0.3429 loss_val: 1.7014 acc_val: 0.3500 time: 0.0169s\n",
            "Epoch: 0032 loss_train: 1.6790 acc_train: 0.3357 loss_val: 1.6940 acc_val: 0.3500 time: 0.0163s\n",
            "Epoch: 0033 loss_train: 1.6664 acc_train: 0.3500 loss_val: 1.6865 acc_val: 0.3467 time: 0.0173s\n",
            "Epoch: 0034 loss_train: 1.6514 acc_train: 0.3643 loss_val: 1.6788 acc_val: 0.3500 time: 0.0207s\n",
            "Epoch: 0035 loss_train: 1.6313 acc_train: 0.3714 loss_val: 1.6710 acc_val: 0.3533 time: 0.0200s\n",
            "Epoch: 0036 loss_train: 1.6574 acc_train: 0.3143 loss_val: 1.6631 acc_val: 0.3600 time: 0.0174s\n",
            "Epoch: 0037 loss_train: 1.6305 acc_train: 0.3786 loss_val: 1.6550 acc_val: 0.3600 time: 0.0185s\n",
            "Epoch: 0038 loss_train: 1.5984 acc_train: 0.4071 loss_val: 1.6467 acc_val: 0.3600 time: 0.0174s\n",
            "Epoch: 0039 loss_train: 1.6229 acc_train: 0.3429 loss_val: 1.6383 acc_val: 0.3667 time: 0.0183s\n",
            "Epoch: 0040 loss_train: 1.5987 acc_train: 0.3714 loss_val: 1.6299 acc_val: 0.3800 time: 0.0170s\n",
            "Epoch: 0041 loss_train: 1.5719 acc_train: 0.4571 loss_val: 1.6211 acc_val: 0.3967 time: 0.0275s\n",
            "Epoch: 0042 loss_train: 1.5693 acc_train: 0.4357 loss_val: 1.6121 acc_val: 0.4033 time: 0.0154s\n",
            "Epoch: 0043 loss_train: 1.5456 acc_train: 0.4500 loss_val: 1.6028 acc_val: 0.4500 time: 0.0169s\n",
            "Epoch: 0044 loss_train: 1.5201 acc_train: 0.4714 loss_val: 1.5930 acc_val: 0.4733 time: 0.0159s\n",
            "Epoch: 0045 loss_train: 1.4953 acc_train: 0.5286 loss_val: 1.5829 acc_val: 0.4900 time: 0.0209s\n",
            "Epoch: 0046 loss_train: 1.5339 acc_train: 0.4643 loss_val: 1.5722 acc_val: 0.5000 time: 0.0194s\n",
            "Epoch: 0047 loss_train: 1.4843 acc_train: 0.5000 loss_val: 1.5610 acc_val: 0.5133 time: 0.0180s\n",
            "Epoch: 0048 loss_train: 1.4708 acc_train: 0.5357 loss_val: 1.5490 acc_val: 0.5267 time: 0.0182s\n",
            "Epoch: 0049 loss_train: 1.4502 acc_train: 0.5143 loss_val: 1.5366 acc_val: 0.5333 time: 0.0181s\n",
            "Epoch: 0050 loss_train: 1.4185 acc_train: 0.5286 loss_val: 1.5236 acc_val: 0.5400 time: 0.0195s\n",
            "Epoch: 0051 loss_train: 1.3960 acc_train: 0.5357 loss_val: 1.5101 acc_val: 0.5400 time: 0.0197s\n",
            "Epoch: 0052 loss_train: 1.3751 acc_train: 0.5857 loss_val: 1.4967 acc_val: 0.5400 time: 0.0246s\n",
            "Epoch: 0053 loss_train: 1.4009 acc_train: 0.5571 loss_val: 1.4832 acc_val: 0.5333 time: 0.0171s\n",
            "Epoch: 0054 loss_train: 1.3755 acc_train: 0.5571 loss_val: 1.4697 acc_val: 0.5400 time: 0.0185s\n",
            "Epoch: 0055 loss_train: 1.3339 acc_train: 0.5429 loss_val: 1.4565 acc_val: 0.5467 time: 0.0177s\n",
            "Epoch: 0056 loss_train: 1.3099 acc_train: 0.6071 loss_val: 1.4435 acc_val: 0.5500 time: 0.0213s\n",
            "Epoch: 0057 loss_train: 1.3421 acc_train: 0.5857 loss_val: 1.4307 acc_val: 0.5500 time: 0.0159s\n",
            "Epoch: 0058 loss_train: 1.2929 acc_train: 0.6357 loss_val: 1.4181 acc_val: 0.5667 time: 0.0204s\n",
            "Epoch: 0059 loss_train: 1.2640 acc_train: 0.6357 loss_val: 1.4055 acc_val: 0.5967 time: 0.0168s\n",
            "Epoch: 0060 loss_train: 1.2654 acc_train: 0.6143 loss_val: 1.3930 acc_val: 0.6067 time: 0.0186s\n",
            "Epoch: 0061 loss_train: 1.2500 acc_train: 0.6286 loss_val: 1.3806 acc_val: 0.6300 time: 0.0173s\n",
            "Epoch: 0062 loss_train: 1.2292 acc_train: 0.6500 loss_val: 1.3679 acc_val: 0.6300 time: 0.0175s\n",
            "Epoch: 0063 loss_train: 1.1792 acc_train: 0.6714 loss_val: 1.3552 acc_val: 0.6367 time: 0.0179s\n",
            "Epoch: 0064 loss_train: 1.2206 acc_train: 0.6714 loss_val: 1.3423 acc_val: 0.6433 time: 0.0152s\n",
            "Epoch: 0065 loss_train: 1.2054 acc_train: 0.6643 loss_val: 1.3294 acc_val: 0.6500 time: 0.0212s\n",
            "Epoch: 0066 loss_train: 1.1787 acc_train: 0.7000 loss_val: 1.3163 acc_val: 0.6467 time: 0.0191s\n",
            "Epoch: 0067 loss_train: 1.1513 acc_train: 0.7214 loss_val: 1.3033 acc_val: 0.6467 time: 0.0256s\n",
            "Epoch: 0068 loss_train: 1.0915 acc_train: 0.7643 loss_val: 1.2909 acc_val: 0.6600 time: 0.0191s\n",
            "Epoch: 0069 loss_train: 1.1464 acc_train: 0.6857 loss_val: 1.2789 acc_val: 0.6700 time: 0.0209s\n",
            "Epoch: 0070 loss_train: 1.0994 acc_train: 0.7071 loss_val: 1.2678 acc_val: 0.6733 time: 0.0194s\n",
            "Epoch: 0071 loss_train: 1.0747 acc_train: 0.6929 loss_val: 1.2579 acc_val: 0.6833 time: 0.0190s\n",
            "Epoch: 0072 loss_train: 1.0920 acc_train: 0.7286 loss_val: 1.2477 acc_val: 0.6967 time: 0.0370s\n",
            "Epoch: 0073 loss_train: 1.0659 acc_train: 0.7286 loss_val: 1.2374 acc_val: 0.7000 time: 0.0194s\n",
            "Epoch: 0074 loss_train: 1.0875 acc_train: 0.7071 loss_val: 1.2264 acc_val: 0.7133 time: 0.0158s\n",
            "Epoch: 0075 loss_train: 1.0447 acc_train: 0.7071 loss_val: 1.2159 acc_val: 0.7167 time: 0.0156s\n",
            "Epoch: 0076 loss_train: 1.0758 acc_train: 0.7357 loss_val: 1.2049 acc_val: 0.7167 time: 0.0193s\n",
            "Epoch: 0077 loss_train: 1.0380 acc_train: 0.7500 loss_val: 1.1948 acc_val: 0.7133 time: 0.0236s\n",
            "Epoch: 0078 loss_train: 1.0167 acc_train: 0.7429 loss_val: 1.1851 acc_val: 0.7133 time: 0.0167s\n",
            "Epoch: 0079 loss_train: 1.0261 acc_train: 0.7357 loss_val: 1.1764 acc_val: 0.7167 time: 0.0165s\n",
            "Epoch: 0080 loss_train: 0.9883 acc_train: 0.7714 loss_val: 1.1680 acc_val: 0.7167 time: 0.0157s\n",
            "Epoch: 0081 loss_train: 1.0101 acc_train: 0.7429 loss_val: 1.1599 acc_val: 0.7167 time: 0.0162s\n",
            "Epoch: 0082 loss_train: 0.9598 acc_train: 0.7929 loss_val: 1.1520 acc_val: 0.7167 time: 0.0161s\n",
            "Epoch: 0083 loss_train: 0.9683 acc_train: 0.7714 loss_val: 1.1440 acc_val: 0.7167 time: 0.0170s\n",
            "Epoch: 0084 loss_train: 0.9676 acc_train: 0.7500 loss_val: 1.1356 acc_val: 0.7200 time: 0.0148s\n",
            "Epoch: 0085 loss_train: 0.9448 acc_train: 0.7429 loss_val: 1.1272 acc_val: 0.7200 time: 0.0148s\n",
            "Epoch: 0086 loss_train: 0.9735 acc_train: 0.7500 loss_val: 1.1183 acc_val: 0.7200 time: 0.0143s\n",
            "Epoch: 0087 loss_train: 0.9703 acc_train: 0.7571 loss_val: 1.1102 acc_val: 0.7233 time: 0.0154s\n",
            "Epoch: 0088 loss_train: 0.9279 acc_train: 0.7714 loss_val: 1.1025 acc_val: 0.7233 time: 0.0155s\n",
            "Epoch: 0089 loss_train: 0.9041 acc_train: 0.7857 loss_val: 1.0953 acc_val: 0.7167 time: 0.0211s\n",
            "Epoch: 0090 loss_train: 0.8855 acc_train: 0.8000 loss_val: 1.0886 acc_val: 0.7167 time: 0.0162s\n",
            "Epoch: 0091 loss_train: 0.9162 acc_train: 0.7714 loss_val: 1.0819 acc_val: 0.7233 time: 0.0187s\n",
            "Epoch: 0092 loss_train: 0.8559 acc_train: 0.7786 loss_val: 1.0761 acc_val: 0.7267 time: 0.0211s\n",
            "Epoch: 0093 loss_train: 0.8733 acc_train: 0.7786 loss_val: 1.0689 acc_val: 0.7233 time: 0.0151s\n",
            "Epoch: 0094 loss_train: 0.8657 acc_train: 0.8000 loss_val: 1.0619 acc_val: 0.7267 time: 0.0156s\n",
            "Epoch: 0095 loss_train: 0.8895 acc_train: 0.7643 loss_val: 1.0547 acc_val: 0.7267 time: 0.0132s\n",
            "Epoch: 0096 loss_train: 0.9172 acc_train: 0.7643 loss_val: 1.0479 acc_val: 0.7267 time: 0.0144s\n",
            "Epoch: 0097 loss_train: 0.8164 acc_train: 0.7929 loss_val: 1.0406 acc_val: 0.7267 time: 0.0128s\n",
            "Epoch: 0098 loss_train: 0.8833 acc_train: 0.7714 loss_val: 1.0331 acc_val: 0.7267 time: 0.0194s\n",
            "Epoch: 0099 loss_train: 0.8761 acc_train: 0.7571 loss_val: 1.0264 acc_val: 0.7267 time: 0.0162s\n",
            "Epoch: 0100 loss_train: 0.8743 acc_train: 0.7714 loss_val: 1.0198 acc_val: 0.7267 time: 0.0154s\n",
            "Epoch: 0101 loss_train: 0.8003 acc_train: 0.8143 loss_val: 1.0145 acc_val: 0.7267 time: 0.0207s\n",
            "Epoch: 0102 loss_train: 0.8014 acc_train: 0.7714 loss_val: 1.0105 acc_val: 0.7267 time: 0.0201s\n",
            "Epoch: 0103 loss_train: 0.7712 acc_train: 0.7571 loss_val: 1.0079 acc_val: 0.7300 time: 0.0194s\n",
            "Epoch: 0104 loss_train: 0.7791 acc_train: 0.7929 loss_val: 1.0061 acc_val: 0.7167 time: 0.0174s\n",
            "Epoch: 0105 loss_train: 0.7647 acc_train: 0.8214 loss_val: 1.0039 acc_val: 0.7200 time: 0.0180s\n",
            "Epoch: 0106 loss_train: 0.7424 acc_train: 0.8357 loss_val: 0.9998 acc_val: 0.7233 time: 0.0199s\n",
            "Epoch: 0107 loss_train: 0.7823 acc_train: 0.8000 loss_val: 0.9932 acc_val: 0.7300 time: 0.0172s\n",
            "Epoch: 0108 loss_train: 0.7797 acc_train: 0.7929 loss_val: 0.9841 acc_val: 0.7300 time: 0.0187s\n",
            "Epoch: 0109 loss_train: 0.7745 acc_train: 0.8143 loss_val: 0.9758 acc_val: 0.7400 time: 0.0174s\n",
            "Epoch: 0110 loss_train: 0.7398 acc_train: 0.8143 loss_val: 0.9695 acc_val: 0.7467 time: 0.0186s\n",
            "Epoch: 0111 loss_train: 0.7673 acc_train: 0.8071 loss_val: 0.9634 acc_val: 0.7500 time: 0.0167s\n",
            "Epoch: 0112 loss_train: 0.7457 acc_train: 0.8214 loss_val: 0.9577 acc_val: 0.7533 time: 0.0226s\n",
            "Epoch: 0113 loss_train: 0.7384 acc_train: 0.8143 loss_val: 0.9526 acc_val: 0.7533 time: 0.0178s\n",
            "Epoch: 0114 loss_train: 0.7426 acc_train: 0.8286 loss_val: 0.9485 acc_val: 0.7567 time: 0.0166s\n",
            "Epoch: 0115 loss_train: 0.7734 acc_train: 0.8071 loss_val: 0.9444 acc_val: 0.7667 time: 0.0166s\n",
            "Epoch: 0116 loss_train: 0.7489 acc_train: 0.8357 loss_val: 0.9394 acc_val: 0.7700 time: 0.0184s\n",
            "Epoch: 0117 loss_train: 0.7321 acc_train: 0.8143 loss_val: 0.9352 acc_val: 0.7633 time: 0.0168s\n",
            "Epoch: 0118 loss_train: 0.6886 acc_train: 0.8643 loss_val: 0.9307 acc_val: 0.7633 time: 0.0198s\n",
            "Epoch: 0119 loss_train: 0.7095 acc_train: 0.8500 loss_val: 0.9261 acc_val: 0.7667 time: 0.0173s\n",
            "Epoch: 0120 loss_train: 0.6926 acc_train: 0.8500 loss_val: 0.9212 acc_val: 0.7633 time: 0.0193s\n",
            "Epoch: 0121 loss_train: 0.7281 acc_train: 0.8357 loss_val: 0.9155 acc_val: 0.7733 time: 0.0174s\n",
            "Epoch: 0122 loss_train: 0.7259 acc_train: 0.8071 loss_val: 0.9102 acc_val: 0.7700 time: 0.0184s\n",
            "Epoch: 0123 loss_train: 0.6946 acc_train: 0.8571 loss_val: 0.9056 acc_val: 0.7667 time: 0.0225s\n",
            "Epoch: 0124 loss_train: 0.6763 acc_train: 0.8214 loss_val: 0.9015 acc_val: 0.7700 time: 0.0204s\n",
            "Epoch: 0125 loss_train: 0.7130 acc_train: 0.8429 loss_val: 0.8979 acc_val: 0.7700 time: 0.0197s\n",
            "Epoch: 0126 loss_train: 0.6945 acc_train: 0.8214 loss_val: 0.8944 acc_val: 0.7767 time: 0.0193s\n",
            "Epoch: 0127 loss_train: 0.6557 acc_train: 0.8571 loss_val: 0.8912 acc_val: 0.7800 time: 0.0195s\n",
            "Epoch: 0128 loss_train: 0.7272 acc_train: 0.8000 loss_val: 0.8879 acc_val: 0.7800 time: 0.0192s\n",
            "Epoch: 0129 loss_train: 0.6417 acc_train: 0.8500 loss_val: 0.8852 acc_val: 0.7833 time: 0.0214s\n",
            "Epoch: 0130 loss_train: 0.6553 acc_train: 0.8571 loss_val: 0.8835 acc_val: 0.7800 time: 0.0183s\n",
            "Epoch: 0131 loss_train: 0.6164 acc_train: 0.8786 loss_val: 0.8821 acc_val: 0.7800 time: 0.0181s\n",
            "Epoch: 0132 loss_train: 0.6756 acc_train: 0.8429 loss_val: 0.8796 acc_val: 0.7767 time: 0.0171s\n",
            "Epoch: 0133 loss_train: 0.6487 acc_train: 0.8571 loss_val: 0.8776 acc_val: 0.7733 time: 0.0165s\n",
            "Epoch: 0134 loss_train: 0.6411 acc_train: 0.8429 loss_val: 0.8746 acc_val: 0.7733 time: 0.0207s\n",
            "Epoch: 0135 loss_train: 0.6484 acc_train: 0.8500 loss_val: 0.8698 acc_val: 0.7767 time: 0.0163s\n",
            "Epoch: 0136 loss_train: 0.6313 acc_train: 0.9000 loss_val: 0.8645 acc_val: 0.7867 time: 0.0173s\n",
            "Epoch: 0137 loss_train: 0.6153 acc_train: 0.8714 loss_val: 0.8589 acc_val: 0.7867 time: 0.0180s\n",
            "Epoch: 0138 loss_train: 0.5918 acc_train: 0.8571 loss_val: 0.8534 acc_val: 0.7967 time: 0.0174s\n",
            "Epoch: 0139 loss_train: 0.6600 acc_train: 0.8143 loss_val: 0.8482 acc_val: 0.8000 time: 0.0173s\n",
            "Epoch: 0140 loss_train: 0.6463 acc_train: 0.8429 loss_val: 0.8443 acc_val: 0.8000 time: 0.0196s\n",
            "Epoch: 0141 loss_train: 0.6254 acc_train: 0.8786 loss_val: 0.8404 acc_val: 0.8000 time: 0.0180s\n",
            "Epoch: 0142 loss_train: 0.5777 acc_train: 0.8786 loss_val: 0.8367 acc_val: 0.8000 time: 0.0179s\n",
            "Epoch: 0143 loss_train: 0.6114 acc_train: 0.8571 loss_val: 0.8334 acc_val: 0.8033 time: 0.0170s\n",
            "Epoch: 0144 loss_train: 0.5882 acc_train: 0.8571 loss_val: 0.8299 acc_val: 0.8033 time: 0.0230s\n",
            "Epoch: 0145 loss_train: 0.5925 acc_train: 0.8643 loss_val: 0.8270 acc_val: 0.8000 time: 0.0229s\n",
            "Epoch: 0146 loss_train: 0.5760 acc_train: 0.8786 loss_val: 0.8255 acc_val: 0.7933 time: 0.0172s\n",
            "Epoch: 0147 loss_train: 0.5675 acc_train: 0.8714 loss_val: 0.8250 acc_val: 0.7900 time: 0.0175s\n",
            "Epoch: 0148 loss_train: 0.5844 acc_train: 0.8786 loss_val: 0.8217 acc_val: 0.7900 time: 0.0176s\n",
            "Epoch: 0149 loss_train: 0.5460 acc_train: 0.8929 loss_val: 0.8169 acc_val: 0.7900 time: 0.0179s\n",
            "Epoch: 0150 loss_train: 0.5412 acc_train: 0.9000 loss_val: 0.8116 acc_val: 0.7900 time: 0.0181s\n",
            "Epoch: 0151 loss_train: 0.5830 acc_train: 0.8571 loss_val: 0.8065 acc_val: 0.7933 time: 0.0190s\n",
            "Epoch: 0152 loss_train: 0.5726 acc_train: 0.8643 loss_val: 0.8029 acc_val: 0.7900 time: 0.0161s\n",
            "Epoch: 0153 loss_train: 0.5248 acc_train: 0.9000 loss_val: 0.7992 acc_val: 0.8000 time: 0.0158s\n",
            "Epoch: 0154 loss_train: 0.5291 acc_train: 0.8929 loss_val: 0.7945 acc_val: 0.8067 time: 0.0180s\n",
            "Epoch: 0155 loss_train: 0.5646 acc_train: 0.8786 loss_val: 0.7896 acc_val: 0.8067 time: 0.0172s\n",
            "Epoch: 0156 loss_train: 0.5416 acc_train: 0.8857 loss_val: 0.7863 acc_val: 0.8167 time: 0.0191s\n",
            "Epoch: 0157 loss_train: 0.5378 acc_train: 0.8643 loss_val: 0.7837 acc_val: 0.8167 time: 0.0231s\n",
            "Epoch: 0158 loss_train: 0.5083 acc_train: 0.9000 loss_val: 0.7812 acc_val: 0.8100 time: 0.0162s\n",
            "Epoch: 0159 loss_train: 0.5433 acc_train: 0.8786 loss_val: 0.7798 acc_val: 0.8000 time: 0.0180s\n",
            "Epoch: 0160 loss_train: 0.5454 acc_train: 0.8857 loss_val: 0.7781 acc_val: 0.7967 time: 0.0254s\n",
            "Epoch: 0161 loss_train: 0.4936 acc_train: 0.9214 loss_val: 0.7753 acc_val: 0.8033 time: 0.0206s\n",
            "Epoch: 0162 loss_train: 0.4894 acc_train: 0.9000 loss_val: 0.7703 acc_val: 0.8033 time: 0.0184s\n",
            "Epoch: 0163 loss_train: 0.5048 acc_train: 0.9214 loss_val: 0.7645 acc_val: 0.8133 time: 0.0219s\n",
            "Epoch: 0164 loss_train: 0.4906 acc_train: 0.8857 loss_val: 0.7596 acc_val: 0.8133 time: 0.0198s\n",
            "Epoch: 0165 loss_train: 0.4567 acc_train: 0.9071 loss_val: 0.7552 acc_val: 0.8167 time: 0.0202s\n",
            "Epoch: 0166 loss_train: 0.5164 acc_train: 0.8929 loss_val: 0.7515 acc_val: 0.8167 time: 0.0211s\n",
            "Epoch: 0167 loss_train: 0.4589 acc_train: 0.9214 loss_val: 0.7486 acc_val: 0.8133 time: 0.0192s\n",
            "Epoch: 0168 loss_train: 0.4849 acc_train: 0.9000 loss_val: 0.7468 acc_val: 0.8033 time: 0.0180s\n",
            "Epoch: 0169 loss_train: 0.5793 acc_train: 0.8786 loss_val: 0.7445 acc_val: 0.8033 time: 0.0179s\n",
            "Epoch: 0170 loss_train: 0.5238 acc_train: 0.8929 loss_val: 0.7424 acc_val: 0.8100 time: 0.0196s\n",
            "Epoch: 0171 loss_train: 0.5154 acc_train: 0.8857 loss_val: 0.7403 acc_val: 0.8100 time: 0.0219s\n",
            "Epoch: 0172 loss_train: 0.4843 acc_train: 0.8786 loss_val: 0.7365 acc_val: 0.8200 time: 0.0179s\n",
            "Epoch: 0173 loss_train: 0.4677 acc_train: 0.9071 loss_val: 0.7337 acc_val: 0.8300 time: 0.0175s\n",
            "Epoch: 0174 loss_train: 0.5622 acc_train: 0.8643 loss_val: 0.7315 acc_val: 0.8333 time: 0.0182s\n",
            "Epoch: 0175 loss_train: 0.4413 acc_train: 0.9214 loss_val: 0.7298 acc_val: 0.8367 time: 0.0175s\n",
            "Epoch: 0176 loss_train: 0.4859 acc_train: 0.9143 loss_val: 0.7301 acc_val: 0.8267 time: 0.0193s\n",
            "Epoch: 0177 loss_train: 0.4792 acc_train: 0.9286 loss_val: 0.7289 acc_val: 0.8167 time: 0.0235s\n",
            "Epoch: 0178 loss_train: 0.4575 acc_train: 0.9214 loss_val: 0.7268 acc_val: 0.8100 time: 0.0191s\n",
            "Epoch: 0179 loss_train: 0.4911 acc_train: 0.9143 loss_val: 0.7250 acc_val: 0.8067 time: 0.0180s\n",
            "Epoch: 0180 loss_train: 0.4368 acc_train: 0.9214 loss_val: 0.7246 acc_val: 0.8067 time: 0.0175s\n",
            "Epoch: 0181 loss_train: 0.4144 acc_train: 0.9500 loss_val: 0.7225 acc_val: 0.8100 time: 0.0167s\n",
            "Epoch: 0182 loss_train: 0.4800 acc_train: 0.9000 loss_val: 0.7205 acc_val: 0.8100 time: 0.0188s\n",
            "Epoch: 0183 loss_train: 0.4482 acc_train: 0.9286 loss_val: 0.7190 acc_val: 0.8100 time: 0.0179s\n",
            "Epoch: 0184 loss_train: 0.4570 acc_train: 0.9071 loss_val: 0.7168 acc_val: 0.8100 time: 0.0171s\n",
            "Epoch: 0185 loss_train: 0.4356 acc_train: 0.9214 loss_val: 0.7159 acc_val: 0.8067 time: 0.0226s\n",
            "Epoch: 0186 loss_train: 0.4653 acc_train: 0.9214 loss_val: 0.7141 acc_val: 0.8067 time: 0.0216s\n",
            "Epoch: 0187 loss_train: 0.4685 acc_train: 0.9143 loss_val: 0.7100 acc_val: 0.8200 time: 0.0176s\n",
            "Epoch: 0188 loss_train: 0.4347 acc_train: 0.9214 loss_val: 0.7075 acc_val: 0.8200 time: 0.0226s\n",
            "Epoch: 0189 loss_train: 0.4444 acc_train: 0.8857 loss_val: 0.7057 acc_val: 0.8167 time: 0.0195s\n",
            "Epoch: 0190 loss_train: 0.4283 acc_train: 0.9214 loss_val: 0.7055 acc_val: 0.8100 time: 0.0186s\n",
            "Epoch: 0191 loss_train: 0.4346 acc_train: 0.9286 loss_val: 0.7050 acc_val: 0.8100 time: 0.0178s\n",
            "Epoch: 0192 loss_train: 0.4244 acc_train: 0.9357 loss_val: 0.7039 acc_val: 0.8100 time: 0.0173s\n",
            "Epoch: 0193 loss_train: 0.3940 acc_train: 0.9071 loss_val: 0.7037 acc_val: 0.8133 time: 0.0180s\n",
            "Epoch: 0194 loss_train: 0.4053 acc_train: 0.9286 loss_val: 0.7033 acc_val: 0.8233 time: 0.0165s\n",
            "Epoch: 0195 loss_train: 0.4380 acc_train: 0.9214 loss_val: 0.7031 acc_val: 0.8300 time: 0.0240s\n",
            "Epoch: 0196 loss_train: 0.3790 acc_train: 0.9571 loss_val: 0.7044 acc_val: 0.8200 time: 0.0183s\n",
            "Epoch: 0197 loss_train: 0.4055 acc_train: 0.9643 loss_val: 0.7052 acc_val: 0.8133 time: 0.0182s\n",
            "Epoch: 0198 loss_train: 0.4083 acc_train: 0.9071 loss_val: 0.7060 acc_val: 0.8133 time: 0.0186s\n",
            "Epoch: 0199 loss_train: 0.4292 acc_train: 0.9286 loss_val: 0.7043 acc_val: 0.8167 time: 0.0232s\n",
            "Epoch: 0200 loss_train: 0.4698 acc_train: 0.9000 loss_val: 0.7038 acc_val: 0.8200 time: 0.0198s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 4.2000s\n",
            "Test set results: loss= 0.7319 accuracy= 0.8340\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJXoZtmW7Nz5",
        "outputId": "cb91679c-b254-495d-8133-69f3ed29573f"
      },
      "source": [
        "#(1.2) Replace the normalization with symmetric normalization and redo (1)\n",
        "# Add adj = normalize_adj(adj) line 40 utils.py\n",
        "\n",
        "%run pygcn/train.py\n",
        "\n",
        "#Observation: Accuracy decreases from (1.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cora dataset...\n",
            "Epoch: 0001 loss_train: 1.8873 acc_train: 0.2714 loss_val: 1.8789 acc_val: 0.3433 time: 0.0202s\n",
            "Epoch: 0002 loss_train: 1.8811 acc_train: 0.3143 loss_val: 1.8707 acc_val: 0.3467 time: 0.0144s\n",
            "Epoch: 0003 loss_train: 1.8653 acc_train: 0.2929 loss_val: 1.8634 acc_val: 0.3467 time: 0.0146s\n",
            "Epoch: 0004 loss_train: 1.8508 acc_train: 0.3000 loss_val: 1.8567 acc_val: 0.3500 time: 0.0175s\n",
            "Epoch: 0005 loss_train: 1.8427 acc_train: 0.3000 loss_val: 1.8504 acc_val: 0.3500 time: 0.0170s\n",
            "Epoch: 0006 loss_train: 1.8412 acc_train: 0.3000 loss_val: 1.8446 acc_val: 0.3533 time: 0.0179s\n",
            "Epoch: 0007 loss_train: 1.8256 acc_train: 0.3214 loss_val: 1.8395 acc_val: 0.3567 time: 0.0185s\n",
            "Epoch: 0008 loss_train: 1.8215 acc_train: 0.3571 loss_val: 1.8348 acc_val: 0.3567 time: 0.0165s\n",
            "Epoch: 0009 loss_train: 1.8101 acc_train: 0.3571 loss_val: 1.8305 acc_val: 0.3600 time: 0.0146s\n",
            "Epoch: 0010 loss_train: 1.8078 acc_train: 0.3143 loss_val: 1.8261 acc_val: 0.3633 time: 0.0160s\n",
            "Epoch: 0011 loss_train: 1.7970 acc_train: 0.3571 loss_val: 1.8218 acc_val: 0.3500 time: 0.0186s\n",
            "Epoch: 0012 loss_train: 1.7919 acc_train: 0.3500 loss_val: 1.8174 acc_val: 0.3533 time: 0.0170s\n",
            "Epoch: 0013 loss_train: 1.7961 acc_train: 0.3071 loss_val: 1.8130 acc_val: 0.3533 time: 0.0230s\n",
            "Epoch: 0014 loss_train: 1.7688 acc_train: 0.3286 loss_val: 1.8082 acc_val: 0.3533 time: 0.0182s\n",
            "Epoch: 0015 loss_train: 1.7800 acc_train: 0.3357 loss_val: 1.8033 acc_val: 0.3533 time: 0.0172s\n",
            "Epoch: 0016 loss_train: 1.7725 acc_train: 0.3357 loss_val: 1.7983 acc_val: 0.3533 time: 0.0155s\n",
            "Epoch: 0017 loss_train: 1.7664 acc_train: 0.3214 loss_val: 1.7927 acc_val: 0.3500 time: 0.0160s\n",
            "Epoch: 0018 loss_train: 1.7400 acc_train: 0.3571 loss_val: 1.7871 acc_val: 0.3533 time: 0.0163s\n",
            "Epoch: 0019 loss_train: 1.7365 acc_train: 0.3357 loss_val: 1.7817 acc_val: 0.3533 time: 0.0160s\n",
            "Epoch: 0020 loss_train: 1.7304 acc_train: 0.3643 loss_val: 1.7761 acc_val: 0.3533 time: 0.0178s\n",
            "Epoch: 0021 loss_train: 1.7295 acc_train: 0.3357 loss_val: 1.7701 acc_val: 0.3600 time: 0.0170s\n",
            "Epoch: 0022 loss_train: 1.7227 acc_train: 0.3571 loss_val: 1.7639 acc_val: 0.3600 time: 0.0184s\n",
            "Epoch: 0023 loss_train: 1.7094 acc_train: 0.3643 loss_val: 1.7577 acc_val: 0.3667 time: 0.0180s\n",
            "Epoch: 0024 loss_train: 1.7075 acc_train: 0.3571 loss_val: 1.7512 acc_val: 0.3733 time: 0.0250s\n",
            "Epoch: 0025 loss_train: 1.6916 acc_train: 0.3857 loss_val: 1.7447 acc_val: 0.3767 time: 0.0221s\n",
            "Epoch: 0026 loss_train: 1.6771 acc_train: 0.3714 loss_val: 1.7379 acc_val: 0.3800 time: 0.0193s\n",
            "Epoch: 0027 loss_train: 1.6799 acc_train: 0.3643 loss_val: 1.7307 acc_val: 0.3833 time: 0.0164s\n",
            "Epoch: 0028 loss_train: 1.6605 acc_train: 0.3857 loss_val: 1.7228 acc_val: 0.3833 time: 0.0176s\n",
            "Epoch: 0029 loss_train: 1.6585 acc_train: 0.3643 loss_val: 1.7148 acc_val: 0.3900 time: 0.0189s\n",
            "Epoch: 0030 loss_train: 1.6522 acc_train: 0.3857 loss_val: 1.7064 acc_val: 0.3933 time: 0.0158s\n",
            "Epoch: 0031 loss_train: 1.6092 acc_train: 0.4286 loss_val: 1.6975 acc_val: 0.3933 time: 0.0206s\n",
            "Epoch: 0032 loss_train: 1.6005 acc_train: 0.4214 loss_val: 1.6884 acc_val: 0.3933 time: 0.0175s\n",
            "Epoch: 0033 loss_train: 1.5862 acc_train: 0.4214 loss_val: 1.6789 acc_val: 0.3967 time: 0.0152s\n",
            "Epoch: 0034 loss_train: 1.5844 acc_train: 0.4000 loss_val: 1.6688 acc_val: 0.3967 time: 0.0158s\n",
            "Epoch: 0035 loss_train: 1.5621 acc_train: 0.4643 loss_val: 1.6585 acc_val: 0.3967 time: 0.0172s\n",
            "Epoch: 0036 loss_train: 1.5626 acc_train: 0.4286 loss_val: 1.6482 acc_val: 0.3967 time: 0.0218s\n",
            "Epoch: 0037 loss_train: 1.5217 acc_train: 0.4286 loss_val: 1.6377 acc_val: 0.4000 time: 0.0167s\n",
            "Epoch: 0038 loss_train: 1.5425 acc_train: 0.4214 loss_val: 1.6272 acc_val: 0.4067 time: 0.0148s\n",
            "Epoch: 0039 loss_train: 1.5116 acc_train: 0.4643 loss_val: 1.6168 acc_val: 0.4233 time: 0.0235s\n",
            "Epoch: 0040 loss_train: 1.4912 acc_train: 0.5214 loss_val: 1.6057 acc_val: 0.4300 time: 0.0180s\n",
            "Epoch: 0041 loss_train: 1.4965 acc_train: 0.4714 loss_val: 1.5940 acc_val: 0.4400 time: 0.0237s\n",
            "Epoch: 0042 loss_train: 1.4798 acc_train: 0.5429 loss_val: 1.5822 acc_val: 0.4400 time: 0.0178s\n",
            "Epoch: 0043 loss_train: 1.4569 acc_train: 0.4786 loss_val: 1.5704 acc_val: 0.4467 time: 0.0175s\n",
            "Epoch: 0044 loss_train: 1.4382 acc_train: 0.5286 loss_val: 1.5582 acc_val: 0.4500 time: 0.0188s\n",
            "Epoch: 0045 loss_train: 1.4341 acc_train: 0.4929 loss_val: 1.5456 acc_val: 0.4467 time: 0.0174s\n",
            "Epoch: 0046 loss_train: 1.3956 acc_train: 0.5357 loss_val: 1.5329 acc_val: 0.4500 time: 0.0227s\n",
            "Epoch: 0047 loss_train: 1.3661 acc_train: 0.5714 loss_val: 1.5203 acc_val: 0.4567 time: 0.0174s\n",
            "Epoch: 0048 loss_train: 1.3865 acc_train: 0.5429 loss_val: 1.5082 acc_val: 0.4833 time: 0.0155s\n",
            "Epoch: 0049 loss_train: 1.3492 acc_train: 0.5786 loss_val: 1.4962 acc_val: 0.5000 time: 0.0169s\n",
            "Epoch: 0050 loss_train: 1.3281 acc_train: 0.6357 loss_val: 1.4841 acc_val: 0.5100 time: 0.0172s\n",
            "Epoch: 0051 loss_train: 1.2961 acc_train: 0.6214 loss_val: 1.4719 acc_val: 0.5200 time: 0.0181s\n",
            "Epoch: 0052 loss_train: 1.3073 acc_train: 0.6071 loss_val: 1.4597 acc_val: 0.5267 time: 0.0214s\n",
            "Epoch: 0053 loss_train: 1.3046 acc_train: 0.5857 loss_val: 1.4487 acc_val: 0.5400 time: 0.0181s\n",
            "Epoch: 0054 loss_train: 1.2708 acc_train: 0.6286 loss_val: 1.4387 acc_val: 0.5600 time: 0.0220s\n",
            "Epoch: 0055 loss_train: 1.2682 acc_train: 0.6357 loss_val: 1.4284 acc_val: 0.5733 time: 0.0182s\n",
            "Epoch: 0056 loss_train: 1.2485 acc_train: 0.6500 loss_val: 1.4172 acc_val: 0.5800 time: 0.0176s\n",
            "Epoch: 0057 loss_train: 1.2260 acc_train: 0.6929 loss_val: 1.4061 acc_val: 0.5900 time: 0.0228s\n",
            "Epoch: 0058 loss_train: 1.2216 acc_train: 0.6571 loss_val: 1.3948 acc_val: 0.5967 time: 0.0195s\n",
            "Epoch: 0059 loss_train: 1.2188 acc_train: 0.6500 loss_val: 1.3836 acc_val: 0.6000 time: 0.0179s\n",
            "Epoch: 0060 loss_train: 1.1834 acc_train: 0.6857 loss_val: 1.3717 acc_val: 0.5967 time: 0.0177s\n",
            "Epoch: 0061 loss_train: 1.1934 acc_train: 0.6571 loss_val: 1.3600 acc_val: 0.5967 time: 0.0180s\n",
            "Epoch: 0062 loss_train: 1.1899 acc_train: 0.6571 loss_val: 1.3485 acc_val: 0.6000 time: 0.0160s\n",
            "Epoch: 0063 loss_train: 1.1379 acc_train: 0.7000 loss_val: 1.3378 acc_val: 0.6033 time: 0.0170s\n",
            "Epoch: 0064 loss_train: 1.1593 acc_train: 0.6429 loss_val: 1.3273 acc_val: 0.6167 time: 0.0202s\n",
            "Epoch: 0065 loss_train: 1.1545 acc_train: 0.6214 loss_val: 1.3173 acc_val: 0.6167 time: 0.0165s\n",
            "Epoch: 0066 loss_train: 1.1009 acc_train: 0.7071 loss_val: 1.3075 acc_val: 0.6267 time: 0.0170s\n",
            "Epoch: 0067 loss_train: 1.1095 acc_train: 0.6786 loss_val: 1.2983 acc_val: 0.6333 time: 0.0169s\n",
            "Epoch: 0068 loss_train: 1.0943 acc_train: 0.7071 loss_val: 1.2889 acc_val: 0.6467 time: 0.0201s\n",
            "Epoch: 0069 loss_train: 1.0698 acc_train: 0.7429 loss_val: 1.2796 acc_val: 0.6533 time: 0.0198s\n",
            "Epoch: 0070 loss_train: 1.0267 acc_train: 0.7286 loss_val: 1.2704 acc_val: 0.6767 time: 0.0167s\n",
            "Epoch: 0071 loss_train: 1.0873 acc_train: 0.6857 loss_val: 1.2614 acc_val: 0.6833 time: 0.0163s\n",
            "Epoch: 0072 loss_train: 1.0238 acc_train: 0.7714 loss_val: 1.2528 acc_val: 0.6833 time: 0.0166s\n",
            "Epoch: 0073 loss_train: 1.0267 acc_train: 0.7571 loss_val: 1.2443 acc_val: 0.6900 time: 0.0158s\n",
            "Epoch: 0074 loss_train: 1.0580 acc_train: 0.7500 loss_val: 1.2360 acc_val: 0.6900 time: 0.0193s\n",
            "Epoch: 0075 loss_train: 0.9997 acc_train: 0.7071 loss_val: 1.2276 acc_val: 0.6933 time: 0.0161s\n",
            "Epoch: 0076 loss_train: 1.0189 acc_train: 0.7786 loss_val: 1.2187 acc_val: 0.6933 time: 0.0160s\n",
            "Epoch: 0077 loss_train: 0.9708 acc_train: 0.7929 loss_val: 1.2101 acc_val: 0.6967 time: 0.0165s\n",
            "Epoch: 0078 loss_train: 1.0043 acc_train: 0.7214 loss_val: 1.2013 acc_val: 0.7033 time: 0.0195s\n",
            "Epoch: 0079 loss_train: 0.9848 acc_train: 0.7429 loss_val: 1.1924 acc_val: 0.7033 time: 0.0161s\n",
            "Epoch: 0080 loss_train: 0.9555 acc_train: 0.8000 loss_val: 1.1838 acc_val: 0.7033 time: 0.0220s\n",
            "Epoch: 0081 loss_train: 0.9540 acc_train: 0.7357 loss_val: 1.1756 acc_val: 0.7033 time: 0.0170s\n",
            "Epoch: 0082 loss_train: 0.9423 acc_train: 0.7643 loss_val: 1.1674 acc_val: 0.7100 time: 0.0158s\n",
            "Epoch: 0083 loss_train: 0.9618 acc_train: 0.7786 loss_val: 1.1593 acc_val: 0.7067 time: 0.0162s\n",
            "Epoch: 0084 loss_train: 0.9395 acc_train: 0.7857 loss_val: 1.1518 acc_val: 0.7100 time: 0.0164s\n",
            "Epoch: 0085 loss_train: 0.9127 acc_train: 0.7786 loss_val: 1.1446 acc_val: 0.7167 time: 0.0177s\n",
            "Epoch: 0086 loss_train: 0.9343 acc_train: 0.7214 loss_val: 1.1380 acc_val: 0.7167 time: 0.0174s\n",
            "Epoch: 0087 loss_train: 0.9163 acc_train: 0.8071 loss_val: 1.1308 acc_val: 0.7233 time: 0.0195s\n",
            "Epoch: 0088 loss_train: 0.9013 acc_train: 0.7500 loss_val: 1.1245 acc_val: 0.7267 time: 0.0180s\n",
            "Epoch: 0089 loss_train: 0.8831 acc_train: 0.7929 loss_val: 1.1178 acc_val: 0.7267 time: 0.0226s\n",
            "Epoch: 0090 loss_train: 0.8492 acc_train: 0.8357 loss_val: 1.1113 acc_val: 0.7267 time: 0.0153s\n",
            "Epoch: 0091 loss_train: 0.9020 acc_train: 0.7786 loss_val: 1.1053 acc_val: 0.7400 time: 0.0142s\n",
            "Epoch: 0092 loss_train: 0.8279 acc_train: 0.8571 loss_val: 1.0995 acc_val: 0.7467 time: 0.0335s\n",
            "Epoch: 0093 loss_train: 0.8805 acc_train: 0.8214 loss_val: 1.0931 acc_val: 0.7433 time: 0.0183s\n",
            "Epoch: 0094 loss_train: 0.8755 acc_train: 0.8071 loss_val: 1.0858 acc_val: 0.7467 time: 0.0184s\n",
            "Epoch: 0095 loss_train: 0.8813 acc_train: 0.8071 loss_val: 1.0775 acc_val: 0.7467 time: 0.0196s\n",
            "Epoch: 0096 loss_train: 0.8461 acc_train: 0.8071 loss_val: 1.0695 acc_val: 0.7433 time: 0.0184s\n",
            "Epoch: 0097 loss_train: 0.8285 acc_train: 0.8214 loss_val: 1.0615 acc_val: 0.7433 time: 0.0167s\n",
            "Epoch: 0098 loss_train: 0.8088 acc_train: 0.8143 loss_val: 1.0541 acc_val: 0.7533 time: 0.0178s\n",
            "Epoch: 0099 loss_train: 0.7539 acc_train: 0.8500 loss_val: 1.0475 acc_val: 0.7567 time: 0.0162s\n",
            "Epoch: 0100 loss_train: 0.8120 acc_train: 0.8071 loss_val: 1.0421 acc_val: 0.7633 time: 0.0165s\n",
            "Epoch: 0101 loss_train: 0.7992 acc_train: 0.8286 loss_val: 1.0381 acc_val: 0.7767 time: 0.0159s\n",
            "Epoch: 0102 loss_train: 0.7746 acc_train: 0.8643 loss_val: 1.0355 acc_val: 0.7767 time: 0.0210s\n",
            "Epoch: 0103 loss_train: 0.7644 acc_train: 0.8714 loss_val: 1.0324 acc_val: 0.7800 time: 0.0162s\n",
            "Epoch: 0104 loss_train: 0.7908 acc_train: 0.8429 loss_val: 1.0277 acc_val: 0.7867 time: 0.0160s\n",
            "Epoch: 0105 loss_train: 0.7681 acc_train: 0.8500 loss_val: 1.0219 acc_val: 0.7800 time: 0.0170s\n",
            "Epoch: 0106 loss_train: 0.7636 acc_train: 0.8500 loss_val: 1.0155 acc_val: 0.7767 time: 0.0160s\n",
            "Epoch: 0107 loss_train: 0.7042 acc_train: 0.8786 loss_val: 1.0082 acc_val: 0.7733 time: 0.0163s\n",
            "Epoch: 0108 loss_train: 0.7764 acc_train: 0.8500 loss_val: 1.0013 acc_val: 0.7800 time: 0.0168s\n",
            "Epoch: 0109 loss_train: 0.7420 acc_train: 0.8286 loss_val: 0.9961 acc_val: 0.7800 time: 0.0204s\n",
            "Epoch: 0110 loss_train: 0.6976 acc_train: 0.8714 loss_val: 0.9916 acc_val: 0.7767 time: 0.0162s\n",
            "Epoch: 0111 loss_train: 0.7189 acc_train: 0.8643 loss_val: 0.9867 acc_val: 0.7767 time: 0.0169s\n",
            "Epoch: 0112 loss_train: 0.6992 acc_train: 0.9000 loss_val: 0.9820 acc_val: 0.7800 time: 0.0158s\n",
            "Epoch: 0113 loss_train: 0.7524 acc_train: 0.8429 loss_val: 0.9771 acc_val: 0.7733 time: 0.0207s\n",
            "Epoch: 0114 loss_train: 0.6869 acc_train: 0.8714 loss_val: 0.9725 acc_val: 0.7733 time: 0.0153s\n",
            "Epoch: 0115 loss_train: 0.7400 acc_train: 0.8500 loss_val: 0.9677 acc_val: 0.7767 time: 0.0178s\n",
            "Epoch: 0116 loss_train: 0.6939 acc_train: 0.8857 loss_val: 0.9627 acc_val: 0.7800 time: 0.0160s\n",
            "Epoch: 0117 loss_train: 0.6921 acc_train: 0.8500 loss_val: 0.9582 acc_val: 0.7933 time: 0.0158s\n",
            "Epoch: 0118 loss_train: 0.6831 acc_train: 0.8857 loss_val: 0.9539 acc_val: 0.7933 time: 0.0163s\n",
            "Epoch: 0119 loss_train: 0.6610 acc_train: 0.8643 loss_val: 0.9509 acc_val: 0.7967 time: 0.0231s\n",
            "Epoch: 0120 loss_train: 0.6568 acc_train: 0.9143 loss_val: 0.9468 acc_val: 0.7967 time: 0.0151s\n",
            "Epoch: 0121 loss_train: 0.6516 acc_train: 0.9071 loss_val: 0.9425 acc_val: 0.7967 time: 0.0171s\n",
            "Epoch: 0122 loss_train: 0.6711 acc_train: 0.8500 loss_val: 0.9388 acc_val: 0.7967 time: 0.0165s\n",
            "Epoch: 0123 loss_train: 0.6283 acc_train: 0.8786 loss_val: 0.9352 acc_val: 0.8000 time: 0.0147s\n",
            "Epoch: 0124 loss_train: 0.6835 acc_train: 0.8500 loss_val: 0.9323 acc_val: 0.7900 time: 0.0153s\n",
            "Epoch: 0125 loss_train: 0.6314 acc_train: 0.8643 loss_val: 0.9290 acc_val: 0.7900 time: 0.0196s\n",
            "Epoch: 0126 loss_train: 0.6142 acc_train: 0.8929 loss_val: 0.9258 acc_val: 0.7833 time: 0.0157s\n",
            "Epoch: 0127 loss_train: 0.6538 acc_train: 0.8929 loss_val: 0.9217 acc_val: 0.7833 time: 0.0156s\n",
            "Epoch: 0128 loss_train: 0.6530 acc_train: 0.8786 loss_val: 0.9174 acc_val: 0.7833 time: 0.0157s\n",
            "Epoch: 0129 loss_train: 0.6506 acc_train: 0.8786 loss_val: 0.9130 acc_val: 0.7800 time: 0.0164s\n",
            "Epoch: 0130 loss_train: 0.5900 acc_train: 0.9143 loss_val: 0.9100 acc_val: 0.7833 time: 0.0175s\n",
            "Epoch: 0131 loss_train: 0.6349 acc_train: 0.8714 loss_val: 0.9068 acc_val: 0.7833 time: 0.0184s\n",
            "Epoch: 0132 loss_train: 0.6196 acc_train: 0.8857 loss_val: 0.9031 acc_val: 0.7900 time: 0.0188s\n",
            "Epoch: 0133 loss_train: 0.6198 acc_train: 0.8714 loss_val: 0.9005 acc_val: 0.7967 time: 0.0157s\n",
            "Epoch: 0134 loss_train: 0.6244 acc_train: 0.8786 loss_val: 0.8990 acc_val: 0.8033 time: 0.0161s\n",
            "Epoch: 0135 loss_train: 0.5644 acc_train: 0.9143 loss_val: 0.8977 acc_val: 0.8033 time: 0.0167s\n",
            "Epoch: 0136 loss_train: 0.6121 acc_train: 0.9071 loss_val: 0.8969 acc_val: 0.8100 time: 0.0178s\n",
            "Epoch: 0137 loss_train: 0.6310 acc_train: 0.8857 loss_val: 0.8938 acc_val: 0.8033 time: 0.0242s\n",
            "Epoch: 0138 loss_train: 0.5550 acc_train: 0.8929 loss_val: 0.8900 acc_val: 0.8033 time: 0.0147s\n",
            "Epoch: 0139 loss_train: 0.5582 acc_train: 0.9214 loss_val: 0.8856 acc_val: 0.8067 time: 0.0130s\n",
            "Epoch: 0140 loss_train: 0.6206 acc_train: 0.8929 loss_val: 0.8811 acc_val: 0.8067 time: 0.0150s\n",
            "Epoch: 0141 loss_train: 0.5488 acc_train: 0.9143 loss_val: 0.8778 acc_val: 0.8067 time: 0.0143s\n",
            "Epoch: 0142 loss_train: 0.5822 acc_train: 0.8857 loss_val: 0.8753 acc_val: 0.8067 time: 0.0173s\n",
            "Epoch: 0143 loss_train: 0.5608 acc_train: 0.8857 loss_val: 0.8734 acc_val: 0.8067 time: 0.0192s\n",
            "Epoch: 0144 loss_train: 0.5719 acc_train: 0.9000 loss_val: 0.8714 acc_val: 0.8067 time: 0.0186s\n",
            "Epoch: 0145 loss_train: 0.5234 acc_train: 0.9357 loss_val: 0.8700 acc_val: 0.8000 time: 0.0164s\n",
            "Epoch: 0146 loss_train: 0.5631 acc_train: 0.8929 loss_val: 0.8685 acc_val: 0.8000 time: 0.0190s\n",
            "Epoch: 0147 loss_train: 0.5254 acc_train: 0.8929 loss_val: 0.8652 acc_val: 0.8033 time: 0.0219s\n",
            "Epoch: 0148 loss_train: 0.5636 acc_train: 0.9286 loss_val: 0.8622 acc_val: 0.8033 time: 0.0168s\n",
            "Epoch: 0149 loss_train: 0.5345 acc_train: 0.8786 loss_val: 0.8597 acc_val: 0.8033 time: 0.0228s\n",
            "Epoch: 0150 loss_train: 0.5213 acc_train: 0.9214 loss_val: 0.8575 acc_val: 0.8033 time: 0.0159s\n",
            "Epoch: 0151 loss_train: 0.5472 acc_train: 0.9000 loss_val: 0.8562 acc_val: 0.8033 time: 0.0147s\n",
            "Epoch: 0152 loss_train: 0.4971 acc_train: 0.9214 loss_val: 0.8550 acc_val: 0.8000 time: 0.0154s\n",
            "Epoch: 0153 loss_train: 0.5307 acc_train: 0.9071 loss_val: 0.8532 acc_val: 0.8000 time: 0.0154s\n",
            "Epoch: 0154 loss_train: 0.4986 acc_train: 0.9143 loss_val: 0.8526 acc_val: 0.8100 time: 0.0194s\n",
            "Epoch: 0155 loss_train: 0.5166 acc_train: 0.9357 loss_val: 0.8519 acc_val: 0.8100 time: 0.0159s\n",
            "Epoch: 0156 loss_train: 0.5074 acc_train: 0.9286 loss_val: 0.8497 acc_val: 0.8133 time: 0.0161s\n",
            "Epoch: 0157 loss_train: 0.5233 acc_train: 0.9143 loss_val: 0.8466 acc_val: 0.8133 time: 0.0162s\n",
            "Epoch: 0158 loss_train: 0.5109 acc_train: 0.9429 loss_val: 0.8425 acc_val: 0.8100 time: 0.0145s\n",
            "Epoch: 0159 loss_train: 0.5054 acc_train: 0.9286 loss_val: 0.8376 acc_val: 0.8133 time: 0.0168s\n",
            "Epoch: 0160 loss_train: 0.4804 acc_train: 0.9500 loss_val: 0.8355 acc_val: 0.8133 time: 0.0190s\n",
            "Epoch: 0161 loss_train: 0.5276 acc_train: 0.9071 loss_val: 0.8320 acc_val: 0.8133 time: 0.0197s\n",
            "Epoch: 0162 loss_train: 0.5034 acc_train: 0.9357 loss_val: 0.8280 acc_val: 0.8067 time: 0.0169s\n",
            "Epoch: 0163 loss_train: 0.5109 acc_train: 0.9071 loss_val: 0.8253 acc_val: 0.8033 time: 0.0158s\n",
            "Epoch: 0164 loss_train: 0.5057 acc_train: 0.8929 loss_val: 0.8237 acc_val: 0.8033 time: 0.0174s\n",
            "Epoch: 0165 loss_train: 0.5033 acc_train: 0.9429 loss_val: 0.8249 acc_val: 0.8067 time: 0.0212s\n",
            "Epoch: 0166 loss_train: 0.5007 acc_train: 0.9286 loss_val: 0.8271 acc_val: 0.8100 time: 0.0178s\n",
            "Epoch: 0167 loss_train: 0.4745 acc_train: 0.9143 loss_val: 0.8286 acc_val: 0.8133 time: 0.0160s\n",
            "Epoch: 0168 loss_train: 0.5112 acc_train: 0.9214 loss_val: 0.8275 acc_val: 0.8100 time: 0.0243s\n",
            "Epoch: 0169 loss_train: 0.5387 acc_train: 0.9214 loss_val: 0.8244 acc_val: 0.8067 time: 0.0163s\n",
            "Epoch: 0170 loss_train: 0.4846 acc_train: 0.9429 loss_val: 0.8199 acc_val: 0.8067 time: 0.0186s\n",
            "Epoch: 0171 loss_train: 0.5255 acc_train: 0.9429 loss_val: 0.8148 acc_val: 0.8067 time: 0.0206s\n",
            "Epoch: 0172 loss_train: 0.4849 acc_train: 0.9143 loss_val: 0.8118 acc_val: 0.8067 time: 0.0142s\n",
            "Epoch: 0173 loss_train: 0.4751 acc_train: 0.9071 loss_val: 0.8112 acc_val: 0.8133 time: 0.0159s\n",
            "Epoch: 0174 loss_train: 0.4968 acc_train: 0.9000 loss_val: 0.8126 acc_val: 0.8067 time: 0.0145s\n",
            "Epoch: 0175 loss_train: 0.4905 acc_train: 0.9071 loss_val: 0.8149 acc_val: 0.8067 time: 0.0156s\n",
            "Epoch: 0176 loss_train: 0.4765 acc_train: 0.9214 loss_val: 0.8181 acc_val: 0.8133 time: 0.0185s\n",
            "Epoch: 0177 loss_train: 0.4760 acc_train: 0.9357 loss_val: 0.8209 acc_val: 0.8133 time: 0.0175s\n",
            "Epoch: 0178 loss_train: 0.4616 acc_train: 0.9429 loss_val: 0.8214 acc_val: 0.8100 time: 0.0144s\n",
            "Epoch: 0179 loss_train: 0.4732 acc_train: 0.9500 loss_val: 0.8191 acc_val: 0.8067 time: 0.0151s\n",
            "Epoch: 0180 loss_train: 0.4463 acc_train: 0.9214 loss_val: 0.8147 acc_val: 0.8100 time: 0.0157s\n",
            "Epoch: 0181 loss_train: 0.4506 acc_train: 0.9500 loss_val: 0.8115 acc_val: 0.8000 time: 0.0166s\n",
            "Epoch: 0182 loss_train: 0.4642 acc_train: 0.9214 loss_val: 0.8083 acc_val: 0.8033 time: 0.0177s\n",
            "Epoch: 0183 loss_train: 0.4902 acc_train: 0.9071 loss_val: 0.8059 acc_val: 0.8033 time: 0.0232s\n",
            "Epoch: 0184 loss_train: 0.4658 acc_train: 0.9429 loss_val: 0.8029 acc_val: 0.8133 time: 0.0203s\n",
            "Epoch: 0185 loss_train: 0.4412 acc_train: 0.9357 loss_val: 0.8001 acc_val: 0.8133 time: 0.0141s\n",
            "Epoch: 0186 loss_train: 0.4883 acc_train: 0.9214 loss_val: 0.7987 acc_val: 0.8067 time: 0.0149s\n",
            "Epoch: 0187 loss_train: 0.4664 acc_train: 0.9286 loss_val: 0.7963 acc_val: 0.8067 time: 0.0169s\n",
            "Epoch: 0188 loss_train: 0.4556 acc_train: 0.9214 loss_val: 0.7951 acc_val: 0.8133 time: 0.0198s\n",
            "Epoch: 0189 loss_train: 0.4440 acc_train: 0.9286 loss_val: 0.7945 acc_val: 0.8133 time: 0.0179s\n",
            "Epoch: 0190 loss_train: 0.4285 acc_train: 0.9500 loss_val: 0.7935 acc_val: 0.8167 time: 0.0167s\n",
            "Epoch: 0191 loss_train: 0.4484 acc_train: 0.9429 loss_val: 0.7932 acc_val: 0.8200 time: 0.0162s\n",
            "Epoch: 0192 loss_train: 0.4607 acc_train: 0.9286 loss_val: 0.7915 acc_val: 0.8167 time: 0.0163s\n",
            "Epoch: 0193 loss_train: 0.4391 acc_train: 0.9571 loss_val: 0.7886 acc_val: 0.8100 time: 0.0168s\n",
            "Epoch: 0194 loss_train: 0.4144 acc_train: 0.9500 loss_val: 0.7877 acc_val: 0.8100 time: 0.0157s\n",
            "Epoch: 0195 loss_train: 0.4575 acc_train: 0.9000 loss_val: 0.7874 acc_val: 0.8100 time: 0.0202s\n",
            "Epoch: 0196 loss_train: 0.3951 acc_train: 0.9714 loss_val: 0.7877 acc_val: 0.8133 time: 0.0157s\n",
            "Epoch: 0197 loss_train: 0.4326 acc_train: 0.9357 loss_val: 0.7878 acc_val: 0.8067 time: 0.0163s\n",
            "Epoch: 0198 loss_train: 0.4370 acc_train: 0.9571 loss_val: 0.7869 acc_val: 0.8100 time: 0.0163s\n",
            "Epoch: 0199 loss_train: 0.4422 acc_train: 0.9357 loss_val: 0.7858 acc_val: 0.8067 time: 0.0150s\n",
            "Epoch: 0200 loss_train: 0.4184 acc_train: 0.9500 loss_val: 0.7849 acc_val: 0.8133 time: 0.0166s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 3.7596s\n",
            "Test set results: loss= 0.8473 accuracy= 0.7680\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZCDXVI--5QJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88107767-b0de-4992-eb38-b84dd345457c"
      },
      "source": [
        "# (1.3) Compare the runtimes of (1) with and without GPU support.\n",
        "%run pygcn/train.py\n",
        "#Observation : Accuracy changed slightly from (1). Time elapsed decreased from (1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cora dataset...\n",
            "Epoch: 0001 loss_train: 1.9541 acc_train: 0.1786 loss_val: 1.9522 acc_val: 0.1567 time: 0.3671s\n",
            "Epoch: 0002 loss_train: 1.9420 acc_train: 0.1929 loss_val: 1.9402 acc_val: 0.1567 time: 0.0118s\n",
            "Epoch: 0003 loss_train: 1.9341 acc_train: 0.2000 loss_val: 1.9276 acc_val: 0.1567 time: 0.0110s\n",
            "Epoch: 0004 loss_train: 1.9190 acc_train: 0.2000 loss_val: 1.9146 acc_val: 0.1567 time: 0.0122s\n",
            "Epoch: 0005 loss_train: 1.9034 acc_train: 0.2000 loss_val: 1.9014 acc_val: 0.1567 time: 0.0126s\n",
            "Epoch: 0006 loss_train: 1.8881 acc_train: 0.2000 loss_val: 1.8884 acc_val: 0.1567 time: 0.0080s\n",
            "Epoch: 0007 loss_train: 1.8694 acc_train: 0.2214 loss_val: 1.8753 acc_val: 0.1567 time: 0.0109s\n",
            "Epoch: 0008 loss_train: 1.8619 acc_train: 0.2214 loss_val: 1.8623 acc_val: 0.1567 time: 0.0157s\n",
            "Epoch: 0009 loss_train: 1.8533 acc_train: 0.2357 loss_val: 1.8496 acc_val: 0.1800 time: 0.0107s\n",
            "Epoch: 0010 loss_train: 1.8507 acc_train: 0.2214 loss_val: 1.8372 acc_val: 0.3000 time: 0.0110s\n",
            "Epoch: 0011 loss_train: 1.8294 acc_train: 0.3000 loss_val: 1.8251 acc_val: 0.4533 time: 0.0095s\n",
            "Epoch: 0012 loss_train: 1.8145 acc_train: 0.3786 loss_val: 1.8133 acc_val: 0.4533 time: 0.0096s\n",
            "Epoch: 0013 loss_train: 1.8062 acc_train: 0.3357 loss_val: 1.8017 acc_val: 0.4067 time: 0.0119s\n",
            "Epoch: 0014 loss_train: 1.8021 acc_train: 0.3500 loss_val: 1.7904 acc_val: 0.3700 time: 0.0136s\n",
            "Epoch: 0015 loss_train: 1.7811 acc_train: 0.3714 loss_val: 1.7792 acc_val: 0.3533 time: 0.0115s\n",
            "Epoch: 0016 loss_train: 1.7984 acc_train: 0.3143 loss_val: 1.7686 acc_val: 0.3533 time: 0.0098s\n",
            "Epoch: 0017 loss_train: 1.7600 acc_train: 0.3571 loss_val: 1.7585 acc_val: 0.3500 time: 0.0107s\n",
            "Epoch: 0018 loss_train: 1.7392 acc_train: 0.3571 loss_val: 1.7487 acc_val: 0.3500 time: 0.0161s\n",
            "Epoch: 0019 loss_train: 1.7303 acc_train: 0.3000 loss_val: 1.7395 acc_val: 0.3500 time: 0.0117s\n",
            "Epoch: 0020 loss_train: 1.7602 acc_train: 0.3214 loss_val: 1.7309 acc_val: 0.3500 time: 0.0115s\n",
            "Epoch: 0021 loss_train: 1.7010 acc_train: 0.3429 loss_val: 1.7228 acc_val: 0.3500 time: 0.0110s\n",
            "Epoch: 0022 loss_train: 1.7340 acc_train: 0.2929 loss_val: 1.7155 acc_val: 0.3500 time: 0.0100s\n",
            "Epoch: 0023 loss_train: 1.7181 acc_train: 0.3000 loss_val: 1.7086 acc_val: 0.3500 time: 0.0108s\n",
            "Epoch: 0024 loss_train: 1.6867 acc_train: 0.3000 loss_val: 1.7019 acc_val: 0.3500 time: 0.0109s\n",
            "Epoch: 0025 loss_train: 1.6905 acc_train: 0.3214 loss_val: 1.6954 acc_val: 0.3500 time: 0.0138s\n",
            "Epoch: 0026 loss_train: 1.6644 acc_train: 0.3214 loss_val: 1.6890 acc_val: 0.3500 time: 0.0132s\n",
            "Epoch: 0027 loss_train: 1.6683 acc_train: 0.3143 loss_val: 1.6826 acc_val: 0.3500 time: 0.0115s\n",
            "Epoch: 0028 loss_train: 1.6653 acc_train: 0.3143 loss_val: 1.6761 acc_val: 0.3533 time: 0.0114s\n",
            "Epoch: 0029 loss_train: 1.6422 acc_train: 0.3429 loss_val: 1.6695 acc_val: 0.3533 time: 0.0109s\n",
            "Epoch: 0030 loss_train: 1.6349 acc_train: 0.3357 loss_val: 1.6626 acc_val: 0.3633 time: 0.0113s\n",
            "Epoch: 0031 loss_train: 1.6447 acc_train: 0.3857 loss_val: 1.6556 acc_val: 0.3667 time: 0.0101s\n",
            "Epoch: 0032 loss_train: 1.6123 acc_train: 0.3786 loss_val: 1.6486 acc_val: 0.3667 time: 0.0114s\n",
            "Epoch: 0033 loss_train: 1.6148 acc_train: 0.4000 loss_val: 1.6413 acc_val: 0.3767 time: 0.0112s\n",
            "Epoch: 0034 loss_train: 1.5914 acc_train: 0.4071 loss_val: 1.6337 acc_val: 0.3933 time: 0.0110s\n",
            "Epoch: 0035 loss_train: 1.5657 acc_train: 0.4000 loss_val: 1.6255 acc_val: 0.4000 time: 0.0099s\n",
            "Epoch: 0036 loss_train: 1.6006 acc_train: 0.4286 loss_val: 1.6165 acc_val: 0.4033 time: 0.0199s\n",
            "Epoch: 0037 loss_train: 1.5968 acc_train: 0.4214 loss_val: 1.6070 acc_val: 0.4133 time: 0.0093s\n",
            "Epoch: 0038 loss_train: 1.5667 acc_train: 0.4500 loss_val: 1.5967 acc_val: 0.4167 time: 0.0114s\n",
            "Epoch: 0039 loss_train: 1.5225 acc_train: 0.4357 loss_val: 1.5857 acc_val: 0.4167 time: 0.0113s\n",
            "Epoch: 0040 loss_train: 1.5099 acc_train: 0.4714 loss_val: 1.5742 acc_val: 0.4200 time: 0.0102s\n",
            "Epoch: 0041 loss_train: 1.4796 acc_train: 0.4786 loss_val: 1.5623 acc_val: 0.4200 time: 0.0110s\n",
            "Epoch: 0042 loss_train: 1.4679 acc_train: 0.4643 loss_val: 1.5502 acc_val: 0.4233 time: 0.0114s\n",
            "Epoch: 0043 loss_train: 1.5034 acc_train: 0.4571 loss_val: 1.5381 acc_val: 0.4333 time: 0.0111s\n",
            "Epoch: 0044 loss_train: 1.4476 acc_train: 0.4571 loss_val: 1.5260 acc_val: 0.4500 time: 0.0114s\n",
            "Epoch: 0045 loss_train: 1.4446 acc_train: 0.4571 loss_val: 1.5140 acc_val: 0.4567 time: 0.0111s\n",
            "Epoch: 0046 loss_train: 1.4270 acc_train: 0.5143 loss_val: 1.5019 acc_val: 0.4667 time: 0.0111s\n",
            "Epoch: 0047 loss_train: 1.4059 acc_train: 0.4929 loss_val: 1.4899 acc_val: 0.4900 time: 0.0109s\n",
            "Epoch: 0048 loss_train: 1.3847 acc_train: 0.4929 loss_val: 1.4780 acc_val: 0.5133 time: 0.0110s\n",
            "Epoch: 0049 loss_train: 1.3758 acc_train: 0.5500 loss_val: 1.4657 acc_val: 0.5367 time: 0.0106s\n",
            "Epoch: 0050 loss_train: 1.3673 acc_train: 0.5643 loss_val: 1.4531 acc_val: 0.5500 time: 0.0108s\n",
            "Epoch: 0051 loss_train: 1.3542 acc_train: 0.5714 loss_val: 1.4400 acc_val: 0.5533 time: 0.0112s\n",
            "Epoch: 0052 loss_train: 1.3370 acc_train: 0.6143 loss_val: 1.4266 acc_val: 0.5567 time: 0.0144s\n",
            "Epoch: 0053 loss_train: 1.2837 acc_train: 0.5571 loss_val: 1.4130 acc_val: 0.5667 time: 0.0108s\n",
            "Epoch: 0054 loss_train: 1.3023 acc_train: 0.5929 loss_val: 1.3992 acc_val: 0.5800 time: 0.0110s\n",
            "Epoch: 0055 loss_train: 1.2776 acc_train: 0.6143 loss_val: 1.3853 acc_val: 0.5933 time: 0.0108s\n",
            "Epoch: 0056 loss_train: 1.2531 acc_train: 0.6571 loss_val: 1.3711 acc_val: 0.6000 time: 0.0108s\n",
            "Epoch: 0057 loss_train: 1.2205 acc_train: 0.6571 loss_val: 1.3574 acc_val: 0.6033 time: 0.0113s\n",
            "Epoch: 0058 loss_train: 1.2276 acc_train: 0.6786 loss_val: 1.3437 acc_val: 0.6167 time: 0.0092s\n",
            "Epoch: 0059 loss_train: 1.2049 acc_train: 0.6357 loss_val: 1.3301 acc_val: 0.6433 time: 0.0105s\n",
            "Epoch: 0060 loss_train: 1.1649 acc_train: 0.6929 loss_val: 1.3169 acc_val: 0.6500 time: 0.0142s\n",
            "Epoch: 0061 loss_train: 1.2072 acc_train: 0.6714 loss_val: 1.3033 acc_val: 0.6500 time: 0.0102s\n",
            "Epoch: 0062 loss_train: 1.1659 acc_train: 0.7143 loss_val: 1.2902 acc_val: 0.6600 time: 0.0110s\n",
            "Epoch: 0063 loss_train: 1.1410 acc_train: 0.6929 loss_val: 1.2776 acc_val: 0.6667 time: 0.0105s\n",
            "Epoch: 0064 loss_train: 1.1312 acc_train: 0.6857 loss_val: 1.2653 acc_val: 0.6833 time: 0.0096s\n",
            "Epoch: 0065 loss_train: 1.1086 acc_train: 0.7286 loss_val: 1.2530 acc_val: 0.6933 time: 0.0113s\n",
            "Epoch: 0066 loss_train: 1.1157 acc_train: 0.6857 loss_val: 1.2406 acc_val: 0.6933 time: 0.0108s\n",
            "Epoch: 0067 loss_train: 1.0763 acc_train: 0.7143 loss_val: 1.2285 acc_val: 0.7000 time: 0.0104s\n",
            "Epoch: 0068 loss_train: 1.0807 acc_train: 0.7214 loss_val: 1.2164 acc_val: 0.7033 time: 0.0110s\n",
            "Epoch: 0069 loss_train: 1.0399 acc_train: 0.7429 loss_val: 1.2045 acc_val: 0.7067 time: 0.0109s\n",
            "Epoch: 0070 loss_train: 1.0290 acc_train: 0.7786 loss_val: 1.1923 acc_val: 0.7133 time: 0.0184s\n",
            "Epoch: 0071 loss_train: 1.0338 acc_train: 0.7357 loss_val: 1.1808 acc_val: 0.7267 time: 0.0109s\n",
            "Epoch: 0072 loss_train: 1.0522 acc_train: 0.6857 loss_val: 1.1698 acc_val: 0.7300 time: 0.0116s\n",
            "Epoch: 0073 loss_train: 1.0041 acc_train: 0.8000 loss_val: 1.1584 acc_val: 0.7300 time: 0.0113s\n",
            "Epoch: 0074 loss_train: 1.0092 acc_train: 0.7643 loss_val: 1.1471 acc_val: 0.7267 time: 0.0109s\n",
            "Epoch: 0075 loss_train: 0.9723 acc_train: 0.7643 loss_val: 1.1362 acc_val: 0.7367 time: 0.0106s\n",
            "Epoch: 0076 loss_train: 0.9763 acc_train: 0.8000 loss_val: 1.1252 acc_val: 0.7433 time: 0.0101s\n",
            "Epoch: 0077 loss_train: 0.9425 acc_train: 0.7714 loss_val: 1.1141 acc_val: 0.7467 time: 0.0143s\n",
            "Epoch: 0078 loss_train: 0.8915 acc_train: 0.8000 loss_val: 1.1043 acc_val: 0.7433 time: 0.0108s\n",
            "Epoch: 0079 loss_train: 0.9326 acc_train: 0.8214 loss_val: 1.0941 acc_val: 0.7467 time: 0.0111s\n",
            "Epoch: 0080 loss_train: 0.8759 acc_train: 0.8214 loss_val: 1.0844 acc_val: 0.7533 time: 0.0110s\n",
            "Epoch: 0081 loss_train: 0.8822 acc_train: 0.8214 loss_val: 1.0744 acc_val: 0.7600 time: 0.0106s\n",
            "Epoch: 0082 loss_train: 0.8915 acc_train: 0.8214 loss_val: 1.0644 acc_val: 0.7600 time: 0.0150s\n",
            "Epoch: 0083 loss_train: 0.8373 acc_train: 0.7929 loss_val: 1.0549 acc_val: 0.7633 time: 0.0102s\n",
            "Epoch: 0084 loss_train: 0.8716 acc_train: 0.8357 loss_val: 1.0447 acc_val: 0.7667 time: 0.0103s\n",
            "Epoch: 0085 loss_train: 0.8343 acc_train: 0.8429 loss_val: 1.0352 acc_val: 0.7733 time: 0.0101s\n",
            "Epoch: 0086 loss_train: 0.8399 acc_train: 0.8214 loss_val: 1.0257 acc_val: 0.7733 time: 0.0104s\n",
            "Epoch: 0087 loss_train: 0.8454 acc_train: 0.8071 loss_val: 1.0175 acc_val: 0.7667 time: 0.0123s\n",
            "Epoch: 0088 loss_train: 0.7975 acc_train: 0.8571 loss_val: 1.0096 acc_val: 0.7700 time: 0.0136s\n",
            "Epoch: 0089 loss_train: 0.7893 acc_train: 0.8643 loss_val: 1.0023 acc_val: 0.7700 time: 0.0136s\n",
            "Epoch: 0090 loss_train: 0.7915 acc_train: 0.8429 loss_val: 0.9940 acc_val: 0.7800 time: 0.0118s\n",
            "Epoch: 0091 loss_train: 0.8053 acc_train: 0.8500 loss_val: 0.9844 acc_val: 0.7833 time: 0.0095s\n",
            "Epoch: 0092 loss_train: 0.7619 acc_train: 0.8786 loss_val: 0.9751 acc_val: 0.7833 time: 0.0113s\n",
            "Epoch: 0093 loss_train: 0.7853 acc_train: 0.8643 loss_val: 0.9664 acc_val: 0.7867 time: 0.0111s\n",
            "Epoch: 0094 loss_train: 0.7821 acc_train: 0.8643 loss_val: 0.9577 acc_val: 0.7867 time: 0.0098s\n",
            "Epoch: 0095 loss_train: 0.7890 acc_train: 0.8429 loss_val: 0.9492 acc_val: 0.8000 time: 0.0106s\n",
            "Epoch: 0096 loss_train: 0.7165 acc_train: 0.8643 loss_val: 0.9411 acc_val: 0.8067 time: 0.0118s\n",
            "Epoch: 0097 loss_train: 0.6783 acc_train: 0.8857 loss_val: 0.9341 acc_val: 0.8067 time: 0.0115s\n",
            "Epoch: 0098 loss_train: 0.7314 acc_train: 0.8714 loss_val: 0.9282 acc_val: 0.7967 time: 0.0168s\n",
            "Epoch: 0099 loss_train: 0.7283 acc_train: 0.8500 loss_val: 0.9229 acc_val: 0.8033 time: 0.0076s\n",
            "Epoch: 0100 loss_train: 0.6847 acc_train: 0.9000 loss_val: 0.9170 acc_val: 0.8000 time: 0.0102s\n",
            "Epoch: 0101 loss_train: 0.7150 acc_train: 0.8571 loss_val: 0.9116 acc_val: 0.8000 time: 0.0109s\n",
            "Epoch: 0102 loss_train: 0.6749 acc_train: 0.9000 loss_val: 0.9060 acc_val: 0.8000 time: 0.0112s\n",
            "Epoch: 0103 loss_train: 0.6928 acc_train: 0.8714 loss_val: 0.9000 acc_val: 0.8033 time: 0.0109s\n",
            "Epoch: 0104 loss_train: 0.6270 acc_train: 0.9143 loss_val: 0.8939 acc_val: 0.8033 time: 0.0126s\n",
            "Epoch: 0105 loss_train: 0.6547 acc_train: 0.8929 loss_val: 0.8871 acc_val: 0.8033 time: 0.0149s\n",
            "Epoch: 0106 loss_train: 0.6407 acc_train: 0.8786 loss_val: 0.8811 acc_val: 0.8067 time: 0.0112s\n",
            "Epoch: 0107 loss_train: 0.6204 acc_train: 0.8857 loss_val: 0.8749 acc_val: 0.8133 time: 0.0110s\n",
            "Epoch: 0108 loss_train: 0.6539 acc_train: 0.8500 loss_val: 0.8694 acc_val: 0.8133 time: 0.0127s\n",
            "Epoch: 0109 loss_train: 0.5991 acc_train: 0.9214 loss_val: 0.8648 acc_val: 0.8200 time: 0.0106s\n",
            "Epoch: 0110 loss_train: 0.6201 acc_train: 0.8929 loss_val: 0.8613 acc_val: 0.8200 time: 0.0110s\n",
            "Epoch: 0111 loss_train: 0.6170 acc_train: 0.9000 loss_val: 0.8569 acc_val: 0.8200 time: 0.0123s\n",
            "Epoch: 0112 loss_train: 0.5775 acc_train: 0.9143 loss_val: 0.8525 acc_val: 0.8200 time: 0.0132s\n",
            "Epoch: 0113 loss_train: 0.6217 acc_train: 0.8857 loss_val: 0.8473 acc_val: 0.8200 time: 0.0113s\n",
            "Epoch: 0114 loss_train: 0.6367 acc_train: 0.9286 loss_val: 0.8420 acc_val: 0.8200 time: 0.0109s\n",
            "Epoch: 0115 loss_train: 0.6052 acc_train: 0.9143 loss_val: 0.8384 acc_val: 0.8167 time: 0.0112s\n",
            "Epoch: 0116 loss_train: 0.5714 acc_train: 0.9286 loss_val: 0.8350 acc_val: 0.8167 time: 0.0108s\n",
            "Epoch: 0117 loss_train: 0.6439 acc_train: 0.8786 loss_val: 0.8314 acc_val: 0.8167 time: 0.0110s\n",
            "Epoch: 0118 loss_train: 0.5660 acc_train: 0.9357 loss_val: 0.8279 acc_val: 0.8200 time: 0.0109s\n",
            "Epoch: 0119 loss_train: 0.5758 acc_train: 0.9143 loss_val: 0.8241 acc_val: 0.8200 time: 0.0108s\n",
            "Epoch: 0120 loss_train: 0.5693 acc_train: 0.9214 loss_val: 0.8225 acc_val: 0.8233 time: 0.0127s\n",
            "Epoch: 0121 loss_train: 0.5792 acc_train: 0.9071 loss_val: 0.8213 acc_val: 0.8133 time: 0.0143s\n",
            "Epoch: 0122 loss_train: 0.5403 acc_train: 0.9214 loss_val: 0.8193 acc_val: 0.8167 time: 0.0117s\n",
            "Epoch: 0123 loss_train: 0.5671 acc_train: 0.9429 loss_val: 0.8153 acc_val: 0.8167 time: 0.0095s\n",
            "Epoch: 0124 loss_train: 0.5873 acc_train: 0.9286 loss_val: 0.8116 acc_val: 0.8233 time: 0.0109s\n",
            "Epoch: 0125 loss_train: 0.5584 acc_train: 0.9143 loss_val: 0.8078 acc_val: 0.8233 time: 0.0109s\n",
            "Epoch: 0126 loss_train: 0.5451 acc_train: 0.9429 loss_val: 0.8021 acc_val: 0.8233 time: 0.0105s\n",
            "Epoch: 0127 loss_train: 0.5336 acc_train: 0.9429 loss_val: 0.7964 acc_val: 0.8233 time: 0.0114s\n",
            "Epoch: 0128 loss_train: 0.5324 acc_train: 0.9357 loss_val: 0.7911 acc_val: 0.8267 time: 0.0103s\n",
            "Epoch: 0129 loss_train: 0.4952 acc_train: 0.9214 loss_val: 0.7874 acc_val: 0.8233 time: 0.0108s\n",
            "Epoch: 0130 loss_train: 0.5448 acc_train: 0.9286 loss_val: 0.7843 acc_val: 0.8200 time: 0.0121s\n",
            "Epoch: 0131 loss_train: 0.5453 acc_train: 0.8929 loss_val: 0.7815 acc_val: 0.8200 time: 0.0115s\n",
            "Epoch: 0132 loss_train: 0.5437 acc_train: 0.9286 loss_val: 0.7786 acc_val: 0.8233 time: 0.0088s\n",
            "Epoch: 0133 loss_train: 0.5312 acc_train: 0.9071 loss_val: 0.7765 acc_val: 0.8267 time: 0.0112s\n",
            "Epoch: 0134 loss_train: 0.4846 acc_train: 0.9571 loss_val: 0.7759 acc_val: 0.8267 time: 0.0111s\n",
            "Epoch: 0135 loss_train: 0.5423 acc_train: 0.9357 loss_val: 0.7762 acc_val: 0.8267 time: 0.0109s\n",
            "Epoch: 0136 loss_train: 0.5120 acc_train: 0.9286 loss_val: 0.7754 acc_val: 0.8233 time: 0.0109s\n",
            "Epoch: 0137 loss_train: 0.5025 acc_train: 0.9214 loss_val: 0.7736 acc_val: 0.8233 time: 0.0103s\n",
            "Epoch: 0138 loss_train: 0.5255 acc_train: 0.9286 loss_val: 0.7699 acc_val: 0.8233 time: 0.0098s\n",
            "Epoch: 0139 loss_train: 0.4908 acc_train: 0.9571 loss_val: 0.7641 acc_val: 0.8233 time: 0.0120s\n",
            "Epoch: 0140 loss_train: 0.4931 acc_train: 0.9214 loss_val: 0.7589 acc_val: 0.8233 time: 0.0167s\n",
            "Epoch: 0141 loss_train: 0.4821 acc_train: 0.9500 loss_val: 0.7549 acc_val: 0.8233 time: 0.0111s\n",
            "Epoch: 0142 loss_train: 0.4763 acc_train: 0.9214 loss_val: 0.7517 acc_val: 0.8233 time: 0.0105s\n",
            "Epoch: 0143 loss_train: 0.4773 acc_train: 0.9143 loss_val: 0.7503 acc_val: 0.8233 time: 0.0116s\n",
            "Epoch: 0144 loss_train: 0.4977 acc_train: 0.9286 loss_val: 0.7497 acc_val: 0.8200 time: 0.0113s\n",
            "Epoch: 0145 loss_train: 0.4804 acc_train: 0.9500 loss_val: 0.7484 acc_val: 0.8200 time: 0.0109s\n",
            "Epoch: 0146 loss_train: 0.5157 acc_train: 0.9143 loss_val: 0.7469 acc_val: 0.8267 time: 0.0105s\n",
            "Epoch: 0147 loss_train: 0.4478 acc_train: 0.9429 loss_val: 0.7453 acc_val: 0.8300 time: 0.0147s\n",
            "Epoch: 0148 loss_train: 0.4902 acc_train: 0.9429 loss_val: 0.7430 acc_val: 0.8267 time: 0.0110s\n",
            "Epoch: 0149 loss_train: 0.4748 acc_train: 0.9571 loss_val: 0.7412 acc_val: 0.8267 time: 0.0108s\n",
            "Epoch: 0150 loss_train: 0.4540 acc_train: 0.9357 loss_val: 0.7397 acc_val: 0.8200 time: 0.0111s\n",
            "Epoch: 0151 loss_train: 0.4721 acc_train: 0.9286 loss_val: 0.7375 acc_val: 0.8267 time: 0.0117s\n",
            "Epoch: 0152 loss_train: 0.4607 acc_train: 0.9214 loss_val: 0.7361 acc_val: 0.8267 time: 0.0119s\n",
            "Epoch: 0153 loss_train: 0.4543 acc_train: 0.9286 loss_val: 0.7352 acc_val: 0.8267 time: 0.0110s\n",
            "Epoch: 0154 loss_train: 0.4924 acc_train: 0.9071 loss_val: 0.7340 acc_val: 0.8233 time: 0.0104s\n",
            "Epoch: 0155 loss_train: 0.4326 acc_train: 0.9286 loss_val: 0.7321 acc_val: 0.8200 time: 0.0116s\n",
            "Epoch: 0156 loss_train: 0.4732 acc_train: 0.9286 loss_val: 0.7301 acc_val: 0.8200 time: 0.0106s\n",
            "Epoch: 0157 loss_train: 0.4772 acc_train: 0.9500 loss_val: 0.7283 acc_val: 0.8200 time: 0.0174s\n",
            "Epoch: 0158 loss_train: 0.5277 acc_train: 0.8857 loss_val: 0.7263 acc_val: 0.8200 time: 0.0105s\n",
            "Epoch: 0159 loss_train: 0.4750 acc_train: 0.9429 loss_val: 0.7247 acc_val: 0.8167 time: 0.0113s\n",
            "Epoch: 0160 loss_train: 0.4695 acc_train: 0.9286 loss_val: 0.7235 acc_val: 0.8167 time: 0.0111s\n",
            "Epoch: 0161 loss_train: 0.4302 acc_train: 0.9571 loss_val: 0.7224 acc_val: 0.8167 time: 0.0102s\n",
            "Epoch: 0162 loss_train: 0.4480 acc_train: 0.9286 loss_val: 0.7213 acc_val: 0.8167 time: 0.0111s\n",
            "Epoch: 0163 loss_train: 0.4547 acc_train: 0.9214 loss_val: 0.7189 acc_val: 0.8167 time: 0.0114s\n",
            "Epoch: 0164 loss_train: 0.4572 acc_train: 0.9357 loss_val: 0.7168 acc_val: 0.8167 time: 0.0120s\n",
            "Epoch: 0165 loss_train: 0.4498 acc_train: 0.9429 loss_val: 0.7149 acc_val: 0.8167 time: 0.0190s\n",
            "Epoch: 0166 loss_train: 0.4179 acc_train: 0.9357 loss_val: 0.7140 acc_val: 0.8167 time: 0.0111s\n",
            "Epoch: 0167 loss_train: 0.4472 acc_train: 0.9500 loss_val: 0.7137 acc_val: 0.8133 time: 0.0114s\n",
            "Epoch: 0168 loss_train: 0.4539 acc_train: 0.9214 loss_val: 0.7144 acc_val: 0.8133 time: 0.0120s\n",
            "Epoch: 0169 loss_train: 0.4195 acc_train: 0.9286 loss_val: 0.7143 acc_val: 0.8167 time: 0.0118s\n",
            "Epoch: 0170 loss_train: 0.4289 acc_train: 0.9786 loss_val: 0.7133 acc_val: 0.8167 time: 0.0122s\n",
            "Epoch: 0171 loss_train: 0.4306 acc_train: 0.9071 loss_val: 0.7121 acc_val: 0.8233 time: 0.0114s\n",
            "Epoch: 0172 loss_train: 0.4339 acc_train: 0.9286 loss_val: 0.7107 acc_val: 0.8267 time: 0.0117s\n",
            "Epoch: 0173 loss_train: 0.4333 acc_train: 0.9500 loss_val: 0.7092 acc_val: 0.8233 time: 0.0145s\n",
            "Epoch: 0174 loss_train: 0.4462 acc_train: 0.9286 loss_val: 0.7063 acc_val: 0.8267 time: 0.0156s\n",
            "Epoch: 0175 loss_train: 0.4164 acc_train: 0.9214 loss_val: 0.7040 acc_val: 0.8100 time: 0.0115s\n",
            "Epoch: 0176 loss_train: 0.4292 acc_train: 0.9500 loss_val: 0.7018 acc_val: 0.8133 time: 0.0110s\n",
            "Epoch: 0177 loss_train: 0.4256 acc_train: 0.9643 loss_val: 0.6995 acc_val: 0.8133 time: 0.0115s\n",
            "Epoch: 0178 loss_train: 0.4421 acc_train: 0.9357 loss_val: 0.6975 acc_val: 0.8167 time: 0.0115s\n",
            "Epoch: 0179 loss_train: 0.4053 acc_train: 0.9286 loss_val: 0.6966 acc_val: 0.8167 time: 0.0112s\n",
            "Epoch: 0180 loss_train: 0.3876 acc_train: 0.9571 loss_val: 0.6961 acc_val: 0.8167 time: 0.0103s\n",
            "Epoch: 0181 loss_train: 0.3824 acc_train: 0.9500 loss_val: 0.6970 acc_val: 0.8200 time: 0.0108s\n",
            "Epoch: 0182 loss_train: 0.4063 acc_train: 0.9429 loss_val: 0.6984 acc_val: 0.8100 time: 0.0133s\n",
            "Epoch: 0183 loss_train: 0.4189 acc_train: 0.9500 loss_val: 0.7012 acc_val: 0.8100 time: 0.0121s\n",
            "Epoch: 0184 loss_train: 0.3716 acc_train: 0.9429 loss_val: 0.7020 acc_val: 0.8100 time: 0.0113s\n",
            "Epoch: 0185 loss_train: 0.4648 acc_train: 0.9286 loss_val: 0.7007 acc_val: 0.8133 time: 0.0113s\n",
            "Epoch: 0186 loss_train: 0.3928 acc_train: 0.9643 loss_val: 0.6990 acc_val: 0.8100 time: 0.0108s\n",
            "Epoch: 0187 loss_train: 0.4927 acc_train: 0.9143 loss_val: 0.6961 acc_val: 0.8133 time: 0.0116s\n",
            "Epoch: 0188 loss_train: 0.4034 acc_train: 0.9357 loss_val: 0.6945 acc_val: 0.8133 time: 0.0112s\n",
            "Epoch: 0189 loss_train: 0.4098 acc_train: 0.9429 loss_val: 0.6932 acc_val: 0.8133 time: 0.0108s\n",
            "Epoch: 0190 loss_train: 0.3935 acc_train: 0.9286 loss_val: 0.6920 acc_val: 0.8167 time: 0.0134s\n",
            "Epoch: 0191 loss_train: 0.4045 acc_train: 0.9429 loss_val: 0.6903 acc_val: 0.8167 time: 0.0161s\n",
            "Epoch: 0192 loss_train: 0.4036 acc_train: 0.9500 loss_val: 0.6901 acc_val: 0.8167 time: 0.0101s\n",
            "Epoch: 0193 loss_train: 0.3791 acc_train: 0.9571 loss_val: 0.6912 acc_val: 0.8167 time: 0.0119s\n",
            "Epoch: 0194 loss_train: 0.3852 acc_train: 0.9357 loss_val: 0.6937 acc_val: 0.8167 time: 0.0114s\n",
            "Epoch: 0195 loss_train: 0.3784 acc_train: 0.9286 loss_val: 0.6956 acc_val: 0.8133 time: 0.0113s\n",
            "Epoch: 0196 loss_train: 0.3856 acc_train: 0.9500 loss_val: 0.6966 acc_val: 0.8133 time: 0.0113s\n",
            "Epoch: 0197 loss_train: 0.4165 acc_train: 0.9286 loss_val: 0.6962 acc_val: 0.8067 time: 0.0128s\n",
            "Epoch: 0198 loss_train: 0.3767 acc_train: 0.9500 loss_val: 0.6917 acc_val: 0.8067 time: 0.0122s\n",
            "Epoch: 0199 loss_train: 0.3871 acc_train: 0.9571 loss_val: 0.6883 acc_val: 0.8167 time: 0.0113s\n",
            "Epoch: 0200 loss_train: 0.3747 acc_train: 0.9500 loss_val: 0.6853 acc_val: 0.8167 time: 0.0114s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 2.8100s\n",
            "Test set results: loss= 0.7182 accuracy= 0.8390\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SOy5cYIAmV6",
        "outputId": "88bb2df8-8fb5-424d-f1c2-d159f5c3fc46"
      },
      "source": [
        "#(1.4) For the train/val/test dataset, use a random split of 70%/10%/20% instead of the original split, and redo (1)\n",
        "    # idx_train = range(1050)\n",
        "    # idx_val = range(1050, 1200)\n",
        "    # idx_test = range(1200, 1500)\n",
        "%run pygcn/train.py\n",
        "#Observation : Accuracy increased from 1.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cora dataset...\n",
            "Epoch: 0001 loss_train: 1.9615 acc_train: 0.1286 loss_val: 1.9647 acc_val: 0.1800 time: 0.0234s\n",
            "Epoch: 0002 loss_train: 1.9472 acc_train: 0.1295 loss_val: 1.9575 acc_val: 0.1800 time: 0.0144s\n",
            "Epoch: 0003 loss_train: 1.9349 acc_train: 0.1305 loss_val: 1.9506 acc_val: 0.1800 time: 0.0179s\n",
            "Epoch: 0004 loss_train: 1.9249 acc_train: 0.1533 loss_val: 1.9443 acc_val: 0.1933 time: 0.0168s\n",
            "Epoch: 0005 loss_train: 1.9128 acc_train: 0.1943 loss_val: 1.9381 acc_val: 0.2933 time: 0.0176s\n",
            "Epoch: 0006 loss_train: 1.8942 acc_train: 0.2790 loss_val: 1.9316 acc_val: 0.2800 time: 0.0176s\n",
            "Epoch: 0007 loss_train: 1.8862 acc_train: 0.3219 loss_val: 1.9246 acc_val: 0.2800 time: 0.0221s\n",
            "Epoch: 0008 loss_train: 1.8748 acc_train: 0.3257 loss_val: 1.9176 acc_val: 0.2800 time: 0.0164s\n",
            "Epoch: 0009 loss_train: 1.8640 acc_train: 0.3238 loss_val: 1.9103 acc_val: 0.2800 time: 0.0134s\n",
            "Epoch: 0010 loss_train: 1.8621 acc_train: 0.3238 loss_val: 1.9031 acc_val: 0.2800 time: 0.0145s\n",
            "Epoch: 0011 loss_train: 1.8545 acc_train: 0.3248 loss_val: 1.8961 acc_val: 0.2800 time: 0.0138s\n",
            "Epoch: 0012 loss_train: 1.8475 acc_train: 0.3276 loss_val: 1.8892 acc_val: 0.2800 time: 0.0155s\n",
            "Epoch: 0013 loss_train: 1.8320 acc_train: 0.3248 loss_val: 1.8824 acc_val: 0.2800 time: 0.0192s\n",
            "Epoch: 0014 loss_train: 1.8098 acc_train: 0.3295 loss_val: 1.8759 acc_val: 0.2800 time: 0.0154s\n",
            "Epoch: 0015 loss_train: 1.8133 acc_train: 0.3248 loss_val: 1.8697 acc_val: 0.2800 time: 0.0141s\n",
            "Epoch: 0016 loss_train: 1.8139 acc_train: 0.3257 loss_val: 1.8638 acc_val: 0.2800 time: 0.0141s\n",
            "Epoch: 0017 loss_train: 1.8027 acc_train: 0.3257 loss_val: 1.8579 acc_val: 0.2800 time: 0.0181s\n",
            "Epoch: 0018 loss_train: 1.7892 acc_train: 0.3248 loss_val: 1.8525 acc_val: 0.2800 time: 0.0197s\n",
            "Epoch: 0019 loss_train: 1.7914 acc_train: 0.3248 loss_val: 1.8473 acc_val: 0.2800 time: 0.0172s\n",
            "Epoch: 0020 loss_train: 1.7807 acc_train: 0.3248 loss_val: 1.8422 acc_val: 0.2800 time: 0.0165s\n",
            "Epoch: 0021 loss_train: 1.7554 acc_train: 0.3248 loss_val: 1.8373 acc_val: 0.2800 time: 0.0161s\n",
            "Epoch: 0022 loss_train: 1.7559 acc_train: 0.3257 loss_val: 1.8325 acc_val: 0.2800 time: 0.0171s\n",
            "Epoch: 0023 loss_train: 1.7579 acc_train: 0.3248 loss_val: 1.8278 acc_val: 0.2800 time: 0.0167s\n",
            "Epoch: 0024 loss_train: 1.7473 acc_train: 0.3248 loss_val: 1.8232 acc_val: 0.2800 time: 0.0294s\n",
            "Epoch: 0025 loss_train: 1.7509 acc_train: 0.3248 loss_val: 1.8181 acc_val: 0.2800 time: 0.0159s\n",
            "Epoch: 0026 loss_train: 1.7331 acc_train: 0.3248 loss_val: 1.8127 acc_val: 0.2800 time: 0.0142s\n",
            "Epoch: 0027 loss_train: 1.7291 acc_train: 0.3248 loss_val: 1.8070 acc_val: 0.2800 time: 0.0182s\n",
            "Epoch: 0028 loss_train: 1.7309 acc_train: 0.3257 loss_val: 1.8009 acc_val: 0.2800 time: 0.0154s\n",
            "Epoch: 0029 loss_train: 1.7296 acc_train: 0.3267 loss_val: 1.7943 acc_val: 0.2800 time: 0.0130s\n",
            "Epoch: 0030 loss_train: 1.7209 acc_train: 0.3324 loss_val: 1.7874 acc_val: 0.2800 time: 0.0143s\n",
            "Epoch: 0031 loss_train: 1.7344 acc_train: 0.3257 loss_val: 1.7803 acc_val: 0.2800 time: 0.0153s\n",
            "Epoch: 0032 loss_train: 1.7017 acc_train: 0.3305 loss_val: 1.7730 acc_val: 0.2800 time: 0.0229s\n",
            "Epoch: 0033 loss_train: 1.6994 acc_train: 0.3267 loss_val: 1.7656 acc_val: 0.2800 time: 0.0171s\n",
            "Epoch: 0034 loss_train: 1.6828 acc_train: 0.3305 loss_val: 1.7584 acc_val: 0.2800 time: 0.0177s\n",
            "Epoch: 0035 loss_train: 1.6774 acc_train: 0.3333 loss_val: 1.7511 acc_val: 0.2800 time: 0.0201s\n",
            "Epoch: 0036 loss_train: 1.6751 acc_train: 0.3410 loss_val: 1.7436 acc_val: 0.2800 time: 0.0221s\n",
            "Epoch: 0037 loss_train: 1.6700 acc_train: 0.3333 loss_val: 1.7363 acc_val: 0.2800 time: 0.0190s\n",
            "Epoch: 0038 loss_train: 1.6586 acc_train: 0.3324 loss_val: 1.7290 acc_val: 0.2800 time: 0.0205s\n",
            "Epoch: 0039 loss_train: 1.6513 acc_train: 0.3400 loss_val: 1.7209 acc_val: 0.2800 time: 0.0204s\n",
            "Epoch: 0040 loss_train: 1.6504 acc_train: 0.3410 loss_val: 1.7122 acc_val: 0.2800 time: 0.0207s\n",
            "Epoch: 0041 loss_train: 1.6426 acc_train: 0.3362 loss_val: 1.7033 acc_val: 0.2800 time: 0.0171s\n",
            "Epoch: 0042 loss_train: 1.6255 acc_train: 0.3429 loss_val: 1.6943 acc_val: 0.2800 time: 0.0172s\n",
            "Epoch: 0043 loss_train: 1.6109 acc_train: 0.3400 loss_val: 1.6852 acc_val: 0.2800 time: 0.0172s\n",
            "Epoch: 0044 loss_train: 1.6064 acc_train: 0.3429 loss_val: 1.6758 acc_val: 0.2800 time: 0.0189s\n",
            "Epoch: 0045 loss_train: 1.5998 acc_train: 0.3390 loss_val: 1.6663 acc_val: 0.2800 time: 0.0242s\n",
            "Epoch: 0046 loss_train: 1.5938 acc_train: 0.3410 loss_val: 1.6565 acc_val: 0.2800 time: 0.0201s\n",
            "Epoch: 0047 loss_train: 1.5673 acc_train: 0.3505 loss_val: 1.6465 acc_val: 0.2800 time: 0.0179s\n",
            "Epoch: 0048 loss_train: 1.5558 acc_train: 0.3448 loss_val: 1.6358 acc_val: 0.2800 time: 0.0183s\n",
            "Epoch: 0049 loss_train: 1.5441 acc_train: 0.3590 loss_val: 1.6249 acc_val: 0.2800 time: 0.0172s\n",
            "Epoch: 0050 loss_train: 1.5364 acc_train: 0.3552 loss_val: 1.6141 acc_val: 0.2867 time: 0.0201s\n",
            "Epoch: 0051 loss_train: 1.5261 acc_train: 0.3695 loss_val: 1.6024 acc_val: 0.2867 time: 0.0195s\n",
            "Epoch: 0052 loss_train: 1.5139 acc_train: 0.3552 loss_val: 1.5894 acc_val: 0.2933 time: 0.0185s\n",
            "Epoch: 0053 loss_train: 1.4871 acc_train: 0.3981 loss_val: 1.5760 acc_val: 0.3133 time: 0.0185s\n",
            "Epoch: 0054 loss_train: 1.4974 acc_train: 0.3990 loss_val: 1.5621 acc_val: 0.3400 time: 0.0163s\n",
            "Epoch: 0055 loss_train: 1.4782 acc_train: 0.3990 loss_val: 1.5482 acc_val: 0.3600 time: 0.0269s\n",
            "Epoch: 0056 loss_train: 1.4549 acc_train: 0.4114 loss_val: 1.5341 acc_val: 0.3733 time: 0.0164s\n",
            "Epoch: 0057 loss_train: 1.4304 acc_train: 0.4438 loss_val: 1.5198 acc_val: 0.4000 time: 0.0178s\n",
            "Epoch: 0058 loss_train: 1.4318 acc_train: 0.4581 loss_val: 1.5049 acc_val: 0.4267 time: 0.0170s\n",
            "Epoch: 0059 loss_train: 1.4333 acc_train: 0.4562 loss_val: 1.4906 acc_val: 0.4333 time: 0.0200s\n",
            "Epoch: 0060 loss_train: 1.4078 acc_train: 0.4762 loss_val: 1.4764 acc_val: 0.4533 time: 0.0203s\n",
            "Epoch: 0061 loss_train: 1.3997 acc_train: 0.4848 loss_val: 1.4619 acc_val: 0.4667 time: 0.0186s\n",
            "Epoch: 0062 loss_train: 1.3737 acc_train: 0.5010 loss_val: 1.4465 acc_val: 0.4933 time: 0.0182s\n",
            "Epoch: 0063 loss_train: 1.3487 acc_train: 0.5162 loss_val: 1.4303 acc_val: 0.5267 time: 0.0220s\n",
            "Epoch: 0064 loss_train: 1.3182 acc_train: 0.5752 loss_val: 1.4142 acc_val: 0.5467 time: 0.0171s\n",
            "Epoch: 0065 loss_train: 1.3291 acc_train: 0.5848 loss_val: 1.3985 acc_val: 0.5733 time: 0.0175s\n",
            "Epoch: 0066 loss_train: 1.3144 acc_train: 0.5971 loss_val: 1.3837 acc_val: 0.5933 time: 0.0211s\n",
            "Epoch: 0067 loss_train: 1.2845 acc_train: 0.6286 loss_val: 1.3687 acc_val: 0.6067 time: 0.0176s\n",
            "Epoch: 0068 loss_train: 1.2860 acc_train: 0.6219 loss_val: 1.3539 acc_val: 0.6133 time: 0.0158s\n",
            "Epoch: 0069 loss_train: 1.2616 acc_train: 0.6390 loss_val: 1.3386 acc_val: 0.6200 time: 0.0176s\n",
            "Epoch: 0070 loss_train: 1.2504 acc_train: 0.6438 loss_val: 1.3228 acc_val: 0.6267 time: 0.0172s\n",
            "Epoch: 0071 loss_train: 1.2380 acc_train: 0.6248 loss_val: 1.3072 acc_val: 0.6267 time: 0.0159s\n",
            "Epoch: 0072 loss_train: 1.2315 acc_train: 0.6543 loss_val: 1.2910 acc_val: 0.6533 time: 0.0188s\n",
            "Epoch: 0073 loss_train: 1.2069 acc_train: 0.6562 loss_val: 1.2748 acc_val: 0.6667 time: 0.0140s\n",
            "Epoch: 0074 loss_train: 1.2011 acc_train: 0.6905 loss_val: 1.2588 acc_val: 0.6733 time: 0.0136s\n",
            "Epoch: 0075 loss_train: 1.2026 acc_train: 0.6724 loss_val: 1.2433 acc_val: 0.7067 time: 0.0221s\n",
            "Epoch: 0076 loss_train: 1.1739 acc_train: 0.7114 loss_val: 1.2302 acc_val: 0.7133 time: 0.0252s\n",
            "Epoch: 0077 loss_train: 1.1685 acc_train: 0.7105 loss_val: 1.2184 acc_val: 0.7133 time: 0.0259s\n",
            "Epoch: 0078 loss_train: 1.1457 acc_train: 0.7133 loss_val: 1.2071 acc_val: 0.7267 time: 0.0186s\n",
            "Epoch: 0079 loss_train: 1.1437 acc_train: 0.6952 loss_val: 1.1953 acc_val: 0.7400 time: 0.0189s\n",
            "Epoch: 0080 loss_train: 1.1372 acc_train: 0.7390 loss_val: 1.1844 acc_val: 0.7400 time: 0.0180s\n",
            "Epoch: 0081 loss_train: 1.1209 acc_train: 0.7410 loss_val: 1.1725 acc_val: 0.7400 time: 0.0173s\n",
            "Epoch: 0082 loss_train: 1.1218 acc_train: 0.7333 loss_val: 1.1595 acc_val: 0.7400 time: 0.0181s\n",
            "Epoch: 0083 loss_train: 1.1233 acc_train: 0.7476 loss_val: 1.1474 acc_val: 0.7400 time: 0.0184s\n",
            "Epoch: 0084 loss_train: 1.0912 acc_train: 0.7238 loss_val: 1.1347 acc_val: 0.7467 time: 0.0200s\n",
            "Epoch: 0085 loss_train: 1.0663 acc_train: 0.7533 loss_val: 1.1211 acc_val: 0.7600 time: 0.0214s\n",
            "Epoch: 0086 loss_train: 1.0432 acc_train: 0.7686 loss_val: 1.1075 acc_val: 0.7667 time: 0.0164s\n",
            "Epoch: 0087 loss_train: 1.0637 acc_train: 0.7543 loss_val: 1.0955 acc_val: 0.7667 time: 0.0159s\n",
            "Epoch: 0088 loss_train: 1.0436 acc_train: 0.7619 loss_val: 1.0840 acc_val: 0.7733 time: 0.0298s\n",
            "Epoch: 0089 loss_train: 1.0304 acc_train: 0.7571 loss_val: 1.0737 acc_val: 0.7733 time: 0.0220s\n",
            "Epoch: 0090 loss_train: 1.0085 acc_train: 0.7771 loss_val: 1.0642 acc_val: 0.7733 time: 0.0177s\n",
            "Epoch: 0091 loss_train: 1.0070 acc_train: 0.7581 loss_val: 1.0547 acc_val: 0.7867 time: 0.0177s\n",
            "Epoch: 0092 loss_train: 0.9997 acc_train: 0.7562 loss_val: 1.0430 acc_val: 0.7867 time: 0.0163s\n",
            "Epoch: 0093 loss_train: 1.0057 acc_train: 0.7562 loss_val: 1.0318 acc_val: 0.7933 time: 0.0205s\n",
            "Epoch: 0094 loss_train: 0.9818 acc_train: 0.7648 loss_val: 1.0210 acc_val: 0.8000 time: 0.0161s\n",
            "Epoch: 0095 loss_train: 1.0021 acc_train: 0.7648 loss_val: 1.0113 acc_val: 0.8067 time: 0.0170s\n",
            "Epoch: 0096 loss_train: 0.9648 acc_train: 0.7876 loss_val: 1.0023 acc_val: 0.8067 time: 0.0175s\n",
            "Epoch: 0097 loss_train: 0.9719 acc_train: 0.7724 loss_val: 0.9940 acc_val: 0.8067 time: 0.0159s\n",
            "Epoch: 0098 loss_train: 0.9447 acc_train: 0.7933 loss_val: 0.9866 acc_val: 0.8067 time: 0.0166s\n",
            "Epoch: 0099 loss_train: 0.9525 acc_train: 0.7629 loss_val: 0.9793 acc_val: 0.7933 time: 0.0222s\n",
            "Epoch: 0100 loss_train: 0.9337 acc_train: 0.7762 loss_val: 0.9706 acc_val: 0.7933 time: 0.0163s\n",
            "Epoch: 0101 loss_train: 0.9433 acc_train: 0.7800 loss_val: 0.9613 acc_val: 0.8000 time: 0.0173s\n",
            "Epoch: 0102 loss_train: 0.9212 acc_train: 0.7667 loss_val: 0.9507 acc_val: 0.8067 time: 0.0176s\n",
            "Epoch: 0103 loss_train: 0.8764 acc_train: 0.7895 loss_val: 0.9402 acc_val: 0.8067 time: 0.0179s\n",
            "Epoch: 0104 loss_train: 0.8987 acc_train: 0.7886 loss_val: 0.9308 acc_val: 0.8133 time: 0.0192s\n",
            "Epoch: 0105 loss_train: 0.8867 acc_train: 0.7952 loss_val: 0.9235 acc_val: 0.8133 time: 0.0198s\n",
            "Epoch: 0106 loss_train: 0.8938 acc_train: 0.8010 loss_val: 0.9162 acc_val: 0.8133 time: 0.0176s\n",
            "Epoch: 0107 loss_train: 0.8738 acc_train: 0.7981 loss_val: 0.9095 acc_val: 0.8133 time: 0.0216s\n",
            "Epoch: 0108 loss_train: 0.8777 acc_train: 0.7905 loss_val: 0.9031 acc_val: 0.8133 time: 0.0157s\n",
            "Epoch: 0109 loss_train: 0.8680 acc_train: 0.7933 loss_val: 0.8962 acc_val: 0.8067 time: 0.0168s\n",
            "Epoch: 0110 loss_train: 0.8669 acc_train: 0.7933 loss_val: 0.8877 acc_val: 0.8067 time: 0.0329s\n",
            "Epoch: 0111 loss_train: 0.8237 acc_train: 0.8038 loss_val: 0.8755 acc_val: 0.8067 time: 0.0187s\n",
            "Epoch: 0112 loss_train: 0.8484 acc_train: 0.8105 loss_val: 0.8641 acc_val: 0.8200 time: 0.0178s\n",
            "Epoch: 0113 loss_train: 0.8541 acc_train: 0.8019 loss_val: 0.8544 acc_val: 0.8267 time: 0.0182s\n",
            "Epoch: 0114 loss_train: 0.8188 acc_train: 0.8086 loss_val: 0.8452 acc_val: 0.8267 time: 0.0207s\n",
            "Epoch: 0115 loss_train: 0.8255 acc_train: 0.7981 loss_val: 0.8368 acc_val: 0.8267 time: 0.0198s\n",
            "Epoch: 0116 loss_train: 0.8358 acc_train: 0.8095 loss_val: 0.8310 acc_val: 0.8200 time: 0.0207s\n",
            "Epoch: 0117 loss_train: 0.8239 acc_train: 0.8038 loss_val: 0.8275 acc_val: 0.8133 time: 0.0181s\n",
            "Epoch: 0118 loss_train: 0.8320 acc_train: 0.8000 loss_val: 0.8247 acc_val: 0.8133 time: 0.0191s\n",
            "Epoch: 0119 loss_train: 0.7924 acc_train: 0.8190 loss_val: 0.8223 acc_val: 0.8000 time: 0.0179s\n",
            "Epoch: 0120 loss_train: 0.7780 acc_train: 0.8010 loss_val: 0.8170 acc_val: 0.8000 time: 0.0174s\n",
            "Epoch: 0121 loss_train: 0.8036 acc_train: 0.8019 loss_val: 0.8096 acc_val: 0.8067 time: 0.0212s\n",
            "Epoch: 0122 loss_train: 0.7821 acc_train: 0.8086 loss_val: 0.8020 acc_val: 0.8133 time: 0.0166s\n",
            "Epoch: 0123 loss_train: 0.7875 acc_train: 0.8086 loss_val: 0.7952 acc_val: 0.8200 time: 0.0180s\n",
            "Epoch: 0124 loss_train: 0.7640 acc_train: 0.8219 loss_val: 0.7888 acc_val: 0.8200 time: 0.0177s\n",
            "Epoch: 0125 loss_train: 0.7786 acc_train: 0.8133 loss_val: 0.7833 acc_val: 0.8200 time: 0.0179s\n",
            "Epoch: 0126 loss_train: 0.7559 acc_train: 0.8190 loss_val: 0.7785 acc_val: 0.8200 time: 0.0231s\n",
            "Epoch: 0127 loss_train: 0.7698 acc_train: 0.8229 loss_val: 0.7740 acc_val: 0.8200 time: 0.0165s\n",
            "Epoch: 0128 loss_train: 0.7679 acc_train: 0.8095 loss_val: 0.7706 acc_val: 0.8133 time: 0.0170s\n",
            "Epoch: 0129 loss_train: 0.7532 acc_train: 0.8229 loss_val: 0.7669 acc_val: 0.8133 time: 0.0182s\n",
            "Epoch: 0130 loss_train: 0.7275 acc_train: 0.8276 loss_val: 0.7621 acc_val: 0.8133 time: 0.0171s\n",
            "Epoch: 0131 loss_train: 0.7373 acc_train: 0.8219 loss_val: 0.7554 acc_val: 0.8133 time: 0.0171s\n",
            "Epoch: 0132 loss_train: 0.7168 acc_train: 0.8181 loss_val: 0.7483 acc_val: 0.8133 time: 0.0219s\n",
            "Epoch: 0133 loss_train: 0.7142 acc_train: 0.8238 loss_val: 0.7418 acc_val: 0.8200 time: 0.0171s\n",
            "Epoch: 0134 loss_train: 0.7221 acc_train: 0.8276 loss_val: 0.7365 acc_val: 0.8333 time: 0.0175s\n",
            "Epoch: 0135 loss_train: 0.7077 acc_train: 0.8352 loss_val: 0.7318 acc_val: 0.8267 time: 0.0187s\n",
            "Epoch: 0136 loss_train: 0.7357 acc_train: 0.8371 loss_val: 0.7296 acc_val: 0.8333 time: 0.0210s\n",
            "Epoch: 0137 loss_train: 0.6951 acc_train: 0.8390 loss_val: 0.7271 acc_val: 0.8333 time: 0.0173s\n",
            "Epoch: 0138 loss_train: 0.7079 acc_train: 0.8286 loss_val: 0.7257 acc_val: 0.8200 time: 0.0193s\n",
            "Epoch: 0139 loss_train: 0.7149 acc_train: 0.8305 loss_val: 0.7248 acc_val: 0.8133 time: 0.0185s\n",
            "Epoch: 0140 loss_train: 0.7161 acc_train: 0.8133 loss_val: 0.7252 acc_val: 0.8067 time: 0.0155s\n",
            "Epoch: 0141 loss_train: 0.6855 acc_train: 0.8390 loss_val: 0.7249 acc_val: 0.8133 time: 0.0169s\n",
            "Epoch: 0142 loss_train: 0.7030 acc_train: 0.8390 loss_val: 0.7216 acc_val: 0.8133 time: 0.0185s\n",
            "Epoch: 0143 loss_train: 0.6950 acc_train: 0.8314 loss_val: 0.7160 acc_val: 0.8133 time: 0.0228s\n",
            "Epoch: 0144 loss_train: 0.6735 acc_train: 0.8410 loss_val: 0.7065 acc_val: 0.8200 time: 0.0174s\n",
            "Epoch: 0145 loss_train: 0.6950 acc_train: 0.8438 loss_val: 0.6990 acc_val: 0.8267 time: 0.0192s\n",
            "Epoch: 0146 loss_train: 0.6848 acc_train: 0.8305 loss_val: 0.6933 acc_val: 0.8333 time: 0.0203s\n",
            "Epoch: 0147 loss_train: 0.6705 acc_train: 0.8610 loss_val: 0.6892 acc_val: 0.8333 time: 0.0175s\n",
            "Epoch: 0148 loss_train: 0.6514 acc_train: 0.8581 loss_val: 0.6861 acc_val: 0.8333 time: 0.0207s\n",
            "Epoch: 0149 loss_train: 0.6714 acc_train: 0.8533 loss_val: 0.6839 acc_val: 0.8400 time: 0.0178s\n",
            "Epoch: 0150 loss_train: 0.6594 acc_train: 0.8552 loss_val: 0.6796 acc_val: 0.8333 time: 0.0188s\n",
            "Epoch: 0151 loss_train: 0.6630 acc_train: 0.8362 loss_val: 0.6755 acc_val: 0.8333 time: 0.0212s\n",
            "Epoch: 0152 loss_train: 0.6709 acc_train: 0.8343 loss_val: 0.6702 acc_val: 0.8333 time: 0.0173s\n",
            "Epoch: 0153 loss_train: 0.6706 acc_train: 0.8505 loss_val: 0.6671 acc_val: 0.8200 time: 0.0217s\n",
            "Epoch: 0154 loss_train: 0.6501 acc_train: 0.8381 loss_val: 0.6652 acc_val: 0.8200 time: 0.0216s\n",
            "Epoch: 0155 loss_train: 0.6621 acc_train: 0.8343 loss_val: 0.6640 acc_val: 0.8200 time: 0.0159s\n",
            "Epoch: 0156 loss_train: 0.6495 acc_train: 0.8352 loss_val: 0.6630 acc_val: 0.8200 time: 0.0165s\n",
            "Epoch: 0157 loss_train: 0.6587 acc_train: 0.8390 loss_val: 0.6627 acc_val: 0.8333 time: 0.0179s\n",
            "Epoch: 0158 loss_train: 0.6458 acc_train: 0.8390 loss_val: 0.6630 acc_val: 0.8600 time: 0.0161s\n",
            "Epoch: 0159 loss_train: 0.6369 acc_train: 0.8552 loss_val: 0.6621 acc_val: 0.8600 time: 0.0226s\n",
            "Epoch: 0160 loss_train: 0.6712 acc_train: 0.8419 loss_val: 0.6603 acc_val: 0.8600 time: 0.0230s\n",
            "Epoch: 0161 loss_train: 0.6081 acc_train: 0.8657 loss_val: 0.6565 acc_val: 0.8467 time: 0.0196s\n",
            "Epoch: 0162 loss_train: 0.6514 acc_train: 0.8495 loss_val: 0.6520 acc_val: 0.8400 time: 0.0191s\n",
            "Epoch: 0163 loss_train: 0.6352 acc_train: 0.8524 loss_val: 0.6479 acc_val: 0.8400 time: 0.0202s\n",
            "Epoch: 0164 loss_train: 0.6443 acc_train: 0.8429 loss_val: 0.6438 acc_val: 0.8400 time: 0.0242s\n",
            "Epoch: 0165 loss_train: 0.6256 acc_train: 0.8562 loss_val: 0.6407 acc_val: 0.8533 time: 0.0199s\n",
            "Epoch: 0166 loss_train: 0.6246 acc_train: 0.8714 loss_val: 0.6387 acc_val: 0.8533 time: 0.0186s\n",
            "Epoch: 0167 loss_train: 0.6190 acc_train: 0.8657 loss_val: 0.6388 acc_val: 0.8400 time: 0.0194s\n",
            "Epoch: 0168 loss_train: 0.6454 acc_train: 0.8457 loss_val: 0.6383 acc_val: 0.8467 time: 0.0212s\n",
            "Epoch: 0169 loss_train: 0.6127 acc_train: 0.8743 loss_val: 0.6362 acc_val: 0.8667 time: 0.0200s\n",
            "Epoch: 0170 loss_train: 0.6227 acc_train: 0.8390 loss_val: 0.6323 acc_val: 0.8600 time: 0.0183s\n",
            "Epoch: 0171 loss_train: 0.6260 acc_train: 0.8457 loss_val: 0.6268 acc_val: 0.8600 time: 0.0177s\n",
            "Epoch: 0172 loss_train: 0.5970 acc_train: 0.8552 loss_val: 0.6228 acc_val: 0.8600 time: 0.0184s\n",
            "Epoch: 0173 loss_train: 0.6361 acc_train: 0.8562 loss_val: 0.6203 acc_val: 0.8467 time: 0.0154s\n",
            "Epoch: 0174 loss_train: 0.5968 acc_train: 0.8733 loss_val: 0.6187 acc_val: 0.8400 time: 0.0226s\n",
            "Epoch: 0175 loss_train: 0.5895 acc_train: 0.8752 loss_val: 0.6174 acc_val: 0.8400 time: 0.0179s\n",
            "Epoch: 0176 loss_train: 0.6148 acc_train: 0.8524 loss_val: 0.6175 acc_val: 0.8400 time: 0.0176s\n",
            "Epoch: 0177 loss_train: 0.5827 acc_train: 0.8733 loss_val: 0.6161 acc_val: 0.8400 time: 0.0177s\n",
            "Epoch: 0178 loss_train: 0.6088 acc_train: 0.8629 loss_val: 0.6144 acc_val: 0.8533 time: 0.0177s\n",
            "Epoch: 0179 loss_train: 0.5964 acc_train: 0.8562 loss_val: 0.6126 acc_val: 0.8533 time: 0.0230s\n",
            "Epoch: 0180 loss_train: 0.5935 acc_train: 0.8543 loss_val: 0.6109 acc_val: 0.8600 time: 0.0162s\n",
            "Epoch: 0181 loss_train: 0.5847 acc_train: 0.8686 loss_val: 0.6078 acc_val: 0.8667 time: 0.0175s\n",
            "Epoch: 0182 loss_train: 0.5773 acc_train: 0.8619 loss_val: 0.6046 acc_val: 0.8600 time: 0.0172s\n",
            "Epoch: 0183 loss_train: 0.5736 acc_train: 0.8619 loss_val: 0.6024 acc_val: 0.8667 time: 0.0172s\n",
            "Epoch: 0184 loss_train: 0.6028 acc_train: 0.8686 loss_val: 0.6019 acc_val: 0.8600 time: 0.0183s\n",
            "Epoch: 0185 loss_train: 0.5948 acc_train: 0.8610 loss_val: 0.6021 acc_val: 0.8600 time: 0.0225s\n",
            "Epoch: 0186 loss_train: 0.5780 acc_train: 0.8648 loss_val: 0.6010 acc_val: 0.8467 time: 0.0182s\n",
            "Epoch: 0187 loss_train: 0.5730 acc_train: 0.8676 loss_val: 0.5990 acc_val: 0.8467 time: 0.0196s\n",
            "Epoch: 0188 loss_train: 0.5859 acc_train: 0.8629 loss_val: 0.5964 acc_val: 0.8600 time: 0.0197s\n",
            "Epoch: 0189 loss_train: 0.5754 acc_train: 0.8543 loss_val: 0.5924 acc_val: 0.8800 time: 0.0231s\n",
            "Epoch: 0190 loss_train: 0.5638 acc_train: 0.8657 loss_val: 0.5888 acc_val: 0.8800 time: 0.0154s\n",
            "Epoch: 0191 loss_train: 0.5878 acc_train: 0.8581 loss_val: 0.5866 acc_val: 0.8867 time: 0.0168s\n",
            "Epoch: 0192 loss_train: 0.5888 acc_train: 0.8705 loss_val: 0.5859 acc_val: 0.8867 time: 0.0182s\n",
            "Epoch: 0193 loss_train: 0.5634 acc_train: 0.8762 loss_val: 0.5860 acc_val: 0.8800 time: 0.0193s\n",
            "Epoch: 0194 loss_train: 0.5763 acc_train: 0.8638 loss_val: 0.5884 acc_val: 0.8800 time: 0.0230s\n",
            "Epoch: 0195 loss_train: 0.5779 acc_train: 0.8571 loss_val: 0.5897 acc_val: 0.8733 time: 0.0217s\n",
            "Epoch: 0196 loss_train: 0.5576 acc_train: 0.8733 loss_val: 0.5877 acc_val: 0.8733 time: 0.0210s\n",
            "Epoch: 0197 loss_train: 0.5542 acc_train: 0.8724 loss_val: 0.5853 acc_val: 0.8733 time: 0.0194s\n",
            "Epoch: 0198 loss_train: 0.5534 acc_train: 0.8771 loss_val: 0.5825 acc_val: 0.8667 time: 0.0179s\n",
            "Epoch: 0199 loss_train: 0.5746 acc_train: 0.8686 loss_val: 0.5799 acc_val: 0.8800 time: 0.0183s\n",
            "Epoch: 0200 loss_train: 0.5834 acc_train: 0.8619 loss_val: 0.5804 acc_val: 0.8800 time: 0.0169s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 4.0120s\n",
            "Test set results: loss= 0.6094 accuracy= 0.8567\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moJfNV8VB4qq",
        "outputId": "ea8b88e9-ed5d-4d75-fc27-0dd3aa5c3d00"
      },
      "source": [
        "#(1.5) Increase the number of layers in GCN and test their performance with 3, 4, 5, 10 and 15 layers.\n",
        "#3-layers\n",
        "%run pygcn/train.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cora dataset...\n",
            "Epoch: 0001 loss_train: 1.9614 acc_train: 0.2000 loss_val: 1.9809 acc_val: 0.1567 time: 0.0281s\n",
            "Epoch: 0002 loss_train: 1.9428 acc_train: 0.2000 loss_val: 1.9678 acc_val: 0.1567 time: 0.0195s\n",
            "Epoch: 0003 loss_train: 1.9403 acc_train: 0.2000 loss_val: 1.9559 acc_val: 0.1567 time: 0.0202s\n",
            "Epoch: 0004 loss_train: 1.9330 acc_train: 0.2000 loss_val: 1.9447 acc_val: 0.1567 time: 0.0184s\n",
            "Epoch: 0005 loss_train: 1.9221 acc_train: 0.2000 loss_val: 1.9353 acc_val: 0.1567 time: 0.0198s\n",
            "Epoch: 0006 loss_train: 1.9123 acc_train: 0.2000 loss_val: 1.9272 acc_val: 0.1567 time: 0.0208s\n",
            "Epoch: 0007 loss_train: 1.9130 acc_train: 0.2000 loss_val: 1.9191 acc_val: 0.1567 time: 0.0217s\n",
            "Epoch: 0008 loss_train: 1.9035 acc_train: 0.2000 loss_val: 1.9111 acc_val: 0.1567 time: 0.0199s\n",
            "Epoch: 0009 loss_train: 1.8976 acc_train: 0.2000 loss_val: 1.9029 acc_val: 0.1567 time: 0.0176s\n",
            "Epoch: 0010 loss_train: 1.8972 acc_train: 0.2000 loss_val: 1.8947 acc_val: 0.1567 time: 0.0170s\n",
            "Epoch: 0011 loss_train: 1.8906 acc_train: 0.2000 loss_val: 1.8869 acc_val: 0.1567 time: 0.0244s\n",
            "Epoch: 0012 loss_train: 1.8710 acc_train: 0.2000 loss_val: 1.8788 acc_val: 0.1567 time: 0.0195s\n",
            "Epoch: 0013 loss_train: 1.8678 acc_train: 0.2000 loss_val: 1.8703 acc_val: 0.1567 time: 0.0200s\n",
            "Epoch: 0014 loss_train: 1.8678 acc_train: 0.2071 loss_val: 1.8617 acc_val: 0.1567 time: 0.0201s\n",
            "Epoch: 0015 loss_train: 1.8507 acc_train: 0.2571 loss_val: 1.8531 acc_val: 0.1567 time: 0.0251s\n",
            "Epoch: 0016 loss_train: 1.8515 acc_train: 0.2429 loss_val: 1.8441 acc_val: 0.4167 time: 0.0179s\n",
            "Epoch: 0017 loss_train: 1.8500 acc_train: 0.2357 loss_val: 1.8352 acc_val: 0.3500 time: 0.0215s\n",
            "Epoch: 0018 loss_train: 1.8324 acc_train: 0.3286 loss_val: 1.8259 acc_val: 0.3500 time: 0.0204s\n",
            "Epoch: 0019 loss_train: 1.8280 acc_train: 0.3429 loss_val: 1.8165 acc_val: 0.3500 time: 0.0224s\n",
            "Epoch: 0020 loss_train: 1.8206 acc_train: 0.2929 loss_val: 1.8067 acc_val: 0.3500 time: 0.0298s\n",
            "Epoch: 0021 loss_train: 1.8017 acc_train: 0.3000 loss_val: 1.7970 acc_val: 0.3500 time: 0.0217s\n",
            "Epoch: 0022 loss_train: 1.7908 acc_train: 0.2929 loss_val: 1.7875 acc_val: 0.3500 time: 0.0209s\n",
            "Epoch: 0023 loss_train: 1.7942 acc_train: 0.2714 loss_val: 1.7782 acc_val: 0.3500 time: 0.0185s\n",
            "Epoch: 0024 loss_train: 1.8044 acc_train: 0.2857 loss_val: 1.7693 acc_val: 0.3500 time: 0.0216s\n",
            "Epoch: 0025 loss_train: 1.7676 acc_train: 0.2929 loss_val: 1.7607 acc_val: 0.3500 time: 0.0222s\n",
            "Epoch: 0026 loss_train: 1.7787 acc_train: 0.2929 loss_val: 1.7527 acc_val: 0.3500 time: 0.0182s\n",
            "Epoch: 0027 loss_train: 1.7654 acc_train: 0.2786 loss_val: 1.7449 acc_val: 0.3500 time: 0.0193s\n",
            "Epoch: 0028 loss_train: 1.7481 acc_train: 0.3000 loss_val: 1.7372 acc_val: 0.3500 time: 0.0194s\n",
            "Epoch: 0029 loss_train: 1.7468 acc_train: 0.3214 loss_val: 1.7298 acc_val: 0.3500 time: 0.0188s\n",
            "Epoch: 0030 loss_train: 1.7511 acc_train: 0.2786 loss_val: 1.7230 acc_val: 0.3500 time: 0.0230s\n",
            "Epoch: 0031 loss_train: 1.7135 acc_train: 0.3429 loss_val: 1.7158 acc_val: 0.3500 time: 0.0186s\n",
            "Epoch: 0032 loss_train: 1.7286 acc_train: 0.3143 loss_val: 1.7078 acc_val: 0.3500 time: 0.0192s\n",
            "Epoch: 0033 loss_train: 1.7087 acc_train: 0.3286 loss_val: 1.6989 acc_val: 0.3467 time: 0.0199s\n",
            "Epoch: 0034 loss_train: 1.6870 acc_train: 0.3143 loss_val: 1.6887 acc_val: 0.3467 time: 0.0209s\n",
            "Epoch: 0035 loss_train: 1.6782 acc_train: 0.3643 loss_val: 1.6778 acc_val: 0.3767 time: 0.0236s\n",
            "Epoch: 0036 loss_train: 1.6548 acc_train: 0.3429 loss_val: 1.6650 acc_val: 0.3933 time: 0.0195s\n",
            "Epoch: 0037 loss_train: 1.6187 acc_train: 0.4286 loss_val: 1.6510 acc_val: 0.4133 time: 0.0194s\n",
            "Epoch: 0038 loss_train: 1.6540 acc_train: 0.3429 loss_val: 1.6365 acc_val: 0.4267 time: 0.0216s\n",
            "Epoch: 0039 loss_train: 1.6139 acc_train: 0.3786 loss_val: 1.6218 acc_val: 0.4300 time: 0.0197s\n",
            "Epoch: 0040 loss_train: 1.5804 acc_train: 0.4071 loss_val: 1.6072 acc_val: 0.4433 time: 0.0226s\n",
            "Epoch: 0041 loss_train: 1.5794 acc_train: 0.4214 loss_val: 1.5925 acc_val: 0.4533 time: 0.0190s\n",
            "Epoch: 0042 loss_train: 1.5469 acc_train: 0.4143 loss_val: 1.5768 acc_val: 0.4633 time: 0.0178s\n",
            "Epoch: 0043 loss_train: 1.5250 acc_train: 0.4286 loss_val: 1.5589 acc_val: 0.4633 time: 0.0184s\n",
            "Epoch: 0044 loss_train: 1.5426 acc_train: 0.4357 loss_val: 1.5378 acc_val: 0.4633 time: 0.0211s\n",
            "Epoch: 0045 loss_train: 1.4724 acc_train: 0.4286 loss_val: 1.5159 acc_val: 0.4633 time: 0.0190s\n",
            "Epoch: 0046 loss_train: 1.4759 acc_train: 0.4500 loss_val: 1.4956 acc_val: 0.4633 time: 0.0200s\n",
            "Epoch: 0047 loss_train: 1.4577 acc_train: 0.4500 loss_val: 1.4768 acc_val: 0.4633 time: 0.0226s\n",
            "Epoch: 0048 loss_train: 1.4129 acc_train: 0.4429 loss_val: 1.4607 acc_val: 0.4633 time: 0.0182s\n",
            "Epoch: 0049 loss_train: 1.3952 acc_train: 0.4571 loss_val: 1.4457 acc_val: 0.4633 time: 0.0217s\n",
            "Epoch: 0050 loss_train: 1.3629 acc_train: 0.4643 loss_val: 1.4321 acc_val: 0.4600 time: 0.0275s\n",
            "Epoch: 0051 loss_train: 1.3927 acc_train: 0.4500 loss_val: 1.4167 acc_val: 0.4633 time: 0.0193s\n",
            "Epoch: 0052 loss_train: 1.3528 acc_train: 0.5000 loss_val: 1.4009 acc_val: 0.4733 time: 0.0188s\n",
            "Epoch: 0053 loss_train: 1.3032 acc_train: 0.4857 loss_val: 1.3855 acc_val: 0.4733 time: 0.0188s\n",
            "Epoch: 0054 loss_train: 1.3236 acc_train: 0.5214 loss_val: 1.3675 acc_val: 0.4733 time: 0.0200s\n",
            "Epoch: 0055 loss_train: 1.3254 acc_train: 0.5214 loss_val: 1.3526 acc_val: 0.4833 time: 0.0211s\n",
            "Epoch: 0056 loss_train: 1.2993 acc_train: 0.5214 loss_val: 1.3411 acc_val: 0.5100 time: 0.0212s\n",
            "Epoch: 0057 loss_train: 1.2783 acc_train: 0.5286 loss_val: 1.3335 acc_val: 0.5100 time: 0.0242s\n",
            "Epoch: 0058 loss_train: 1.2829 acc_train: 0.4643 loss_val: 1.3269 acc_val: 0.5133 time: 0.0207s\n",
            "Epoch: 0059 loss_train: 1.2651 acc_train: 0.5286 loss_val: 1.3156 acc_val: 0.5133 time: 0.0213s\n",
            "Epoch: 0060 loss_train: 1.2567 acc_train: 0.5571 loss_val: 1.2992 acc_val: 0.5100 time: 0.0326s\n",
            "Epoch: 0061 loss_train: 1.2398 acc_train: 0.5429 loss_val: 1.2832 acc_val: 0.5100 time: 0.0213s\n",
            "Epoch: 0062 loss_train: 1.2153 acc_train: 0.5357 loss_val: 1.2736 acc_val: 0.4967 time: 0.0209s\n",
            "Epoch: 0063 loss_train: 1.2555 acc_train: 0.5357 loss_val: 1.2636 acc_val: 0.4867 time: 0.0215s\n",
            "Epoch: 0064 loss_train: 1.1973 acc_train: 0.5214 loss_val: 1.2578 acc_val: 0.4833 time: 0.0243s\n",
            "Epoch: 0065 loss_train: 1.1822 acc_train: 0.5429 loss_val: 1.2560 acc_val: 0.4900 time: 0.0206s\n",
            "Epoch: 0066 loss_train: 1.2015 acc_train: 0.5214 loss_val: 1.2503 acc_val: 0.4933 time: 0.0175s\n",
            "Epoch: 0067 loss_train: 1.1423 acc_train: 0.5929 loss_val: 1.2378 acc_val: 0.5133 time: 0.0205s\n",
            "Epoch: 0068 loss_train: 1.1841 acc_train: 0.5786 loss_val: 1.2220 acc_val: 0.5167 time: 0.0193s\n",
            "Epoch: 0069 loss_train: 1.1797 acc_train: 0.5429 loss_val: 1.2119 acc_val: 0.5400 time: 0.0189s\n",
            "Epoch: 0070 loss_train: 1.1185 acc_train: 0.6143 loss_val: 1.2045 acc_val: 0.5600 time: 0.0239s\n",
            "Epoch: 0071 loss_train: 1.0567 acc_train: 0.6357 loss_val: 1.1964 acc_val: 0.5700 time: 0.0210s\n",
            "Epoch: 0072 loss_train: 1.0795 acc_train: 0.6143 loss_val: 1.1893 acc_val: 0.5733 time: 0.0194s\n",
            "Epoch: 0073 loss_train: 1.0410 acc_train: 0.6429 loss_val: 1.1823 acc_val: 0.5700 time: 0.0184s\n",
            "Epoch: 0074 loss_train: 1.0709 acc_train: 0.6071 loss_val: 1.1748 acc_val: 0.5700 time: 0.0220s\n",
            "Epoch: 0075 loss_train: 1.0298 acc_train: 0.6143 loss_val: 1.1624 acc_val: 0.5700 time: 0.0180s\n",
            "Epoch: 0076 loss_train: 1.0185 acc_train: 0.6500 loss_val: 1.1474 acc_val: 0.5700 time: 0.0196s\n",
            "Epoch: 0077 loss_train: 1.0459 acc_train: 0.6357 loss_val: 1.1384 acc_val: 0.5700 time: 0.0215s\n",
            "Epoch: 0078 loss_train: 1.0444 acc_train: 0.6071 loss_val: 1.1266 acc_val: 0.5833 time: 0.0224s\n",
            "Epoch: 0079 loss_train: 0.9800 acc_train: 0.6357 loss_val: 1.1147 acc_val: 0.5967 time: 0.0202s\n",
            "Epoch: 0080 loss_train: 0.9768 acc_train: 0.6357 loss_val: 1.1057 acc_val: 0.5967 time: 0.0245s\n",
            "Epoch: 0081 loss_train: 0.9942 acc_train: 0.6143 loss_val: 1.0992 acc_val: 0.6033 time: 0.0206s\n",
            "Epoch: 0082 loss_train: 0.9869 acc_train: 0.6000 loss_val: 1.0957 acc_val: 0.6067 time: 0.0189s\n",
            "Epoch: 0083 loss_train: 0.9494 acc_train: 0.6357 loss_val: 1.0856 acc_val: 0.6100 time: 0.0214s\n",
            "Epoch: 0084 loss_train: 0.9384 acc_train: 0.6429 loss_val: 1.0702 acc_val: 0.6167 time: 0.0221s\n",
            "Epoch: 0085 loss_train: 0.9547 acc_train: 0.6714 loss_val: 1.0565 acc_val: 0.6200 time: 0.0163s\n",
            "Epoch: 0086 loss_train: 0.9311 acc_train: 0.6357 loss_val: 1.0468 acc_val: 0.6400 time: 0.0169s\n",
            "Epoch: 0087 loss_train: 0.9355 acc_train: 0.6714 loss_val: 1.0387 acc_val: 0.6467 time: 0.0164s\n",
            "Epoch: 0088 loss_train: 0.9222 acc_train: 0.6571 loss_val: 1.0313 acc_val: 0.6567 time: 0.0178s\n",
            "Epoch: 0089 loss_train: 0.8945 acc_train: 0.6786 loss_val: 1.0264 acc_val: 0.6600 time: 0.0173s\n",
            "Epoch: 0090 loss_train: 0.8457 acc_train: 0.7286 loss_val: 1.0246 acc_val: 0.6567 time: 0.0198s\n",
            "Epoch: 0091 loss_train: 0.8203 acc_train: 0.7143 loss_val: 1.0154 acc_val: 0.6467 time: 0.0241s\n",
            "Epoch: 0092 loss_train: 0.8573 acc_train: 0.6857 loss_val: 1.0038 acc_val: 0.6533 time: 0.0182s\n",
            "Epoch: 0093 loss_train: 0.8154 acc_train: 0.7214 loss_val: 0.9958 acc_val: 0.6400 time: 0.0197s\n",
            "Epoch: 0094 loss_train: 0.7802 acc_train: 0.7071 loss_val: 0.9881 acc_val: 0.6433 time: 0.0174s\n",
            "Epoch: 0095 loss_train: 0.7637 acc_train: 0.7000 loss_val: 0.9801 acc_val: 0.6700 time: 0.0196s\n",
            "Epoch: 0096 loss_train: 0.7538 acc_train: 0.7357 loss_val: 0.9777 acc_val: 0.6733 time: 0.0189s\n",
            "Epoch: 0097 loss_train: 0.7802 acc_train: 0.7429 loss_val: 0.9754 acc_val: 0.6800 time: 0.0185s\n",
            "Epoch: 0098 loss_train: 0.7408 acc_train: 0.7429 loss_val: 0.9684 acc_val: 0.6667 time: 0.0221s\n",
            "Epoch: 0099 loss_train: 0.7793 acc_train: 0.7286 loss_val: 0.9568 acc_val: 0.6800 time: 0.0195s\n",
            "Epoch: 0100 loss_train: 0.7458 acc_train: 0.6786 loss_val: 0.9509 acc_val: 0.6767 time: 0.0261s\n",
            "Epoch: 0101 loss_train: 0.7577 acc_train: 0.7214 loss_val: 0.9457 acc_val: 0.6833 time: 0.0221s\n",
            "Epoch: 0102 loss_train: 0.7312 acc_train: 0.7071 loss_val: 0.9402 acc_val: 0.6833 time: 0.0215s\n",
            "Epoch: 0103 loss_train: 0.7157 acc_train: 0.7000 loss_val: 0.9402 acc_val: 0.6967 time: 0.0247s\n",
            "Epoch: 0104 loss_train: 0.7008 acc_train: 0.7357 loss_val: 0.9395 acc_val: 0.6833 time: 0.0202s\n",
            "Epoch: 0105 loss_train: 0.6770 acc_train: 0.7214 loss_val: 0.9398 acc_val: 0.6800 time: 0.0238s\n",
            "Epoch: 0106 loss_train: 0.6770 acc_train: 0.8000 loss_val: 0.9349 acc_val: 0.6967 time: 0.0226s\n",
            "Epoch: 0107 loss_train: 0.6849 acc_train: 0.7429 loss_val: 0.9288 acc_val: 0.6933 time: 0.0287s\n",
            "Epoch: 0108 loss_train: 0.6878 acc_train: 0.7500 loss_val: 0.9222 acc_val: 0.7000 time: 0.0198s\n",
            "Epoch: 0109 loss_train: 0.6817 acc_train: 0.7643 loss_val: 0.9097 acc_val: 0.7200 time: 0.0250s\n",
            "Epoch: 0110 loss_train: 0.6309 acc_train: 0.7786 loss_val: 0.9045 acc_val: 0.7033 time: 0.0198s\n",
            "Epoch: 0111 loss_train: 0.6594 acc_train: 0.7929 loss_val: 0.9101 acc_val: 0.7067 time: 0.0183s\n",
            "Epoch: 0112 loss_train: 0.6494 acc_train: 0.7857 loss_val: 0.9135 acc_val: 0.7133 time: 0.0182s\n",
            "Epoch: 0113 loss_train: 0.6469 acc_train: 0.7786 loss_val: 0.9010 acc_val: 0.7033 time: 0.0185s\n",
            "Epoch: 0114 loss_train: 0.6408 acc_train: 0.7571 loss_val: 0.8906 acc_val: 0.7233 time: 0.0196s\n",
            "Epoch: 0115 loss_train: 0.6125 acc_train: 0.7643 loss_val: 0.8877 acc_val: 0.7233 time: 0.0235s\n",
            "Epoch: 0116 loss_train: 0.6040 acc_train: 0.7929 loss_val: 0.8892 acc_val: 0.7267 time: 0.0236s\n",
            "Epoch: 0117 loss_train: 0.6406 acc_train: 0.7786 loss_val: 0.8845 acc_val: 0.7300 time: 0.0170s\n",
            "Epoch: 0118 loss_train: 0.5418 acc_train: 0.7929 loss_val: 0.8805 acc_val: 0.7267 time: 0.0179s\n",
            "Epoch: 0119 loss_train: 0.5907 acc_train: 0.7571 loss_val: 0.8874 acc_val: 0.7300 time: 0.0210s\n",
            "Epoch: 0120 loss_train: 0.5908 acc_train: 0.8286 loss_val: 0.8973 acc_val: 0.7233 time: 0.0177s\n",
            "Epoch: 0121 loss_train: 0.6180 acc_train: 0.7857 loss_val: 0.8947 acc_val: 0.7233 time: 0.0195s\n",
            "Epoch: 0122 loss_train: 0.6190 acc_train: 0.8000 loss_val: 0.8793 acc_val: 0.7300 time: 0.0190s\n",
            "Epoch: 0123 loss_train: 0.5584 acc_train: 0.8071 loss_val: 0.8798 acc_val: 0.7267 time: 0.0197s\n",
            "Epoch: 0124 loss_train: 0.5851 acc_train: 0.8000 loss_val: 0.8813 acc_val: 0.7233 time: 0.0196s\n",
            "Epoch: 0125 loss_train: 0.5556 acc_train: 0.8214 loss_val: 0.8798 acc_val: 0.7233 time: 0.0201s\n",
            "Epoch: 0126 loss_train: 0.5583 acc_train: 0.8357 loss_val: 0.8864 acc_val: 0.7300 time: 0.0186s\n",
            "Epoch: 0127 loss_train: 0.5482 acc_train: 0.8571 loss_val: 0.8985 acc_val: 0.7167 time: 0.0195s\n",
            "Epoch: 0128 loss_train: 0.5493 acc_train: 0.8286 loss_val: 0.8933 acc_val: 0.7200 time: 0.0194s\n",
            "Epoch: 0129 loss_train: 0.5244 acc_train: 0.8500 loss_val: 0.8804 acc_val: 0.7267 time: 0.0251s\n",
            "Epoch: 0130 loss_train: 0.4627 acc_train: 0.8857 loss_val: 0.8822 acc_val: 0.7267 time: 0.0194s\n",
            "Epoch: 0131 loss_train: 0.5131 acc_train: 0.8214 loss_val: 0.8844 acc_val: 0.7233 time: 0.0175s\n",
            "Epoch: 0132 loss_train: 0.5330 acc_train: 0.8357 loss_val: 0.8783 acc_val: 0.7233 time: 0.0190s\n",
            "Epoch: 0133 loss_train: 0.5098 acc_train: 0.8143 loss_val: 0.8721 acc_val: 0.7333 time: 0.0189s\n",
            "Epoch: 0134 loss_train: 0.5141 acc_train: 0.8286 loss_val: 0.8797 acc_val: 0.7267 time: 0.0204s\n",
            "Epoch: 0135 loss_train: 0.5087 acc_train: 0.8286 loss_val: 0.8795 acc_val: 0.7267 time: 0.0187s\n",
            "Epoch: 0136 loss_train: 0.5115 acc_train: 0.8214 loss_val: 0.8650 acc_val: 0.7233 time: 0.0186s\n",
            "Epoch: 0137 loss_train: 0.5539 acc_train: 0.7929 loss_val: 0.8543 acc_val: 0.7300 time: 0.0188s\n",
            "Epoch: 0138 loss_train: 0.4797 acc_train: 0.8500 loss_val: 0.8535 acc_val: 0.7300 time: 0.0190s\n",
            "Epoch: 0139 loss_train: 0.4508 acc_train: 0.8786 loss_val: 0.8550 acc_val: 0.7267 time: 0.0187s\n",
            "Epoch: 0140 loss_train: 0.4875 acc_train: 0.8571 loss_val: 0.8526 acc_val: 0.7300 time: 0.0235s\n",
            "Epoch: 0141 loss_train: 0.5374 acc_train: 0.8143 loss_val: 0.8545 acc_val: 0.7267 time: 0.0200s\n",
            "Epoch: 0142 loss_train: 0.4763 acc_train: 0.8143 loss_val: 0.8636 acc_val: 0.7300 time: 0.0194s\n",
            "Epoch: 0143 loss_train: 0.5012 acc_train: 0.8143 loss_val: 0.8577 acc_val: 0.7267 time: 0.0193s\n",
            "Epoch: 0144 loss_train: 0.4518 acc_train: 0.8571 loss_val: 0.8490 acc_val: 0.7367 time: 0.0301s\n",
            "Epoch: 0145 loss_train: 0.4605 acc_train: 0.8643 loss_val: 0.8535 acc_val: 0.7200 time: 0.0244s\n",
            "Epoch: 0146 loss_train: 0.4719 acc_train: 0.8429 loss_val: 0.8574 acc_val: 0.7300 time: 0.0207s\n",
            "Epoch: 0147 loss_train: 0.4469 acc_train: 0.8643 loss_val: 0.8576 acc_val: 0.7233 time: 0.0208s\n",
            "Epoch: 0148 loss_train: 0.4594 acc_train: 0.8357 loss_val: 0.8570 acc_val: 0.7367 time: 0.0246s\n",
            "Epoch: 0149 loss_train: 0.4059 acc_train: 0.8929 loss_val: 0.8593 acc_val: 0.7367 time: 0.0286s\n",
            "Epoch: 0150 loss_train: 0.4191 acc_train: 0.8929 loss_val: 0.8703 acc_val: 0.7333 time: 0.0226s\n",
            "Epoch: 0151 loss_train: 0.4695 acc_train: 0.8643 loss_val: 0.8747 acc_val: 0.7300 time: 0.0204s\n",
            "Epoch: 0152 loss_train: 0.5120 acc_train: 0.8357 loss_val: 0.8638 acc_val: 0.7367 time: 0.0211s\n",
            "Epoch: 0153 loss_train: 0.4449 acc_train: 0.8643 loss_val: 0.8582 acc_val: 0.7367 time: 0.0307s\n",
            "Epoch: 0154 loss_train: 0.3893 acc_train: 0.8929 loss_val: 0.8646 acc_val: 0.7333 time: 0.0189s\n",
            "Epoch: 0155 loss_train: 0.4455 acc_train: 0.8571 loss_val: 0.8701 acc_val: 0.7267 time: 0.0201s\n",
            "Epoch: 0156 loss_train: 0.4236 acc_train: 0.8357 loss_val: 0.8674 acc_val: 0.7333 time: 0.0201s\n",
            "Epoch: 0157 loss_train: 0.4257 acc_train: 0.8714 loss_val: 0.8607 acc_val: 0.7400 time: 0.0254s\n",
            "Epoch: 0158 loss_train: 0.3900 acc_train: 0.8643 loss_val: 0.8670 acc_val: 0.7300 time: 0.0182s\n",
            "Epoch: 0159 loss_train: 0.3915 acc_train: 0.8643 loss_val: 0.8788 acc_val: 0.7200 time: 0.0174s\n",
            "Epoch: 0160 loss_train: 0.3857 acc_train: 0.8857 loss_val: 0.8738 acc_val: 0.7300 time: 0.0185s\n",
            "Epoch: 0161 loss_train: 0.4393 acc_train: 0.8786 loss_val: 0.8648 acc_val: 0.7333 time: 0.0223s\n",
            "Epoch: 0162 loss_train: 0.3478 acc_train: 0.9214 loss_val: 0.8629 acc_val: 0.7367 time: 0.0223s\n",
            "Epoch: 0163 loss_train: 0.3794 acc_train: 0.9071 loss_val: 0.8682 acc_val: 0.7267 time: 0.0184s\n",
            "Epoch: 0164 loss_train: 0.3829 acc_train: 0.9000 loss_val: 0.8650 acc_val: 0.7300 time: 0.0190s\n",
            "Epoch: 0165 loss_train: 0.4038 acc_train: 0.8714 loss_val: 0.8556 acc_val: 0.7333 time: 0.0181s\n",
            "Epoch: 0166 loss_train: 0.3755 acc_train: 0.9071 loss_val: 0.8475 acc_val: 0.7367 time: 0.0193s\n",
            "Epoch: 0167 loss_train: 0.3960 acc_train: 0.8857 loss_val: 0.8432 acc_val: 0.7433 time: 0.0241s\n",
            "Epoch: 0168 loss_train: 0.3784 acc_train: 0.9000 loss_val: 0.8446 acc_val: 0.7400 time: 0.0181s\n",
            "Epoch: 0169 loss_train: 0.3874 acc_train: 0.8786 loss_val: 0.8592 acc_val: 0.7400 time: 0.0188s\n",
            "Epoch: 0170 loss_train: 0.4163 acc_train: 0.8643 loss_val: 0.8641 acc_val: 0.7333 time: 0.0180s\n",
            "Epoch: 0171 loss_train: 0.3573 acc_train: 0.8929 loss_val: 0.8581 acc_val: 0.7400 time: 0.0185s\n",
            "Epoch: 0172 loss_train: 0.4508 acc_train: 0.8643 loss_val: 0.8415 acc_val: 0.7400 time: 0.0189s\n",
            "Epoch: 0173 loss_train: 0.3734 acc_train: 0.8857 loss_val: 0.8478 acc_val: 0.7367 time: 0.0211s\n",
            "Epoch: 0174 loss_train: 0.3861 acc_train: 0.8786 loss_val: 0.8585 acc_val: 0.7367 time: 0.0197s\n",
            "Epoch: 0175 loss_train: 0.3680 acc_train: 0.8857 loss_val: 0.8547 acc_val: 0.7400 time: 0.0190s\n",
            "Epoch: 0176 loss_train: 0.3436 acc_train: 0.9143 loss_val: 0.8471 acc_val: 0.7433 time: 0.0196s\n",
            "Epoch: 0177 loss_train: 0.3773 acc_train: 0.8857 loss_val: 0.8450 acc_val: 0.7367 time: 0.0216s\n",
            "Epoch: 0178 loss_train: 0.3368 acc_train: 0.9143 loss_val: 0.8553 acc_val: 0.7367 time: 0.0214s\n",
            "Epoch: 0179 loss_train: 0.3620 acc_train: 0.8786 loss_val: 0.8656 acc_val: 0.7300 time: 0.0193s\n",
            "Epoch: 0180 loss_train: 0.3717 acc_train: 0.8857 loss_val: 0.8678 acc_val: 0.7267 time: 0.0208s\n",
            "Epoch: 0181 loss_train: 0.3324 acc_train: 0.9429 loss_val: 0.8564 acc_val: 0.7433 time: 0.0201s\n",
            "Epoch: 0182 loss_train: 0.3520 acc_train: 0.9214 loss_val: 0.8475 acc_val: 0.7433 time: 0.0212s\n",
            "Epoch: 0183 loss_train: 0.2988 acc_train: 0.9214 loss_val: 0.8608 acc_val: 0.7567 time: 0.0183s\n",
            "Epoch: 0184 loss_train: 0.3348 acc_train: 0.9000 loss_val: 0.8822 acc_val: 0.7467 time: 0.0192s\n",
            "Epoch: 0185 loss_train: 0.3390 acc_train: 0.9000 loss_val: 0.8815 acc_val: 0.7500 time: 0.0215s\n",
            "Epoch: 0186 loss_train: 0.3767 acc_train: 0.8714 loss_val: 0.8515 acc_val: 0.7600 time: 0.0195s\n",
            "Epoch: 0187 loss_train: 0.3605 acc_train: 0.9143 loss_val: 0.8403 acc_val: 0.7533 time: 0.0274s\n",
            "Epoch: 0188 loss_train: 0.3171 acc_train: 0.8929 loss_val: 0.8475 acc_val: 0.7600 time: 0.0208s\n",
            "Epoch: 0189 loss_train: 0.3220 acc_train: 0.9286 loss_val: 0.8509 acc_val: 0.7667 time: 0.0207s\n",
            "Epoch: 0190 loss_train: 0.3528 acc_train: 0.9071 loss_val: 0.8558 acc_val: 0.7700 time: 0.0210s\n",
            "Epoch: 0191 loss_train: 0.4071 acc_train: 0.8857 loss_val: 0.8471 acc_val: 0.7733 time: 0.0203s\n",
            "Epoch: 0192 loss_train: 0.3206 acc_train: 0.9071 loss_val: 0.8324 acc_val: 0.7733 time: 0.0228s\n",
            "Epoch: 0193 loss_train: 0.2999 acc_train: 0.9143 loss_val: 0.8170 acc_val: 0.7900 time: 0.0286s\n",
            "Epoch: 0194 loss_train: 0.2636 acc_train: 0.9357 loss_val: 0.8149 acc_val: 0.7833 time: 0.0234s\n",
            "Epoch: 0195 loss_train: 0.2536 acc_train: 0.9357 loss_val: 0.8257 acc_val: 0.7867 time: 0.0204s\n",
            "Epoch: 0196 loss_train: 0.2492 acc_train: 0.9500 loss_val: 0.8271 acc_val: 0.7833 time: 0.0253s\n",
            "Epoch: 0197 loss_train: 0.2672 acc_train: 0.9214 loss_val: 0.8234 acc_val: 0.7900 time: 0.0223s\n",
            "Epoch: 0198 loss_train: 0.3028 acc_train: 0.9571 loss_val: 0.8156 acc_val: 0.7900 time: 0.0224s\n",
            "Epoch: 0199 loss_train: 0.3263 acc_train: 0.9357 loss_val: 0.8080 acc_val: 0.7900 time: 0.0246s\n",
            "Epoch: 0200 loss_train: 0.3127 acc_train: 0.9286 loss_val: 0.8151 acc_val: 0.7867 time: 0.0199s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 4.4507s\n",
            "Test set results: loss= 0.8407 accuracy= 0.7550\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-Kvw-VTEcp5",
        "outputId": "19a241c8-8b2a-40d1-d5a1-5aa86af6b368"
      },
      "source": [
        "#4-layers\n",
        "%run pygcn/train.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cora dataset...\n",
            "Epoch: 0001 loss_train: 1.9668 acc_train: 0.1500 loss_val: 1.9655 acc_val: 0.1267 time: 0.0262s\n",
            "Epoch: 0002 loss_train: 1.9542 acc_train: 0.1214 loss_val: 1.9517 acc_val: 0.1267 time: 0.0208s\n",
            "Epoch: 0003 loss_train: 1.9378 acc_train: 0.1357 loss_val: 1.9393 acc_val: 0.1267 time: 0.0196s\n",
            "Epoch: 0004 loss_train: 1.9295 acc_train: 0.1500 loss_val: 1.9272 acc_val: 0.1267 time: 0.0199s\n",
            "Epoch: 0005 loss_train: 1.9142 acc_train: 0.1500 loss_val: 1.9149 acc_val: 0.1567 time: 0.0217s\n",
            "Epoch: 0006 loss_train: 1.8984 acc_train: 0.2214 loss_val: 1.9027 acc_val: 0.1567 time: 0.0216s\n",
            "Epoch: 0007 loss_train: 1.8914 acc_train: 0.1786 loss_val: 1.8906 acc_val: 0.1567 time: 0.0232s\n",
            "Epoch: 0008 loss_train: 1.8690 acc_train: 0.2786 loss_val: 1.8788 acc_val: 0.1567 time: 0.0275s\n",
            "Epoch: 0009 loss_train: 1.8751 acc_train: 0.2286 loss_val: 1.8681 acc_val: 0.3500 time: 0.0262s\n",
            "Epoch: 0010 loss_train: 1.8584 acc_train: 0.2857 loss_val: 1.8581 acc_val: 0.3500 time: 0.0250s\n",
            "Epoch: 0011 loss_train: 1.8716 acc_train: 0.2786 loss_val: 1.8490 acc_val: 0.3500 time: 0.0213s\n",
            "Epoch: 0012 loss_train: 1.8606 acc_train: 0.3071 loss_val: 1.8405 acc_val: 0.3500 time: 0.0223s\n",
            "Epoch: 0013 loss_train: 1.8475 acc_train: 0.1714 loss_val: 1.8332 acc_val: 0.3500 time: 0.0227s\n",
            "Epoch: 0014 loss_train: 1.8531 acc_train: 0.2714 loss_val: 1.8269 acc_val: 0.3500 time: 0.0273s\n",
            "Epoch: 0015 loss_train: 1.8174 acc_train: 0.2857 loss_val: 1.8209 acc_val: 0.3500 time: 0.0227s\n",
            "Epoch: 0016 loss_train: 1.8370 acc_train: 0.2786 loss_val: 1.8158 acc_val: 0.3500 time: 0.0213s\n",
            "Epoch: 0017 loss_train: 1.8467 acc_train: 0.2500 loss_val: 1.8121 acc_val: 0.3500 time: 0.0240s\n",
            "Epoch: 0018 loss_train: 1.8377 acc_train: 0.2643 loss_val: 1.8083 acc_val: 0.3500 time: 0.0215s\n",
            "Epoch: 0019 loss_train: 1.8260 acc_train: 0.2857 loss_val: 1.8046 acc_val: 0.3500 time: 0.0357s\n",
            "Epoch: 0020 loss_train: 1.8540 acc_train: 0.2357 loss_val: 1.8016 acc_val: 0.3500 time: 0.0269s\n",
            "Epoch: 0021 loss_train: 1.8181 acc_train: 0.2429 loss_val: 1.7993 acc_val: 0.3500 time: 0.0240s\n",
            "Epoch: 0022 loss_train: 1.8529 acc_train: 0.2643 loss_val: 1.7975 acc_val: 0.3500 time: 0.0257s\n",
            "Epoch: 0023 loss_train: 1.7965 acc_train: 0.2643 loss_val: 1.7958 acc_val: 0.3500 time: 0.0269s\n",
            "Epoch: 0024 loss_train: 1.8035 acc_train: 0.2929 loss_val: 1.7945 acc_val: 0.3500 time: 0.0259s\n",
            "Epoch: 0025 loss_train: 1.8033 acc_train: 0.3000 loss_val: 1.7934 acc_val: 0.3500 time: 0.0246s\n",
            "Epoch: 0026 loss_train: 1.8248 acc_train: 0.2929 loss_val: 1.7927 acc_val: 0.3500 time: 0.0243s\n",
            "Epoch: 0027 loss_train: 1.8034 acc_train: 0.2857 loss_val: 1.7922 acc_val: 0.3500 time: 0.0262s\n",
            "Epoch: 0028 loss_train: 1.7938 acc_train: 0.2786 loss_val: 1.7915 acc_val: 0.3500 time: 0.0242s\n",
            "Epoch: 0029 loss_train: 1.8318 acc_train: 0.2571 loss_val: 1.7908 acc_val: 0.3500 time: 0.0227s\n",
            "Epoch: 0030 loss_train: 1.8315 acc_train: 0.2786 loss_val: 1.7901 acc_val: 0.3500 time: 0.0227s\n",
            "Epoch: 0031 loss_train: 1.7908 acc_train: 0.2786 loss_val: 1.7885 acc_val: 0.3500 time: 0.0229s\n",
            "Epoch: 0032 loss_train: 1.8143 acc_train: 0.2714 loss_val: 1.7863 acc_val: 0.3500 time: 0.0224s\n",
            "Epoch: 0033 loss_train: 1.7918 acc_train: 0.3071 loss_val: 1.7832 acc_val: 0.3500 time: 0.0241s\n",
            "Epoch: 0034 loss_train: 1.8004 acc_train: 0.2714 loss_val: 1.7797 acc_val: 0.3500 time: 0.0228s\n",
            "Epoch: 0035 loss_train: 1.7769 acc_train: 0.2929 loss_val: 1.7750 acc_val: 0.3500 time: 0.0236s\n",
            "Epoch: 0036 loss_train: 1.8015 acc_train: 0.2786 loss_val: 1.7698 acc_val: 0.3500 time: 0.0262s\n",
            "Epoch: 0037 loss_train: 1.7757 acc_train: 0.2857 loss_val: 1.7637 acc_val: 0.3500 time: 0.0228s\n",
            "Epoch: 0038 loss_train: 1.7828 acc_train: 0.2857 loss_val: 1.7567 acc_val: 0.3500 time: 0.0239s\n",
            "Epoch: 0039 loss_train: 1.7587 acc_train: 0.2929 loss_val: 1.7480 acc_val: 0.3500 time: 0.0226s\n",
            "Epoch: 0040 loss_train: 1.7693 acc_train: 0.2857 loss_val: 1.7379 acc_val: 0.3500 time: 0.0226s\n",
            "Epoch: 0041 loss_train: 1.7361 acc_train: 0.3000 loss_val: 1.7251 acc_val: 0.3500 time: 0.0250s\n",
            "Epoch: 0042 loss_train: 1.7379 acc_train: 0.2857 loss_val: 1.7104 acc_val: 0.3500 time: 0.0253s\n",
            "Epoch: 0043 loss_train: 1.6987 acc_train: 0.2857 loss_val: 1.6928 acc_val: 0.3500 time: 0.0284s\n",
            "Epoch: 0044 loss_train: 1.7063 acc_train: 0.2929 loss_val: 1.6720 acc_val: 0.3500 time: 0.0220s\n",
            "Epoch: 0045 loss_train: 1.6872 acc_train: 0.2929 loss_val: 1.6474 acc_val: 0.3500 time: 0.0278s\n",
            "Epoch: 0046 loss_train: 1.6495 acc_train: 0.2929 loss_val: 1.6197 acc_val: 0.3500 time: 0.0232s\n",
            "Epoch: 0047 loss_train: 1.5911 acc_train: 0.2929 loss_val: 1.5873 acc_val: 0.3533 time: 0.0228s\n",
            "Epoch: 0048 loss_train: 1.5732 acc_train: 0.3214 loss_val: 1.5535 acc_val: 0.3800 time: 0.0305s\n",
            "Epoch: 0049 loss_train: 1.5456 acc_train: 0.3429 loss_val: 1.5213 acc_val: 0.4100 time: 0.0305s\n",
            "Epoch: 0050 loss_train: 1.4850 acc_train: 0.3286 loss_val: 1.4933 acc_val: 0.4300 time: 0.0237s\n",
            "Epoch: 0051 loss_train: 1.4885 acc_train: 0.4071 loss_val: 1.4683 acc_val: 0.4333 time: 0.0222s\n",
            "Epoch: 0052 loss_train: 1.4404 acc_train: 0.4286 loss_val: 1.4432 acc_val: 0.4367 time: 0.0233s\n",
            "Epoch: 0053 loss_train: 1.4394 acc_train: 0.4214 loss_val: 1.4250 acc_val: 0.4400 time: 0.0265s\n",
            "Epoch: 0054 loss_train: 1.4260 acc_train: 0.4286 loss_val: 1.4132 acc_val: 0.4467 time: 0.0213s\n",
            "Epoch: 0055 loss_train: 1.4184 acc_train: 0.4286 loss_val: 1.4074 acc_val: 0.4500 time: 0.0209s\n",
            "Epoch: 0056 loss_train: 1.4403 acc_train: 0.4000 loss_val: 1.4124 acc_val: 0.4400 time: 0.0205s\n",
            "Epoch: 0057 loss_train: 1.3915 acc_train: 0.3857 loss_val: 1.4000 acc_val: 0.4400 time: 0.0225s\n",
            "Epoch: 0058 loss_train: 1.3817 acc_train: 0.4143 loss_val: 1.3844 acc_val: 0.4467 time: 0.0314s\n",
            "Epoch: 0059 loss_train: 1.3623 acc_train: 0.4071 loss_val: 1.3738 acc_val: 0.4500 time: 0.0212s\n",
            "Epoch: 0060 loss_train: 1.3327 acc_train: 0.4429 loss_val: 1.3708 acc_val: 0.4467 time: 0.0243s\n",
            "Epoch: 0061 loss_train: 1.3309 acc_train: 0.4214 loss_val: 1.3648 acc_val: 0.4467 time: 0.0270s\n",
            "Epoch: 0062 loss_train: 1.3047 acc_train: 0.4143 loss_val: 1.3581 acc_val: 0.4467 time: 0.0291s\n",
            "Epoch: 0063 loss_train: 1.3247 acc_train: 0.4143 loss_val: 1.3588 acc_val: 0.4500 time: 0.0206s\n",
            "Epoch: 0064 loss_train: 1.2971 acc_train: 0.4143 loss_val: 1.3577 acc_val: 0.4500 time: 0.0223s\n",
            "Epoch: 0065 loss_train: 1.2703 acc_train: 0.4357 loss_val: 1.3452 acc_val: 0.4500 time: 0.0226s\n",
            "Epoch: 0066 loss_train: 1.2655 acc_train: 0.4429 loss_val: 1.3326 acc_val: 0.4500 time: 0.0302s\n",
            "Epoch: 0067 loss_train: 1.2416 acc_train: 0.4286 loss_val: 1.3275 acc_val: 0.4533 time: 0.0269s\n",
            "Epoch: 0068 loss_train: 1.2650 acc_train: 0.4500 loss_val: 1.3250 acc_val: 0.4500 time: 0.0238s\n",
            "Epoch: 0069 loss_train: 1.2506 acc_train: 0.4357 loss_val: 1.3242 acc_val: 0.4467 time: 0.0217s\n",
            "Epoch: 0070 loss_train: 1.2334 acc_train: 0.4286 loss_val: 1.3271 acc_val: 0.4467 time: 0.0281s\n",
            "Epoch: 0071 loss_train: 1.2118 acc_train: 0.4357 loss_val: 1.3250 acc_val: 0.4367 time: 0.0236s\n",
            "Epoch: 0072 loss_train: 1.2327 acc_train: 0.4357 loss_val: 1.3203 acc_val: 0.4533 time: 0.0236s\n",
            "Epoch: 0073 loss_train: 1.2030 acc_train: 0.4357 loss_val: 1.3200 acc_val: 0.4500 time: 0.0236s\n",
            "Epoch: 0074 loss_train: 1.1907 acc_train: 0.4357 loss_val: 1.3239 acc_val: 0.4400 time: 0.0266s\n",
            "Epoch: 0075 loss_train: 1.1891 acc_train: 0.4357 loss_val: 1.3357 acc_val: 0.4333 time: 0.0238s\n",
            "Epoch: 0076 loss_train: 1.2205 acc_train: 0.4214 loss_val: 1.3191 acc_val: 0.4400 time: 0.0228s\n",
            "Epoch: 0077 loss_train: 1.1761 acc_train: 0.4214 loss_val: 1.3260 acc_val: 0.4467 time: 0.0241s\n",
            "Epoch: 0078 loss_train: 1.1899 acc_train: 0.4286 loss_val: 1.3175 acc_val: 0.4533 time: 0.0254s\n",
            "Epoch: 0079 loss_train: 1.1652 acc_train: 0.4286 loss_val: 1.3330 acc_val: 0.4333 time: 0.0274s\n",
            "Epoch: 0080 loss_train: 1.1679 acc_train: 0.4786 loss_val: 1.3228 acc_val: 0.4333 time: 0.0221s\n",
            "Epoch: 0081 loss_train: 1.1669 acc_train: 0.4214 loss_val: 1.3114 acc_val: 0.4467 time: 0.0248s\n",
            "Epoch: 0082 loss_train: 1.1226 acc_train: 0.4500 loss_val: 1.3105 acc_val: 0.4433 time: 0.0261s\n",
            "Epoch: 0083 loss_train: 1.1332 acc_train: 0.4500 loss_val: 1.3122 acc_val: 0.4367 time: 0.0220s\n",
            "Epoch: 0084 loss_train: 1.1387 acc_train: 0.4429 loss_val: 1.3127 acc_val: 0.4233 time: 0.0234s\n",
            "Epoch: 0085 loss_train: 1.1299 acc_train: 0.4286 loss_val: 1.3185 acc_val: 0.4167 time: 0.0228s\n",
            "Epoch: 0086 loss_train: 1.1246 acc_train: 0.4714 loss_val: 1.3113 acc_val: 0.4133 time: 0.0226s\n",
            "Epoch: 0087 loss_train: 1.1168 acc_train: 0.4857 loss_val: 1.3110 acc_val: 0.4367 time: 0.0281s\n",
            "Epoch: 0088 loss_train: 1.1354 acc_train: 0.4643 loss_val: 1.3127 acc_val: 0.4467 time: 0.0316s\n",
            "Epoch: 0089 loss_train: 1.0990 acc_train: 0.5071 loss_val: 1.3171 acc_val: 0.4667 time: 0.0245s\n",
            "Epoch: 0090 loss_train: 1.1067 acc_train: 0.5286 loss_val: 1.3058 acc_val: 0.4467 time: 0.0232s\n",
            "Epoch: 0091 loss_train: 1.0491 acc_train: 0.5429 loss_val: 1.3091 acc_val: 0.4433 time: 0.0241s\n",
            "Epoch: 0092 loss_train: 1.1022 acc_train: 0.4929 loss_val: 1.3051 acc_val: 0.4633 time: 0.0223s\n",
            "Epoch: 0093 loss_train: 1.0462 acc_train: 0.5500 loss_val: 1.3257 acc_val: 0.5333 time: 0.0221s\n",
            "Epoch: 0094 loss_train: 1.0872 acc_train: 0.5643 loss_val: 1.3057 acc_val: 0.5100 time: 0.0226s\n",
            "Epoch: 0095 loss_train: 1.0211 acc_train: 0.5571 loss_val: 1.3121 acc_val: 0.4867 time: 0.0259s\n",
            "Epoch: 0096 loss_train: 1.0646 acc_train: 0.5643 loss_val: 1.2984 acc_val: 0.5267 time: 0.0262s\n",
            "Epoch: 0097 loss_train: 1.0290 acc_train: 0.5857 loss_val: 1.2990 acc_val: 0.5533 time: 0.0245s\n",
            "Epoch: 0098 loss_train: 1.0129 acc_train: 0.6071 loss_val: 1.2921 acc_val: 0.5400 time: 0.0232s\n",
            "Epoch: 0099 loss_train: 0.9936 acc_train: 0.5786 loss_val: 1.2905 acc_val: 0.5433 time: 0.0274s\n",
            "Epoch: 0100 loss_train: 0.9811 acc_train: 0.5929 loss_val: 1.2920 acc_val: 0.5400 time: 0.0252s\n",
            "Epoch: 0101 loss_train: 0.9786 acc_train: 0.6000 loss_val: 1.3317 acc_val: 0.4933 time: 0.0257s\n",
            "Epoch: 0102 loss_train: 0.9776 acc_train: 0.6286 loss_val: 1.3029 acc_val: 0.5067 time: 0.0234s\n",
            "Epoch: 0103 loss_train: 0.9511 acc_train: 0.5857 loss_val: 1.2953 acc_val: 0.5467 time: 0.0277s\n",
            "Epoch: 0104 loss_train: 0.9309 acc_train: 0.6357 loss_val: 1.2845 acc_val: 0.5600 time: 0.0232s\n",
            "Epoch: 0105 loss_train: 0.9445 acc_train: 0.6000 loss_val: 1.2790 acc_val: 0.5533 time: 0.0227s\n",
            "Epoch: 0106 loss_train: 0.9538 acc_train: 0.5571 loss_val: 1.2788 acc_val: 0.5567 time: 0.0236s\n",
            "Epoch: 0107 loss_train: 0.9204 acc_train: 0.6429 loss_val: 1.2718 acc_val: 0.5533 time: 0.0231s\n",
            "Epoch: 0108 loss_train: 0.8976 acc_train: 0.6214 loss_val: 1.2776 acc_val: 0.5467 time: 0.0249s\n",
            "Epoch: 0109 loss_train: 0.9227 acc_train: 0.6071 loss_val: 1.2778 acc_val: 0.5600 time: 0.0241s\n",
            "Epoch: 0110 loss_train: 0.8903 acc_train: 0.6500 loss_val: 1.2837 acc_val: 0.5633 time: 0.0234s\n",
            "Epoch: 0111 loss_train: 0.8676 acc_train: 0.6357 loss_val: 1.2740 acc_val: 0.5467 time: 0.0224s\n",
            "Epoch: 0112 loss_train: 0.8522 acc_train: 0.6429 loss_val: 1.2934 acc_val: 0.5467 time: 0.0285s\n",
            "Epoch: 0113 loss_train: 0.8426 acc_train: 0.6357 loss_val: 1.2812 acc_val: 0.5433 time: 0.0226s\n",
            "Epoch: 0114 loss_train: 0.8120 acc_train: 0.6571 loss_val: 1.2875 acc_val: 0.5500 time: 0.0224s\n",
            "Epoch: 0115 loss_train: 0.8076 acc_train: 0.6571 loss_val: 1.2833 acc_val: 0.5367 time: 0.0216s\n",
            "Epoch: 0116 loss_train: 0.8221 acc_train: 0.6571 loss_val: 1.2919 acc_val: 0.5433 time: 0.0303s\n",
            "Epoch: 0117 loss_train: 0.8008 acc_train: 0.6429 loss_val: 1.2837 acc_val: 0.5367 time: 0.0224s\n",
            "Epoch: 0118 loss_train: 0.7561 acc_train: 0.7071 loss_val: 1.2985 acc_val: 0.5433 time: 0.0227s\n",
            "Epoch: 0119 loss_train: 0.8004 acc_train: 0.6643 loss_val: 1.2880 acc_val: 0.5533 time: 0.0232s\n",
            "Epoch: 0120 loss_train: 0.8196 acc_train: 0.6571 loss_val: 1.2970 acc_val: 0.5700 time: 0.0248s\n",
            "Epoch: 0121 loss_train: 0.7428 acc_train: 0.6643 loss_val: 1.3185 acc_val: 0.5600 time: 0.0310s\n",
            "Epoch: 0122 loss_train: 0.8080 acc_train: 0.6643 loss_val: 1.3209 acc_val: 0.5733 time: 0.0204s\n",
            "Epoch: 0123 loss_train: 0.7683 acc_train: 0.6929 loss_val: 1.3523 acc_val: 0.5633 time: 0.0217s\n",
            "Epoch: 0124 loss_train: 0.8007 acc_train: 0.6500 loss_val: 1.3317 acc_val: 0.5733 time: 0.0246s\n",
            "Epoch: 0125 loss_train: 0.7582 acc_train: 0.6786 loss_val: 1.3513 acc_val: 0.5767 time: 0.0227s\n",
            "Epoch: 0126 loss_train: 0.6733 acc_train: 0.7143 loss_val: 1.3688 acc_val: 0.5733 time: 0.0454s\n",
            "Epoch: 0127 loss_train: 0.7505 acc_train: 0.6857 loss_val: 1.4052 acc_val: 0.5500 time: 0.0352s\n",
            "Epoch: 0128 loss_train: 0.7601 acc_train: 0.6714 loss_val: 1.4073 acc_val: 0.5500 time: 0.0306s\n",
            "Epoch: 0129 loss_train: 0.7193 acc_train: 0.7286 loss_val: 1.4128 acc_val: 0.5367 time: 0.0276s\n",
            "Epoch: 0130 loss_train: 0.7140 acc_train: 0.7071 loss_val: 1.4106 acc_val: 0.5533 time: 0.0243s\n",
            "Epoch: 0131 loss_train: 0.7042 acc_train: 0.6929 loss_val: 1.4053 acc_val: 0.5733 time: 0.0247s\n",
            "Epoch: 0132 loss_train: 0.7328 acc_train: 0.7429 loss_val: 1.3841 acc_val: 0.6067 time: 0.0388s\n",
            "Epoch: 0133 loss_train: 0.6940 acc_train: 0.7357 loss_val: 1.3771 acc_val: 0.6233 time: 0.0211s\n",
            "Epoch: 0134 loss_train: 0.6319 acc_train: 0.7429 loss_val: 1.3698 acc_val: 0.6333 time: 0.0225s\n",
            "Epoch: 0135 loss_train: 0.6791 acc_train: 0.7571 loss_val: 1.3751 acc_val: 0.6033 time: 0.0225s\n",
            "Epoch: 0136 loss_train: 0.6965 acc_train: 0.7143 loss_val: 1.3810 acc_val: 0.6233 time: 0.0305s\n",
            "Epoch: 0137 loss_train: 0.6804 acc_train: 0.7429 loss_val: 1.3950 acc_val: 0.6133 time: 0.0237s\n",
            "Epoch: 0138 loss_train: 0.6708 acc_train: 0.7071 loss_val: 1.4007 acc_val: 0.6167 time: 0.0228s\n",
            "Epoch: 0139 loss_train: 0.6680 acc_train: 0.7429 loss_val: 1.4061 acc_val: 0.6067 time: 0.0236s\n",
            "Epoch: 0140 loss_train: 0.6491 acc_train: 0.7571 loss_val: 1.4209 acc_val: 0.6000 time: 0.0233s\n",
            "Epoch: 0141 loss_train: 0.6145 acc_train: 0.8071 loss_val: 1.4463 acc_val: 0.5900 time: 0.0223s\n",
            "Epoch: 0142 loss_train: 0.6735 acc_train: 0.7286 loss_val: 1.4449 acc_val: 0.5967 time: 0.0235s\n",
            "Epoch: 0143 loss_train: 0.6450 acc_train: 0.7714 loss_val: 1.4482 acc_val: 0.6033 time: 0.0231s\n",
            "Epoch: 0144 loss_train: 0.5957 acc_train: 0.7571 loss_val: 1.5195 acc_val: 0.5733 time: 0.0213s\n",
            "Epoch: 0145 loss_train: 0.6800 acc_train: 0.7429 loss_val: 1.4816 acc_val: 0.5933 time: 0.0252s\n",
            "Epoch: 0146 loss_train: 0.6324 acc_train: 0.7429 loss_val: 1.5062 acc_val: 0.5967 time: 0.0222s\n",
            "Epoch: 0147 loss_train: 0.6637 acc_train: 0.7643 loss_val: 1.5028 acc_val: 0.6100 time: 0.0230s\n",
            "Epoch: 0148 loss_train: 0.5586 acc_train: 0.7643 loss_val: 1.5122 acc_val: 0.6100 time: 0.0223s\n",
            "Epoch: 0149 loss_train: 0.6259 acc_train: 0.7500 loss_val: 1.5381 acc_val: 0.5900 time: 0.0227s\n",
            "Epoch: 0150 loss_train: 0.5563 acc_train: 0.7786 loss_val: 1.5360 acc_val: 0.6100 time: 0.0274s\n",
            "Epoch: 0151 loss_train: 0.5917 acc_train: 0.7571 loss_val: 1.5435 acc_val: 0.6167 time: 0.0252s\n",
            "Epoch: 0152 loss_train: 0.5934 acc_train: 0.7643 loss_val: 1.5515 acc_val: 0.6033 time: 0.0232s\n",
            "Epoch: 0153 loss_train: 0.5295 acc_train: 0.8071 loss_val: 1.5597 acc_val: 0.5900 time: 0.0228s\n",
            "Epoch: 0154 loss_train: 0.5885 acc_train: 0.7643 loss_val: 1.5507 acc_val: 0.6200 time: 0.0280s\n",
            "Epoch: 0155 loss_train: 0.5955 acc_train: 0.7643 loss_val: 1.5704 acc_val: 0.5900 time: 0.0218s\n",
            "Epoch: 0156 loss_train: 0.5614 acc_train: 0.7786 loss_val: 1.6215 acc_val: 0.5867 time: 0.0231s\n",
            "Epoch: 0157 loss_train: 0.5983 acc_train: 0.7929 loss_val: 1.5670 acc_val: 0.6100 time: 0.0220s\n",
            "Epoch: 0158 loss_train: 0.5611 acc_train: 0.7929 loss_val: 1.5747 acc_val: 0.6100 time: 0.0308s\n",
            "Epoch: 0159 loss_train: 0.5997 acc_train: 0.7714 loss_val: 1.5984 acc_val: 0.6067 time: 0.0214s\n",
            "Epoch: 0160 loss_train: 0.5934 acc_train: 0.7929 loss_val: 1.6194 acc_val: 0.6100 time: 0.0218s\n",
            "Epoch: 0161 loss_train: 0.5791 acc_train: 0.7786 loss_val: 1.6434 acc_val: 0.5967 time: 0.0221s\n",
            "Epoch: 0162 loss_train: 0.5944 acc_train: 0.7714 loss_val: 1.6449 acc_val: 0.6000 time: 0.0249s\n",
            "Epoch: 0163 loss_train: 0.5538 acc_train: 0.7929 loss_val: 1.6440 acc_val: 0.6000 time: 0.0292s\n",
            "Epoch: 0164 loss_train: 0.5176 acc_train: 0.8000 loss_val: 1.6485 acc_val: 0.5933 time: 0.0226s\n",
            "Epoch: 0165 loss_train: 0.5006 acc_train: 0.8143 loss_val: 1.6476 acc_val: 0.5900 time: 0.0219s\n",
            "Epoch: 0166 loss_train: 0.5710 acc_train: 0.7929 loss_val: 1.6351 acc_val: 0.6033 time: 0.0310s\n",
            "Epoch: 0167 loss_train: 0.5624 acc_train: 0.7571 loss_val: 1.6185 acc_val: 0.6033 time: 0.0220s\n",
            "Epoch: 0168 loss_train: 0.5355 acc_train: 0.7643 loss_val: 1.6383 acc_val: 0.5867 time: 0.0226s\n",
            "Epoch: 0169 loss_train: 0.5364 acc_train: 0.8286 loss_val: 1.6165 acc_val: 0.6033 time: 0.0226s\n",
            "Epoch: 0170 loss_train: 0.5207 acc_train: 0.7500 loss_val: 1.6338 acc_val: 0.6100 time: 0.0251s\n",
            "Epoch: 0171 loss_train: 0.5532 acc_train: 0.7786 loss_val: 1.6628 acc_val: 0.5967 time: 0.0246s\n",
            "Epoch: 0172 loss_train: 0.5578 acc_train: 0.7786 loss_val: 1.7012 acc_val: 0.6033 time: 0.0283s\n",
            "Epoch: 0173 loss_train: 0.5169 acc_train: 0.7571 loss_val: 1.7350 acc_val: 0.6000 time: 0.0227s\n",
            "Epoch: 0174 loss_train: 0.5567 acc_train: 0.7857 loss_val: 1.7582 acc_val: 0.5933 time: 0.0235s\n",
            "Epoch: 0175 loss_train: 0.5554 acc_train: 0.7786 loss_val: 1.7767 acc_val: 0.5767 time: 0.0229s\n",
            "Epoch: 0176 loss_train: 0.5299 acc_train: 0.8286 loss_val: 1.7541 acc_val: 0.6133 time: 0.0254s\n",
            "Epoch: 0177 loss_train: 0.5815 acc_train: 0.7643 loss_val: 1.7463 acc_val: 0.6000 time: 0.0231s\n",
            "Epoch: 0178 loss_train: 0.5212 acc_train: 0.7714 loss_val: 1.7554 acc_val: 0.5800 time: 0.0233s\n",
            "Epoch: 0179 loss_train: 0.5761 acc_train: 0.7714 loss_val: 1.6960 acc_val: 0.6000 time: 0.0244s\n",
            "Epoch: 0180 loss_train: 0.5517 acc_train: 0.7857 loss_val: 1.7051 acc_val: 0.5800 time: 0.0311s\n",
            "Epoch: 0181 loss_train: 0.4865 acc_train: 0.8071 loss_val: 1.7362 acc_val: 0.5900 time: 0.0251s\n",
            "Epoch: 0182 loss_train: 0.5277 acc_train: 0.7571 loss_val: 1.7351 acc_val: 0.5767 time: 0.0234s\n",
            "Epoch: 0183 loss_train: 0.5453 acc_train: 0.7929 loss_val: 1.7423 acc_val: 0.5867 time: 0.0232s\n",
            "Epoch: 0184 loss_train: 0.5552 acc_train: 0.7929 loss_val: 1.7941 acc_val: 0.5833 time: 0.0258s\n",
            "Epoch: 0185 loss_train: 0.5314 acc_train: 0.7571 loss_val: 1.7891 acc_val: 0.5833 time: 0.0238s\n",
            "Epoch: 0186 loss_train: 0.5437 acc_train: 0.7429 loss_val: 1.7973 acc_val: 0.5800 time: 0.0234s\n",
            "Epoch: 0187 loss_train: 0.5580 acc_train: 0.7643 loss_val: 1.7852 acc_val: 0.5967 time: 0.0220s\n",
            "Epoch: 0188 loss_train: 0.4917 acc_train: 0.8000 loss_val: 1.8133 acc_val: 0.6033 time: 0.0273s\n",
            "Epoch: 0189 loss_train: 0.5097 acc_train: 0.7857 loss_val: 1.8305 acc_val: 0.6033 time: 0.0311s\n",
            "Epoch: 0190 loss_train: 0.5216 acc_train: 0.7857 loss_val: 1.8294 acc_val: 0.6000 time: 0.0266s\n",
            "Epoch: 0191 loss_train: 0.4716 acc_train: 0.8286 loss_val: 1.8362 acc_val: 0.6000 time: 0.0252s\n",
            "Epoch: 0192 loss_train: 0.4763 acc_train: 0.8357 loss_val: 1.8301 acc_val: 0.6000 time: 0.0267s\n",
            "Epoch: 0193 loss_train: 0.5014 acc_train: 0.7643 loss_val: 1.8116 acc_val: 0.6000 time: 0.0302s\n",
            "Epoch: 0194 loss_train: 0.5494 acc_train: 0.7500 loss_val: 1.8057 acc_val: 0.5967 time: 0.0227s\n",
            "Epoch: 0195 loss_train: 0.5043 acc_train: 0.8000 loss_val: 1.8119 acc_val: 0.6000 time: 0.0237s\n",
            "Epoch: 0196 loss_train: 0.5247 acc_train: 0.8000 loss_val: 1.8335 acc_val: 0.5867 time: 0.0273s\n",
            "Epoch: 0197 loss_train: 0.5020 acc_train: 0.7929 loss_val: 1.8321 acc_val: 0.6000 time: 0.0240s\n",
            "Epoch: 0198 loss_train: 0.4838 acc_train: 0.8286 loss_val: 1.8500 acc_val: 0.6000 time: 0.0232s\n",
            "Epoch: 0199 loss_train: 0.4764 acc_train: 0.8143 loss_val: 1.8931 acc_val: 0.5867 time: 0.0227s\n",
            "Epoch: 0200 loss_train: 0.5071 acc_train: 0.7786 loss_val: 1.9032 acc_val: 0.5900 time: 0.0230s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 5.1754s\n",
            "Test set results: loss= 1.8659 accuracy= 0.5130\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqIce2x0FACW",
        "outputId": "b153737a-d5f0-414c-ded7-f7c1bd8459f4"
      },
      "source": [
        "#5-layers\n",
        "%run pygcn/train.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cora dataset...\n",
            "Epoch: 0001 loss_train: 1.9884 acc_train: 0.0500 loss_val: 1.9498 acc_val: 0.0667 time: 0.0247s\n",
            "Epoch: 0002 loss_train: 1.9686 acc_train: 0.0857 loss_val: 1.9354 acc_val: 0.0667 time: 0.0240s\n",
            "Epoch: 0003 loss_train: 1.9564 acc_train: 0.0643 loss_val: 1.9213 acc_val: 0.3500 time: 0.0244s\n",
            "Epoch: 0004 loss_train: 1.9376 acc_train: 0.1786 loss_val: 1.9076 acc_val: 0.3500 time: 0.0248s\n",
            "Epoch: 0005 loss_train: 1.9114 acc_train: 0.2571 loss_val: 1.8946 acc_val: 0.3500 time: 0.0256s\n",
            "Epoch: 0006 loss_train: 1.8936 acc_train: 0.2857 loss_val: 1.8825 acc_val: 0.3500 time: 0.0257s\n",
            "Epoch: 0007 loss_train: 1.8809 acc_train: 0.2786 loss_val: 1.8716 acc_val: 0.3500 time: 0.0286s\n",
            "Epoch: 0008 loss_train: 1.8977 acc_train: 0.2714 loss_val: 1.8623 acc_val: 0.3500 time: 0.0262s\n",
            "Epoch: 0009 loss_train: 1.8705 acc_train: 0.2143 loss_val: 1.8543 acc_val: 0.3500 time: 0.0283s\n",
            "Epoch: 0010 loss_train: 1.8571 acc_train: 0.2357 loss_val: 1.8478 acc_val: 0.3500 time: 0.0227s\n",
            "Epoch: 0011 loss_train: 1.8515 acc_train: 0.3000 loss_val: 1.8422 acc_val: 0.3500 time: 0.0252s\n",
            "Epoch: 0012 loss_train: 1.8545 acc_train: 0.2857 loss_val: 1.8359 acc_val: 0.3500 time: 0.0251s\n",
            "Epoch: 0013 loss_train: 1.8282 acc_train: 0.3214 loss_val: 1.8297 acc_val: 0.3500 time: 0.0284s\n",
            "Epoch: 0014 loss_train: 1.8379 acc_train: 0.2786 loss_val: 1.8232 acc_val: 0.3500 time: 0.0246s\n",
            "Epoch: 0015 loss_train: 1.8401 acc_train: 0.3071 loss_val: 1.8167 acc_val: 0.3500 time: 0.0249s\n",
            "Epoch: 0016 loss_train: 1.8504 acc_train: 0.2429 loss_val: 1.8105 acc_val: 0.3500 time: 0.0310s\n",
            "Epoch: 0017 loss_train: 1.8497 acc_train: 0.2786 loss_val: 1.8052 acc_val: 0.3500 time: 0.0293s\n",
            "Epoch: 0018 loss_train: 1.8401 acc_train: 0.3071 loss_val: 1.8008 acc_val: 0.3500 time: 0.0241s\n",
            "Epoch: 0019 loss_train: 1.8217 acc_train: 0.2714 loss_val: 1.7976 acc_val: 0.3500 time: 0.0272s\n",
            "Epoch: 0020 loss_train: 1.8258 acc_train: 0.2857 loss_val: 1.7954 acc_val: 0.3500 time: 0.0256s\n",
            "Epoch: 0021 loss_train: 1.8342 acc_train: 0.2929 loss_val: 1.7944 acc_val: 0.3500 time: 0.0321s\n",
            "Epoch: 0022 loss_train: 1.8290 acc_train: 0.2929 loss_val: 1.7939 acc_val: 0.3500 time: 0.0297s\n",
            "Epoch: 0023 loss_train: 1.8071 acc_train: 0.2929 loss_val: 1.7939 acc_val: 0.3500 time: 0.0296s\n",
            "Epoch: 0024 loss_train: 1.8269 acc_train: 0.2929 loss_val: 1.7942 acc_val: 0.3500 time: 0.0311s\n",
            "Epoch: 0025 loss_train: 1.8035 acc_train: 0.2929 loss_val: 1.7946 acc_val: 0.3500 time: 0.0294s\n",
            "Epoch: 0026 loss_train: 1.8021 acc_train: 0.2929 loss_val: 1.7952 acc_val: 0.3500 time: 0.0273s\n",
            "Epoch: 0027 loss_train: 1.8321 acc_train: 0.2929 loss_val: 1.7961 acc_val: 0.3500 time: 0.0262s\n",
            "Epoch: 0028 loss_train: 1.8272 acc_train: 0.2929 loss_val: 1.7973 acc_val: 0.3500 time: 0.0269s\n",
            "Epoch: 0029 loss_train: 1.8271 acc_train: 0.2929 loss_val: 1.7982 acc_val: 0.3500 time: 0.0258s\n",
            "Epoch: 0030 loss_train: 1.8268 acc_train: 0.2857 loss_val: 1.7992 acc_val: 0.3500 time: 0.0237s\n",
            "Epoch: 0031 loss_train: 1.8426 acc_train: 0.2929 loss_val: 1.7997 acc_val: 0.3500 time: 0.0304s\n",
            "Epoch: 0032 loss_train: 1.8284 acc_train: 0.2857 loss_val: 1.7998 acc_val: 0.3500 time: 0.0319s\n",
            "Epoch: 0033 loss_train: 1.8222 acc_train: 0.2857 loss_val: 1.7992 acc_val: 0.3500 time: 0.0274s\n",
            "Epoch: 0034 loss_train: 1.8165 acc_train: 0.3000 loss_val: 1.7978 acc_val: 0.3500 time: 0.0252s\n",
            "Epoch: 0035 loss_train: 1.8101 acc_train: 0.3000 loss_val: 1.7953 acc_val: 0.3500 time: 0.0261s\n",
            "Epoch: 0036 loss_train: 1.7997 acc_train: 0.3071 loss_val: 1.7909 acc_val: 0.3500 time: 0.0263s\n",
            "Epoch: 0037 loss_train: 1.7954 acc_train: 0.2929 loss_val: 1.7853 acc_val: 0.3500 time: 0.0343s\n",
            "Epoch: 0038 loss_train: 1.7955 acc_train: 0.2929 loss_val: 1.7770 acc_val: 0.3500 time: 0.0273s\n",
            "Epoch: 0039 loss_train: 1.7622 acc_train: 0.2929 loss_val: 1.7648 acc_val: 0.3500 time: 0.0243s\n",
            "Epoch: 0040 loss_train: 1.7651 acc_train: 0.3000 loss_val: 1.7504 acc_val: 0.3500 time: 0.0300s\n",
            "Epoch: 0041 loss_train: 1.7655 acc_train: 0.3143 loss_val: 1.7313 acc_val: 0.3500 time: 0.0268s\n",
            "Epoch: 0042 loss_train: 1.7357 acc_train: 0.2857 loss_val: 1.7077 acc_val: 0.3500 time: 0.0285s\n",
            "Epoch: 0043 loss_train: 1.6804 acc_train: 0.2929 loss_val: 1.6797 acc_val: 0.3500 time: 0.0263s\n",
            "Epoch: 0044 loss_train: 1.6724 acc_train: 0.2643 loss_val: 1.6476 acc_val: 0.3500 time: 0.0306s\n",
            "Epoch: 0045 loss_train: 1.6343 acc_train: 0.3071 loss_val: 1.6163 acc_val: 0.3500 time: 0.0263s\n",
            "Epoch: 0046 loss_train: 1.5933 acc_train: 0.3000 loss_val: 1.5753 acc_val: 0.3533 time: 0.0269s\n",
            "Epoch: 0047 loss_train: 1.5544 acc_train: 0.3357 loss_val: 1.5375 acc_val: 0.3700 time: 0.0267s\n",
            "Epoch: 0048 loss_train: 1.5250 acc_train: 0.2929 loss_val: 1.5082 acc_val: 0.3767 time: 0.0298s\n",
            "Epoch: 0049 loss_train: 1.4759 acc_train: 0.3429 loss_val: 1.4883 acc_val: 0.3733 time: 0.0259s\n",
            "Epoch: 0050 loss_train: 1.4730 acc_train: 0.3429 loss_val: 1.4692 acc_val: 0.3767 time: 0.0246s\n",
            "Epoch: 0051 loss_train: 1.4276 acc_train: 0.3214 loss_val: 1.4483 acc_val: 0.3767 time: 0.0247s\n",
            "Epoch: 0052 loss_train: 1.4204 acc_train: 0.3143 loss_val: 1.4306 acc_val: 0.3767 time: 0.0245s\n",
            "Epoch: 0053 loss_train: 1.3944 acc_train: 0.3143 loss_val: 1.4233 acc_val: 0.3967 time: 0.0283s\n",
            "Epoch: 0054 loss_train: 1.3804 acc_train: 0.3571 loss_val: 1.4162 acc_val: 0.4633 time: 0.0250s\n",
            "Epoch: 0055 loss_train: 1.4327 acc_train: 0.2429 loss_val: 1.4242 acc_val: 0.4667 time: 0.0246s\n",
            "Epoch: 0056 loss_train: 1.3639 acc_train: 0.4286 loss_val: 1.4061 acc_val: 0.4467 time: 0.0309s\n",
            "Epoch: 0057 loss_train: 1.3359 acc_train: 0.4571 loss_val: 1.3894 acc_val: 0.4733 time: 0.0248s\n",
            "Epoch: 0058 loss_train: 1.3200 acc_train: 0.3786 loss_val: 1.3813 acc_val: 0.4533 time: 0.0261s\n",
            "Epoch: 0059 loss_train: 1.3244 acc_train: 0.3643 loss_val: 1.3860 acc_val: 0.4833 time: 0.0381s\n",
            "Epoch: 0060 loss_train: 1.2813 acc_train: 0.4571 loss_val: 1.3821 acc_val: 0.4567 time: 0.0307s\n",
            "Epoch: 0061 loss_train: 1.2707 acc_train: 0.4286 loss_val: 1.3700 acc_val: 0.3533 time: 0.0271s\n",
            "Epoch: 0062 loss_train: 1.2478 acc_train: 0.4214 loss_val: 1.3631 acc_val: 0.4333 time: 0.0284s\n",
            "Epoch: 0063 loss_train: 1.2407 acc_train: 0.4571 loss_val: 1.3906 acc_val: 0.4967 time: 0.0365s\n",
            "Epoch: 0064 loss_train: 1.2343 acc_train: 0.5214 loss_val: 1.3252 acc_val: 0.5033 time: 0.0263s\n",
            "Epoch: 0065 loss_train: 1.2156 acc_train: 0.4786 loss_val: 1.3240 acc_val: 0.4200 time: 0.0239s\n",
            "Epoch: 0066 loss_train: 1.1928 acc_train: 0.5000 loss_val: 1.2903 acc_val: 0.5167 time: 0.0274s\n",
            "Epoch: 0067 loss_train: 1.1870 acc_train: 0.5143 loss_val: 1.3320 acc_val: 0.5067 time: 0.0326s\n",
            "Epoch: 0068 loss_train: 1.1921 acc_train: 0.5500 loss_val: 1.2877 acc_val: 0.5300 time: 0.0278s\n",
            "Epoch: 0069 loss_train: 1.1632 acc_train: 0.6000 loss_val: 1.2630 acc_val: 0.5433 time: 0.0257s\n",
            "Epoch: 0070 loss_train: 1.1027 acc_train: 0.5929 loss_val: 1.3050 acc_val: 0.4633 time: 0.0274s\n",
            "Epoch: 0071 loss_train: 1.1573 acc_train: 0.4857 loss_val: 1.2364 acc_val: 0.5633 time: 0.0309s\n",
            "Epoch: 0072 loss_train: 1.0808 acc_train: 0.6143 loss_val: 1.2818 acc_val: 0.5600 time: 0.0259s\n",
            "Epoch: 0073 loss_train: 1.1127 acc_train: 0.5143 loss_val: 1.2525 acc_val: 0.5633 time: 0.0266s\n",
            "Epoch: 0074 loss_train: 1.0764 acc_train: 0.5857 loss_val: 1.2093 acc_val: 0.5600 time: 0.0289s\n",
            "Epoch: 0075 loss_train: 1.0667 acc_train: 0.5714 loss_val: 1.1937 acc_val: 0.5700 time: 0.0274s\n",
            "Epoch: 0076 loss_train: 1.0583 acc_train: 0.5857 loss_val: 1.1789 acc_val: 0.5733 time: 0.0257s\n",
            "Epoch: 0077 loss_train: 1.0137 acc_train: 0.6071 loss_val: 1.1986 acc_val: 0.5700 time: 0.0268s\n",
            "Epoch: 0078 loss_train: 1.0203 acc_train: 0.6429 loss_val: 1.1534 acc_val: 0.5767 time: 0.0263s\n",
            "Epoch: 0079 loss_train: 0.9748 acc_train: 0.6071 loss_val: 1.1432 acc_val: 0.5600 time: 0.0322s\n",
            "Epoch: 0080 loss_train: 1.0012 acc_train: 0.5714 loss_val: 1.1275 acc_val: 0.5633 time: 0.0262s\n",
            "Epoch: 0081 loss_train: 0.9277 acc_train: 0.6071 loss_val: 1.1347 acc_val: 0.5833 time: 0.0253s\n",
            "Epoch: 0082 loss_train: 0.9180 acc_train: 0.6429 loss_val: 1.1418 acc_val: 0.5800 time: 0.0268s\n",
            "Epoch: 0083 loss_train: 0.9023 acc_train: 0.7000 loss_val: 1.1045 acc_val: 0.5833 time: 0.0260s\n",
            "Epoch: 0084 loss_train: 0.8990 acc_train: 0.6286 loss_val: 1.0986 acc_val: 0.6000 time: 0.0259s\n",
            "Epoch: 0085 loss_train: 0.8698 acc_train: 0.7000 loss_val: 1.1217 acc_val: 0.5767 time: 0.0252s\n",
            "Epoch: 0086 loss_train: 0.8962 acc_train: 0.6714 loss_val: 1.0660 acc_val: 0.6100 time: 0.0260s\n",
            "Epoch: 0087 loss_train: 0.8258 acc_train: 0.6714 loss_val: 1.0501 acc_val: 0.6133 time: 0.0289s\n",
            "Epoch: 0088 loss_train: 0.8135 acc_train: 0.6571 loss_val: 1.0622 acc_val: 0.6233 time: 0.0238s\n",
            "Epoch: 0089 loss_train: 0.8080 acc_train: 0.6786 loss_val: 1.0399 acc_val: 0.6200 time: 0.0238s\n",
            "Epoch: 0090 loss_train: 0.7771 acc_train: 0.6786 loss_val: 1.0399 acc_val: 0.6200 time: 0.0299s\n",
            "Epoch: 0091 loss_train: 0.7769 acc_train: 0.6786 loss_val: 1.0531 acc_val: 0.6167 time: 0.0276s\n",
            "Epoch: 0092 loss_train: 0.8316 acc_train: 0.6571 loss_val: 1.0437 acc_val: 0.6333 time: 0.0292s\n",
            "Epoch: 0093 loss_train: 0.7111 acc_train: 0.7429 loss_val: 1.0495 acc_val: 0.6300 time: 0.0298s\n",
            "Epoch: 0094 loss_train: 0.7221 acc_train: 0.7143 loss_val: 1.0597 acc_val: 0.6067 time: 0.0303s\n",
            "Epoch: 0095 loss_train: 0.7333 acc_train: 0.6929 loss_val: 1.0575 acc_val: 0.6367 time: 0.0318s\n",
            "Epoch: 0096 loss_train: 0.7416 acc_train: 0.7000 loss_val: 1.0525 acc_val: 0.6267 time: 0.0278s\n",
            "Epoch: 0097 loss_train: 0.6590 acc_train: 0.7571 loss_val: 1.0489 acc_val: 0.6233 time: 0.0281s\n",
            "Epoch: 0098 loss_train: 0.6964 acc_train: 0.7214 loss_val: 1.0552 acc_val: 0.6300 time: 0.0293s\n",
            "Epoch: 0099 loss_train: 0.6621 acc_train: 0.7143 loss_val: 1.0616 acc_val: 0.6300 time: 0.0269s\n",
            "Epoch: 0100 loss_train: 0.6855 acc_train: 0.7500 loss_val: 1.0857 acc_val: 0.6233 time: 0.0258s\n",
            "Epoch: 0101 loss_train: 0.5991 acc_train: 0.8071 loss_val: 1.1240 acc_val: 0.6067 time: 0.0242s\n",
            "Epoch: 0102 loss_train: 0.6774 acc_train: 0.6643 loss_val: 1.2389 acc_val: 0.5800 time: 0.0369s\n",
            "Epoch: 0103 loss_train: 0.7087 acc_train: 0.7071 loss_val: 1.1456 acc_val: 0.6267 time: 0.0234s\n",
            "Epoch: 0104 loss_train: 0.5962 acc_train: 0.7429 loss_val: 1.1734 acc_val: 0.6433 time: 0.0253s\n",
            "Epoch: 0105 loss_train: 0.6412 acc_train: 0.7500 loss_val: 1.1498 acc_val: 0.6267 time: 0.0305s\n",
            "Epoch: 0106 loss_train: 0.5817 acc_train: 0.7857 loss_val: 1.1771 acc_val: 0.6133 time: 0.0279s\n",
            "Epoch: 0107 loss_train: 0.6019 acc_train: 0.7286 loss_val: 1.1682 acc_val: 0.6367 time: 0.0244s\n",
            "Epoch: 0108 loss_train: 0.6062 acc_train: 0.7500 loss_val: 1.1998 acc_val: 0.6400 time: 0.0248s\n",
            "Epoch: 0109 loss_train: 0.6118 acc_train: 0.7071 loss_val: 1.1731 acc_val: 0.6400 time: 0.0247s\n",
            "Epoch: 0110 loss_train: 0.5439 acc_train: 0.7571 loss_val: 1.1972 acc_val: 0.6367 time: 0.0303s\n",
            "Epoch: 0111 loss_train: 0.5984 acc_train: 0.7286 loss_val: 1.1772 acc_val: 0.6467 time: 0.0244s\n",
            "Epoch: 0112 loss_train: 0.5606 acc_train: 0.7357 loss_val: 1.2101 acc_val: 0.6400 time: 0.0256s\n",
            "Epoch: 0113 loss_train: 0.5466 acc_train: 0.7429 loss_val: 1.1937 acc_val: 0.6300 time: 0.0272s\n",
            "Epoch: 0114 loss_train: 0.6230 acc_train: 0.7643 loss_val: 1.2223 acc_val: 0.6167 time: 0.0253s\n",
            "Epoch: 0115 loss_train: 0.5406 acc_train: 0.7929 loss_val: 1.2400 acc_val: 0.6333 time: 0.0223s\n",
            "Epoch: 0116 loss_train: 0.5547 acc_train: 0.7357 loss_val: 1.2675 acc_val: 0.6333 time: 0.0258s\n",
            "Epoch: 0117 loss_train: 0.5362 acc_train: 0.7357 loss_val: 1.2835 acc_val: 0.6200 time: 0.0248s\n",
            "Epoch: 0118 loss_train: 0.5137 acc_train: 0.7786 loss_val: 1.3134 acc_val: 0.6033 time: 0.0283s\n",
            "Epoch: 0119 loss_train: 0.5191 acc_train: 0.7857 loss_val: 1.3046 acc_val: 0.6233 time: 0.0225s\n",
            "Epoch: 0120 loss_train: 0.5187 acc_train: 0.7643 loss_val: 1.3904 acc_val: 0.6333 time: 0.0252s\n",
            "Epoch: 0121 loss_train: 0.5321 acc_train: 0.7786 loss_val: 1.2999 acc_val: 0.6200 time: 0.0258s\n",
            "Epoch: 0122 loss_train: 0.5007 acc_train: 0.7857 loss_val: 1.3170 acc_val: 0.6133 time: 0.0295s\n",
            "Epoch: 0123 loss_train: 0.4886 acc_train: 0.8071 loss_val: 1.2881 acc_val: 0.6300 time: 0.0255s\n",
            "Epoch: 0124 loss_train: 0.4520 acc_train: 0.8357 loss_val: 1.3344 acc_val: 0.6267 time: 0.0263s\n",
            "Epoch: 0125 loss_train: 0.4724 acc_train: 0.8214 loss_val: 1.2888 acc_val: 0.6267 time: 0.0258s\n",
            "Epoch: 0126 loss_train: 0.4986 acc_train: 0.7857 loss_val: 1.3011 acc_val: 0.6267 time: 0.0303s\n",
            "Epoch: 0127 loss_train: 0.4978 acc_train: 0.7857 loss_val: 1.3885 acc_val: 0.6233 time: 0.0237s\n",
            "Epoch: 0128 loss_train: 0.5376 acc_train: 0.7929 loss_val: 1.3114 acc_val: 0.6267 time: 0.0250s\n",
            "Epoch: 0129 loss_train: 0.4443 acc_train: 0.8071 loss_val: 1.3551 acc_val: 0.6200 time: 0.0264s\n",
            "Epoch: 0130 loss_train: 0.5116 acc_train: 0.8214 loss_val: 1.3325 acc_val: 0.6300 time: 0.0244s\n",
            "Epoch: 0131 loss_train: 0.4483 acc_train: 0.8429 loss_val: 1.4467 acc_val: 0.6367 time: 0.0262s\n",
            "Epoch: 0132 loss_train: 0.4749 acc_train: 0.7786 loss_val: 1.4697 acc_val: 0.6300 time: 0.0256s\n",
            "Epoch: 0133 loss_train: 0.5020 acc_train: 0.7929 loss_val: 1.4568 acc_val: 0.5967 time: 0.0279s\n",
            "Epoch: 0134 loss_train: 0.5516 acc_train: 0.7643 loss_val: 1.4495 acc_val: 0.6100 time: 0.0275s\n",
            "Epoch: 0135 loss_train: 0.4905 acc_train: 0.7643 loss_val: 1.5956 acc_val: 0.6233 time: 0.0248s\n",
            "Epoch: 0136 loss_train: 0.4878 acc_train: 0.7786 loss_val: 1.5355 acc_val: 0.6067 time: 0.0266s\n",
            "Epoch: 0137 loss_train: 0.4077 acc_train: 0.8214 loss_val: 1.4761 acc_val: 0.6033 time: 0.0258s\n",
            "Epoch: 0138 loss_train: 0.4568 acc_train: 0.8071 loss_val: 1.4554 acc_val: 0.6133 time: 0.0234s\n",
            "Epoch: 0139 loss_train: 0.4090 acc_train: 0.8357 loss_val: 1.4412 acc_val: 0.6067 time: 0.0334s\n",
            "Epoch: 0140 loss_train: 0.4296 acc_train: 0.8571 loss_val: 1.4638 acc_val: 0.6100 time: 0.0268s\n",
            "Epoch: 0141 loss_train: 0.4265 acc_train: 0.8143 loss_val: 1.4921 acc_val: 0.6200 time: 0.0274s\n",
            "Epoch: 0142 loss_train: 0.4250 acc_train: 0.8214 loss_val: 1.4423 acc_val: 0.6233 time: 0.0296s\n",
            "Epoch: 0143 loss_train: 0.4134 acc_train: 0.8429 loss_val: 1.4452 acc_val: 0.6333 time: 0.0232s\n",
            "Epoch: 0144 loss_train: 0.4317 acc_train: 0.8357 loss_val: 1.5508 acc_val: 0.6233 time: 0.0255s\n",
            "Epoch: 0145 loss_train: 0.4856 acc_train: 0.7500 loss_val: 1.4750 acc_val: 0.6133 time: 0.0234s\n",
            "Epoch: 0146 loss_train: 0.4515 acc_train: 0.8000 loss_val: 1.5097 acc_val: 0.6000 time: 0.0281s\n",
            "Epoch: 0147 loss_train: 0.4700 acc_train: 0.8071 loss_val: 1.5466 acc_val: 0.6200 time: 0.0245s\n",
            "Epoch: 0148 loss_train: 0.4496 acc_train: 0.8000 loss_val: 1.6459 acc_val: 0.6167 time: 0.0262s\n",
            "Epoch: 0149 loss_train: 0.4818 acc_train: 0.7786 loss_val: 1.6141 acc_val: 0.5900 time: 0.0313s\n",
            "Epoch: 0150 loss_train: 0.4919 acc_train: 0.7857 loss_val: 1.6118 acc_val: 0.5900 time: 0.0261s\n",
            "Epoch: 0151 loss_train: 0.4140 acc_train: 0.8214 loss_val: 1.6894 acc_val: 0.6200 time: 0.0239s\n",
            "Epoch: 0152 loss_train: 0.4193 acc_train: 0.7929 loss_val: 1.7946 acc_val: 0.6100 time: 0.0239s\n",
            "Epoch: 0153 loss_train: 0.4337 acc_train: 0.8214 loss_val: 1.6194 acc_val: 0.6033 time: 0.0242s\n",
            "Epoch: 0154 loss_train: 0.3893 acc_train: 0.8500 loss_val: 1.6036 acc_val: 0.5867 time: 0.0270s\n",
            "Epoch: 0155 loss_train: 0.3812 acc_train: 0.8143 loss_val: 1.5880 acc_val: 0.6167 time: 0.0250s\n",
            "Epoch: 0156 loss_train: 0.3948 acc_train: 0.8429 loss_val: 1.6610 acc_val: 0.6200 time: 0.0246s\n",
            "Epoch: 0157 loss_train: 0.4220 acc_train: 0.8429 loss_val: 1.7039 acc_val: 0.6267 time: 0.0308s\n",
            "Epoch: 0158 loss_train: 0.4516 acc_train: 0.7929 loss_val: 1.5998 acc_val: 0.6100 time: 0.0247s\n",
            "Epoch: 0159 loss_train: 0.3846 acc_train: 0.8357 loss_val: 1.6584 acc_val: 0.5833 time: 0.0259s\n",
            "Epoch: 0160 loss_train: 0.5084 acc_train: 0.8000 loss_val: 1.6572 acc_val: 0.6067 time: 0.0256s\n",
            "Epoch: 0161 loss_train: 0.4571 acc_train: 0.8143 loss_val: 1.7612 acc_val: 0.6033 time: 0.0298s\n",
            "Epoch: 0162 loss_train: 0.4452 acc_train: 0.8143 loss_val: 1.6346 acc_val: 0.6067 time: 0.0255s\n",
            "Epoch: 0163 loss_train: 0.3886 acc_train: 0.8500 loss_val: 1.6836 acc_val: 0.5967 time: 0.0255s\n",
            "Epoch: 0164 loss_train: 0.4652 acc_train: 0.7929 loss_val: 1.7316 acc_val: 0.6100 time: 0.0251s\n",
            "Epoch: 0165 loss_train: 0.3530 acc_train: 0.8571 loss_val: 1.8417 acc_val: 0.6133 time: 0.0413s\n",
            "Epoch: 0166 loss_train: 0.4788 acc_train: 0.7714 loss_val: 1.6932 acc_val: 0.6133 time: 0.0348s\n",
            "Epoch: 0167 loss_train: 0.3943 acc_train: 0.8214 loss_val: 1.7977 acc_val: 0.5633 time: 0.0271s\n",
            "Epoch: 0168 loss_train: 0.4749 acc_train: 0.8071 loss_val: 1.7388 acc_val: 0.5900 time: 0.0280s\n",
            "Epoch: 0169 loss_train: 0.4396 acc_train: 0.7714 loss_val: 1.9568 acc_val: 0.6133 time: 0.0282s\n",
            "Epoch: 0170 loss_train: 0.5065 acc_train: 0.7357 loss_val: 1.8215 acc_val: 0.5967 time: 0.0288s\n",
            "Epoch: 0171 loss_train: 0.3879 acc_train: 0.8500 loss_val: 1.7498 acc_val: 0.6000 time: 0.0249s\n",
            "Epoch: 0172 loss_train: 0.3778 acc_train: 0.8571 loss_val: 1.7398 acc_val: 0.5967 time: 0.0257s\n",
            "Epoch: 0173 loss_train: 0.4371 acc_train: 0.8143 loss_val: 1.7092 acc_val: 0.6133 time: 0.0296s\n",
            "Epoch: 0174 loss_train: 0.3534 acc_train: 0.8143 loss_val: 1.7588 acc_val: 0.6200 time: 0.0241s\n",
            "Epoch: 0175 loss_train: 0.3758 acc_train: 0.8357 loss_val: 1.7201 acc_val: 0.6200 time: 0.0308s\n",
            "Epoch: 0176 loss_train: 0.3426 acc_train: 0.8286 loss_val: 1.6551 acc_val: 0.6133 time: 0.0300s\n",
            "Epoch: 0177 loss_train: 0.3650 acc_train: 0.8143 loss_val: 1.6630 acc_val: 0.6033 time: 0.0247s\n",
            "Epoch: 0178 loss_train: 0.4121 acc_train: 0.8429 loss_val: 1.6873 acc_val: 0.6133 time: 0.0254s\n",
            "Epoch: 0179 loss_train: 0.3590 acc_train: 0.8429 loss_val: 1.7377 acc_val: 0.6200 time: 0.0230s\n",
            "Epoch: 0180 loss_train: 0.4001 acc_train: 0.8286 loss_val: 1.7395 acc_val: 0.6167 time: 0.0247s\n",
            "Epoch: 0181 loss_train: 0.4071 acc_train: 0.8214 loss_val: 1.7147 acc_val: 0.6167 time: 0.0303s\n",
            "Epoch: 0182 loss_train: 0.3506 acc_train: 0.8500 loss_val: 1.7469 acc_val: 0.6167 time: 0.0256s\n",
            "Epoch: 0183 loss_train: 0.3680 acc_train: 0.8000 loss_val: 1.7675 acc_val: 0.6167 time: 0.0260s\n",
            "Epoch: 0184 loss_train: 0.4172 acc_train: 0.7857 loss_val: 1.7565 acc_val: 0.6067 time: 0.0275s\n",
            "Epoch: 0185 loss_train: 0.3734 acc_train: 0.8429 loss_val: 1.7850 acc_val: 0.6033 time: 0.0284s\n",
            "Epoch: 0186 loss_train: 0.3928 acc_train: 0.8357 loss_val: 1.8093 acc_val: 0.6000 time: 0.0262s\n",
            "Epoch: 0187 loss_train: 0.3504 acc_train: 0.8571 loss_val: 1.8230 acc_val: 0.6000 time: 0.0265s\n",
            "Epoch: 0188 loss_train: 0.3594 acc_train: 0.8429 loss_val: 1.8447 acc_val: 0.6033 time: 0.0272s\n",
            "Epoch: 0189 loss_train: 0.3607 acc_train: 0.8357 loss_val: 1.8075 acc_val: 0.6033 time: 0.0281s\n",
            "Epoch: 0190 loss_train: 0.3800 acc_train: 0.8357 loss_val: 1.7656 acc_val: 0.6100 time: 0.0245s\n",
            "Epoch: 0191 loss_train: 0.3767 acc_train: 0.8000 loss_val: 1.7616 acc_val: 0.6100 time: 0.0241s\n",
            "Epoch: 0192 loss_train: 0.3320 acc_train: 0.8643 loss_val: 1.7964 acc_val: 0.6267 time: 0.0231s\n",
            "Epoch: 0193 loss_train: 0.3604 acc_train: 0.8286 loss_val: 1.8867 acc_val: 0.6133 time: 0.0312s\n",
            "Epoch: 0194 loss_train: 0.4502 acc_train: 0.8071 loss_val: 1.7982 acc_val: 0.6067 time: 0.0251s\n",
            "Epoch: 0195 loss_train: 0.3416 acc_train: 0.8429 loss_val: 1.8192 acc_val: 0.6067 time: 0.0276s\n",
            "Epoch: 0196 loss_train: 0.4460 acc_train: 0.8571 loss_val: 1.8987 acc_val: 0.6000 time: 0.0277s\n",
            "Epoch: 0197 loss_train: 0.3880 acc_train: 0.8214 loss_val: 1.9792 acc_val: 0.6033 time: 0.0327s\n",
            "Epoch: 0198 loss_train: 0.3704 acc_train: 0.8286 loss_val: 1.8506 acc_val: 0.6200 time: 0.0226s\n",
            "Epoch: 0199 loss_train: 0.3992 acc_train: 0.8143 loss_val: 1.8236 acc_val: 0.6100 time: 0.0248s\n",
            "Epoch: 0200 loss_train: 0.3390 acc_train: 0.8357 loss_val: 1.8141 acc_val: 0.6100 time: 0.0240s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 5.6481s\n",
            "Test set results: loss= 2.0850 accuracy= 0.5200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgyZ4fXkFBoj",
        "outputId": "6ab4028b-c77b-44b2-8e7b-e4a2fbae6479"
      },
      "source": [
        "#10-layers\n",
        "%run pygcn/train.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cora dataset...\n",
            "Epoch: 0001 loss_train: 1.9716 acc_train: 0.1500 loss_val: 1.9590 acc_val: 0.1267 time: 0.0374s\n",
            "Epoch: 0002 loss_train: 1.9586 acc_train: 0.1429 loss_val: 1.9369 acc_val: 0.1267 time: 0.0325s\n",
            "Epoch: 0003 loss_train: 1.9393 acc_train: 0.1429 loss_val: 1.9152 acc_val: 0.1267 time: 0.0317s\n",
            "Epoch: 0004 loss_train: 1.9213 acc_train: 0.1429 loss_val: 1.8933 acc_val: 0.3500 time: 0.0325s\n",
            "Epoch: 0005 loss_train: 1.8919 acc_train: 0.2500 loss_val: 1.8737 acc_val: 0.3500 time: 0.0332s\n",
            "Epoch: 0006 loss_train: 1.8911 acc_train: 0.2500 loss_val: 1.8566 acc_val: 0.3500 time: 0.0411s\n",
            "Epoch: 0007 loss_train: 1.8621 acc_train: 0.2714 loss_val: 1.8408 acc_val: 0.3500 time: 0.0398s\n",
            "Epoch: 0008 loss_train: 1.8594 acc_train: 0.2643 loss_val: 1.8280 acc_val: 0.3500 time: 0.0346s\n",
            "Epoch: 0009 loss_train: 1.8550 acc_train: 0.2857 loss_val: 1.8198 acc_val: 0.3500 time: 0.0353s\n",
            "Epoch: 0010 loss_train: 1.8449 acc_train: 0.2714 loss_val: 1.8159 acc_val: 0.3500 time: 0.0345s\n",
            "Epoch: 0011 loss_train: 1.8507 acc_train: 0.2857 loss_val: 1.8132 acc_val: 0.3500 time: 0.0362s\n",
            "Epoch: 0012 loss_train: 1.8459 acc_train: 0.2786 loss_val: 1.8099 acc_val: 0.3500 time: 0.0407s\n",
            "Epoch: 0013 loss_train: 1.8461 acc_train: 0.2929 loss_val: 1.8065 acc_val: 0.3500 time: 0.0441s\n",
            "Epoch: 0014 loss_train: 1.8326 acc_train: 0.2929 loss_val: 1.8037 acc_val: 0.3500 time: 0.0323s\n",
            "Epoch: 0015 loss_train: 1.8609 acc_train: 0.2929 loss_val: 1.8018 acc_val: 0.3500 time: 0.0344s\n",
            "Epoch: 0016 loss_train: 1.8449 acc_train: 0.2857 loss_val: 1.8013 acc_val: 0.3500 time: 0.0420s\n",
            "Epoch: 0017 loss_train: 1.8213 acc_train: 0.2929 loss_val: 1.8016 acc_val: 0.3500 time: 0.0382s\n",
            "Epoch: 0018 loss_train: 1.8390 acc_train: 0.2929 loss_val: 1.8026 acc_val: 0.3500 time: 0.0378s\n",
            "Epoch: 0019 loss_train: 1.8033 acc_train: 0.2929 loss_val: 1.8035 acc_val: 0.3500 time: 0.0385s\n",
            "Epoch: 0020 loss_train: 1.8221 acc_train: 0.2929 loss_val: 1.8041 acc_val: 0.3500 time: 0.0361s\n",
            "Epoch: 0021 loss_train: 1.8191 acc_train: 0.2929 loss_val: 1.8046 acc_val: 0.3500 time: 0.0367s\n",
            "Epoch: 0022 loss_train: 1.8328 acc_train: 0.2929 loss_val: 1.8049 acc_val: 0.3500 time: 0.0338s\n",
            "Epoch: 0023 loss_train: 1.8343 acc_train: 0.2929 loss_val: 1.8051 acc_val: 0.3500 time: 0.0350s\n",
            "Epoch: 0024 loss_train: 1.8145 acc_train: 0.2929 loss_val: 1.8048 acc_val: 0.3500 time: 0.0346s\n",
            "Epoch: 0025 loss_train: 1.8457 acc_train: 0.2929 loss_val: 1.8047 acc_val: 0.3500 time: 0.0376s\n",
            "Epoch: 0026 loss_train: 1.8632 acc_train: 0.2929 loss_val: 1.8048 acc_val: 0.3500 time: 0.0343s\n",
            "Epoch: 0027 loss_train: 1.8318 acc_train: 0.2929 loss_val: 1.8049 acc_val: 0.3500 time: 0.0340s\n",
            "Epoch: 0028 loss_train: 1.8158 acc_train: 0.2929 loss_val: 1.8047 acc_val: 0.3500 time: 0.0409s\n",
            "Epoch: 0029 loss_train: 1.8250 acc_train: 0.2929 loss_val: 1.8043 acc_val: 0.3500 time: 0.0351s\n",
            "Epoch: 0030 loss_train: 1.8137 acc_train: 0.2929 loss_val: 1.8040 acc_val: 0.3500 time: 0.0336s\n",
            "Epoch: 0031 loss_train: 1.8161 acc_train: 0.2929 loss_val: 1.8035 acc_val: 0.3500 time: 0.0359s\n",
            "Epoch: 0032 loss_train: 1.8143 acc_train: 0.2929 loss_val: 1.8031 acc_val: 0.3500 time: 0.0348s\n",
            "Epoch: 0033 loss_train: 1.8184 acc_train: 0.2929 loss_val: 1.8027 acc_val: 0.3500 time: 0.0396s\n",
            "Epoch: 0034 loss_train: 1.8146 acc_train: 0.2929 loss_val: 1.8025 acc_val: 0.3500 time: 0.0369s\n",
            "Epoch: 0035 loss_train: 1.8432 acc_train: 0.2929 loss_val: 1.8023 acc_val: 0.3500 time: 0.0359s\n",
            "Epoch: 0036 loss_train: 1.7962 acc_train: 0.2929 loss_val: 1.8022 acc_val: 0.3500 time: 0.0355s\n",
            "Epoch: 0037 loss_train: 1.8254 acc_train: 0.2929 loss_val: 1.8025 acc_val: 0.3500 time: 0.0414s\n",
            "Epoch: 0038 loss_train: 1.8460 acc_train: 0.2929 loss_val: 1.8028 acc_val: 0.3500 time: 0.0334s\n",
            "Epoch: 0039 loss_train: 1.8150 acc_train: 0.2929 loss_val: 1.8032 acc_val: 0.3500 time: 0.0337s\n",
            "Epoch: 0040 loss_train: 1.8303 acc_train: 0.2929 loss_val: 1.8039 acc_val: 0.3500 time: 0.0377s\n",
            "Epoch: 0041 loss_train: 1.8411 acc_train: 0.2929 loss_val: 1.8046 acc_val: 0.3500 time: 0.0329s\n",
            "Epoch: 0042 loss_train: 1.8180 acc_train: 0.2929 loss_val: 1.8053 acc_val: 0.3500 time: 0.0353s\n",
            "Epoch: 0043 loss_train: 1.8381 acc_train: 0.2929 loss_val: 1.8060 acc_val: 0.3500 time: 0.0381s\n",
            "Epoch: 0044 loss_train: 1.8322 acc_train: 0.2929 loss_val: 1.8067 acc_val: 0.3500 time: 0.0377s\n",
            "Epoch: 0045 loss_train: 1.8222 acc_train: 0.2929 loss_val: 1.8074 acc_val: 0.3500 time: 0.0338s\n",
            "Epoch: 0046 loss_train: 1.8220 acc_train: 0.2929 loss_val: 1.8078 acc_val: 0.3500 time: 0.0406s\n",
            "Epoch: 0047 loss_train: 1.8313 acc_train: 0.2929 loss_val: 1.8081 acc_val: 0.3500 time: 0.0347s\n",
            "Epoch: 0048 loss_train: 1.8434 acc_train: 0.2929 loss_val: 1.8086 acc_val: 0.3500 time: 0.0413s\n",
            "Epoch: 0049 loss_train: 1.8222 acc_train: 0.2929 loss_val: 1.8089 acc_val: 0.3500 time: 0.0401s\n",
            "Epoch: 0050 loss_train: 1.8142 acc_train: 0.2929 loss_val: 1.8087 acc_val: 0.3500 time: 0.0361s\n",
            "Epoch: 0051 loss_train: 1.8223 acc_train: 0.2929 loss_val: 1.8084 acc_val: 0.3500 time: 0.0361s\n",
            "Epoch: 0052 loss_train: 1.8228 acc_train: 0.2929 loss_val: 1.8080 acc_val: 0.3500 time: 0.0362s\n",
            "Epoch: 0053 loss_train: 1.8133 acc_train: 0.2929 loss_val: 1.8074 acc_val: 0.3500 time: 0.0331s\n",
            "Epoch: 0054 loss_train: 1.8313 acc_train: 0.2929 loss_val: 1.8068 acc_val: 0.3500 time: 0.0342s\n",
            "Epoch: 0055 loss_train: 1.7987 acc_train: 0.2929 loss_val: 1.8061 acc_val: 0.3500 time: 0.0369s\n",
            "Epoch: 0056 loss_train: 1.8213 acc_train: 0.2929 loss_val: 1.8053 acc_val: 0.3500 time: 0.0365s\n",
            "Epoch: 0057 loss_train: 1.8107 acc_train: 0.2929 loss_val: 1.8046 acc_val: 0.3500 time: 0.0339s\n",
            "Epoch: 0058 loss_train: 1.8157 acc_train: 0.2929 loss_val: 1.8040 acc_val: 0.3500 time: 0.0364s\n",
            "Epoch: 0059 loss_train: 1.8040 acc_train: 0.2929 loss_val: 1.8035 acc_val: 0.3500 time: 0.0354s\n",
            "Epoch: 0060 loss_train: 1.8168 acc_train: 0.2929 loss_val: 1.8033 acc_val: 0.3500 time: 0.0424s\n",
            "Epoch: 0061 loss_train: 1.8237 acc_train: 0.2929 loss_val: 1.8032 acc_val: 0.3500 time: 0.0436s\n",
            "Epoch: 0062 loss_train: 1.8264 acc_train: 0.2929 loss_val: 1.8031 acc_val: 0.3500 time: 0.0362s\n",
            "Epoch: 0063 loss_train: 1.8026 acc_train: 0.2929 loss_val: 1.8030 acc_val: 0.3500 time: 0.0329s\n",
            "Epoch: 0064 loss_train: 1.8125 acc_train: 0.2929 loss_val: 1.8028 acc_val: 0.3500 time: 0.0376s\n",
            "Epoch: 0065 loss_train: 1.7946 acc_train: 0.2929 loss_val: 1.8025 acc_val: 0.3500 time: 0.0330s\n",
            "Epoch: 0066 loss_train: 1.8190 acc_train: 0.2929 loss_val: 1.8024 acc_val: 0.3500 time: 0.0340s\n",
            "Epoch: 0067 loss_train: 1.8091 acc_train: 0.2929 loss_val: 1.8024 acc_val: 0.3500 time: 0.0379s\n",
            "Epoch: 0068 loss_train: 1.8165 acc_train: 0.2929 loss_val: 1.8025 acc_val: 0.3500 time: 0.0351s\n",
            "Epoch: 0069 loss_train: 1.8275 acc_train: 0.2929 loss_val: 1.8027 acc_val: 0.3500 time: 0.0337s\n",
            "Epoch: 0070 loss_train: 1.8222 acc_train: 0.2929 loss_val: 1.8030 acc_val: 0.3500 time: 0.0345s\n",
            "Epoch: 0071 loss_train: 1.8282 acc_train: 0.2929 loss_val: 1.8033 acc_val: 0.3500 time: 0.0356s\n",
            "Epoch: 0072 loss_train: 1.8111 acc_train: 0.2929 loss_val: 1.8035 acc_val: 0.3500 time: 0.0372s\n",
            "Epoch: 0073 loss_train: 1.8211 acc_train: 0.2929 loss_val: 1.8038 acc_val: 0.3500 time: 0.0401s\n",
            "Epoch: 0074 loss_train: 1.8326 acc_train: 0.2929 loss_val: 1.8042 acc_val: 0.3500 time: 0.0376s\n",
            "Epoch: 0075 loss_train: 1.8313 acc_train: 0.2929 loss_val: 1.8046 acc_val: 0.3500 time: 0.0347s\n",
            "Epoch: 0076 loss_train: 1.8293 acc_train: 0.2929 loss_val: 1.8050 acc_val: 0.3500 time: 0.0438s\n",
            "Epoch: 0077 loss_train: 1.8045 acc_train: 0.2929 loss_val: 1.8050 acc_val: 0.3500 time: 0.0376s\n",
            "Epoch: 0078 loss_train: 1.8346 acc_train: 0.2929 loss_val: 1.8050 acc_val: 0.3500 time: 0.0359s\n",
            "Epoch: 0079 loss_train: 1.8255 acc_train: 0.2929 loss_val: 1.8048 acc_val: 0.3500 time: 0.0388s\n",
            "Epoch: 0080 loss_train: 1.8228 acc_train: 0.2929 loss_val: 1.8045 acc_val: 0.3500 time: 0.0347s\n",
            "Epoch: 0081 loss_train: 1.8162 acc_train: 0.2929 loss_val: 1.8042 acc_val: 0.3500 time: 0.0396s\n",
            "Epoch: 0082 loss_train: 1.8436 acc_train: 0.2929 loss_val: 1.8040 acc_val: 0.3500 time: 0.0346s\n",
            "Epoch: 0083 loss_train: 1.8175 acc_train: 0.2929 loss_val: 1.8038 acc_val: 0.3500 time: 0.0341s\n",
            "Epoch: 0084 loss_train: 1.8274 acc_train: 0.2929 loss_val: 1.8041 acc_val: 0.3500 time: 0.0397s\n",
            "Epoch: 0085 loss_train: 1.8185 acc_train: 0.2929 loss_val: 1.8045 acc_val: 0.3500 time: 0.0434s\n",
            "Epoch: 0086 loss_train: 1.8096 acc_train: 0.2929 loss_val: 1.8045 acc_val: 0.3500 time: 0.0405s\n",
            "Epoch: 0087 loss_train: 1.8201 acc_train: 0.2929 loss_val: 1.8045 acc_val: 0.3500 time: 0.0336s\n",
            "Epoch: 0088 loss_train: 1.8255 acc_train: 0.2929 loss_val: 1.8043 acc_val: 0.3500 time: 0.0399s\n",
            "Epoch: 0089 loss_train: 1.8357 acc_train: 0.2929 loss_val: 1.8042 acc_val: 0.3500 time: 0.0345s\n",
            "Epoch: 0090 loss_train: 1.8101 acc_train: 0.2929 loss_val: 1.8041 acc_val: 0.3500 time: 0.0346s\n",
            "Epoch: 0091 loss_train: 1.8121 acc_train: 0.2929 loss_val: 1.8036 acc_val: 0.3500 time: 0.0397s\n",
            "Epoch: 0092 loss_train: 1.8133 acc_train: 0.2929 loss_val: 1.8029 acc_val: 0.3500 time: 0.0369s\n",
            "Epoch: 0093 loss_train: 1.8161 acc_train: 0.2929 loss_val: 1.8022 acc_val: 0.3500 time: 0.0531s\n",
            "Epoch: 0094 loss_train: 1.8177 acc_train: 0.2929 loss_val: 1.8017 acc_val: 0.3500 time: 0.0328s\n",
            "Epoch: 0095 loss_train: 1.8326 acc_train: 0.2929 loss_val: 1.8014 acc_val: 0.3500 time: 0.0351s\n",
            "Epoch: 0096 loss_train: 1.8268 acc_train: 0.2929 loss_val: 1.8013 acc_val: 0.3500 time: 0.0377s\n",
            "Epoch: 0097 loss_train: 1.8191 acc_train: 0.2929 loss_val: 1.8014 acc_val: 0.3500 time: 0.0383s\n",
            "Epoch: 0098 loss_train: 1.8269 acc_train: 0.2929 loss_val: 1.8015 acc_val: 0.3500 time: 0.0369s\n",
            "Epoch: 0099 loss_train: 1.8088 acc_train: 0.2929 loss_val: 1.8015 acc_val: 0.3500 time: 0.0405s\n",
            "Epoch: 0100 loss_train: 1.8227 acc_train: 0.2929 loss_val: 1.8014 acc_val: 0.3500 time: 0.0340s\n",
            "Epoch: 0101 loss_train: 1.8108 acc_train: 0.2929 loss_val: 1.8016 acc_val: 0.3500 time: 0.0339s\n",
            "Epoch: 0102 loss_train: 1.8184 acc_train: 0.2929 loss_val: 1.8017 acc_val: 0.3500 time: 0.0386s\n",
            "Epoch: 0103 loss_train: 1.8160 acc_train: 0.2929 loss_val: 1.8017 acc_val: 0.3500 time: 0.0342s\n",
            "Epoch: 0104 loss_train: 1.8256 acc_train: 0.2929 loss_val: 1.8018 acc_val: 0.3500 time: 0.0344s\n",
            "Epoch: 0105 loss_train: 1.8228 acc_train: 0.2929 loss_val: 1.8020 acc_val: 0.3500 time: 0.0386s\n",
            "Epoch: 0106 loss_train: 1.8067 acc_train: 0.2929 loss_val: 1.8022 acc_val: 0.3500 time: 0.0355s\n",
            "Epoch: 0107 loss_train: 1.8211 acc_train: 0.2929 loss_val: 1.8024 acc_val: 0.3500 time: 0.0400s\n",
            "Epoch: 0108 loss_train: 1.8182 acc_train: 0.2929 loss_val: 1.8025 acc_val: 0.3500 time: 0.0378s\n",
            "Epoch: 0109 loss_train: 1.8259 acc_train: 0.2929 loss_val: 1.8026 acc_val: 0.3500 time: 0.0327s\n",
            "Epoch: 0110 loss_train: 1.8309 acc_train: 0.2929 loss_val: 1.8026 acc_val: 0.3500 time: 0.0346s\n",
            "Epoch: 0111 loss_train: 1.8152 acc_train: 0.2929 loss_val: 1.8026 acc_val: 0.3500 time: 0.0369s\n",
            "Epoch: 0112 loss_train: 1.8275 acc_train: 0.2929 loss_val: 1.8027 acc_val: 0.3500 time: 0.0307s\n",
            "Epoch: 0113 loss_train: 1.8287 acc_train: 0.2929 loss_val: 1.8026 acc_val: 0.3500 time: 0.0373s\n",
            "Epoch: 0114 loss_train: 1.8112 acc_train: 0.2929 loss_val: 1.8025 acc_val: 0.3500 time: 0.0358s\n",
            "Epoch: 0115 loss_train: 1.8306 acc_train: 0.2929 loss_val: 1.8025 acc_val: 0.3500 time: 0.0335s\n",
            "Epoch: 0116 loss_train: 1.8094 acc_train: 0.2929 loss_val: 1.8024 acc_val: 0.3500 time: 0.0335s\n",
            "Epoch: 0117 loss_train: 1.8290 acc_train: 0.2929 loss_val: 1.8026 acc_val: 0.3500 time: 0.0347s\n",
            "Epoch: 0118 loss_train: 1.8177 acc_train: 0.2929 loss_val: 1.8025 acc_val: 0.3500 time: 0.0344s\n",
            "Epoch: 0119 loss_train: 1.8317 acc_train: 0.2929 loss_val: 1.8023 acc_val: 0.3500 time: 0.0332s\n",
            "Epoch: 0120 loss_train: 1.8170 acc_train: 0.2929 loss_val: 1.8022 acc_val: 0.3500 time: 0.0384s\n",
            "Epoch: 0121 loss_train: 1.8184 acc_train: 0.2929 loss_val: 1.8021 acc_val: 0.3500 time: 0.0342s\n",
            "Epoch: 0122 loss_train: 1.8199 acc_train: 0.2929 loss_val: 1.8020 acc_val: 0.3500 time: 0.0343s\n",
            "Epoch: 0123 loss_train: 1.8158 acc_train: 0.2929 loss_val: 1.8019 acc_val: 0.3500 time: 0.0423s\n",
            "Epoch: 0124 loss_train: 1.8288 acc_train: 0.2929 loss_val: 1.8019 acc_val: 0.3500 time: 0.0339s\n",
            "Epoch: 0125 loss_train: 1.8300 acc_train: 0.2929 loss_val: 1.8020 acc_val: 0.3500 time: 0.0366s\n",
            "Epoch: 0126 loss_train: 1.8236 acc_train: 0.2929 loss_val: 1.8020 acc_val: 0.3500 time: 0.0397s\n",
            "Epoch: 0127 loss_train: 1.8256 acc_train: 0.2929 loss_val: 1.8020 acc_val: 0.3500 time: 0.0330s\n",
            "Epoch: 0128 loss_train: 1.8181 acc_train: 0.2929 loss_val: 1.8019 acc_val: 0.3500 time: 0.0357s\n",
            "Epoch: 0129 loss_train: 1.8075 acc_train: 0.2929 loss_val: 1.8018 acc_val: 0.3500 time: 0.0381s\n",
            "Epoch: 0130 loss_train: 1.8295 acc_train: 0.2929 loss_val: 1.8017 acc_val: 0.3500 time: 0.0344s\n",
            "Epoch: 0131 loss_train: 1.8252 acc_train: 0.2929 loss_val: 1.8016 acc_val: 0.3500 time: 0.0348s\n",
            "Epoch: 0132 loss_train: 1.8109 acc_train: 0.2929 loss_val: 1.8015 acc_val: 0.3500 time: 0.0386s\n",
            "Epoch: 0133 loss_train: 1.8056 acc_train: 0.2929 loss_val: 1.8013 acc_val: 0.3500 time: 0.0356s\n",
            "Epoch: 0134 loss_train: 1.8213 acc_train: 0.2929 loss_val: 1.8012 acc_val: 0.3500 time: 0.0355s\n",
            "Epoch: 0135 loss_train: 1.8207 acc_train: 0.2929 loss_val: 1.8011 acc_val: 0.3500 time: 0.0349s\n",
            "Epoch: 0136 loss_train: 1.8067 acc_train: 0.2929 loss_val: 1.8010 acc_val: 0.3500 time: 0.0364s\n",
            "Epoch: 0137 loss_train: 1.8182 acc_train: 0.2929 loss_val: 1.8011 acc_val: 0.3500 time: 0.0345s\n",
            "Epoch: 0138 loss_train: 1.8287 acc_train: 0.2929 loss_val: 1.8013 acc_val: 0.3500 time: 0.0392s\n",
            "Epoch: 0139 loss_train: 1.8123 acc_train: 0.2929 loss_val: 1.8014 acc_val: 0.3500 time: 0.0355s\n",
            "Epoch: 0140 loss_train: 1.8112 acc_train: 0.2929 loss_val: 1.8015 acc_val: 0.3500 time: 0.0469s\n",
            "Epoch: 0141 loss_train: 1.8174 acc_train: 0.2929 loss_val: 1.8016 acc_val: 0.3500 time: 0.0372s\n",
            "Epoch: 0142 loss_train: 1.8111 acc_train: 0.2929 loss_val: 1.8017 acc_val: 0.3500 time: 0.0391s\n",
            "Epoch: 0143 loss_train: 1.8227 acc_train: 0.2929 loss_val: 1.8019 acc_val: 0.3500 time: 0.0333s\n",
            "Epoch: 0144 loss_train: 1.8263 acc_train: 0.2929 loss_val: 1.8020 acc_val: 0.3500 time: 0.0375s\n",
            "Epoch: 0145 loss_train: 1.8215 acc_train: 0.2929 loss_val: 1.8023 acc_val: 0.3500 time: 0.0336s\n",
            "Epoch: 0146 loss_train: 1.8146 acc_train: 0.2929 loss_val: 1.8027 acc_val: 0.3500 time: 0.0350s\n",
            "Epoch: 0147 loss_train: 1.8223 acc_train: 0.2929 loss_val: 1.8031 acc_val: 0.3500 time: 0.0393s\n",
            "Epoch: 0148 loss_train: 1.8196 acc_train: 0.2929 loss_val: 1.8034 acc_val: 0.3500 time: 0.0342s\n",
            "Epoch: 0149 loss_train: 1.8205 acc_train: 0.2929 loss_val: 1.8037 acc_val: 0.3500 time: 0.0347s\n",
            "Epoch: 0150 loss_train: 1.8176 acc_train: 0.2929 loss_val: 1.8040 acc_val: 0.3500 time: 0.0395s\n",
            "Epoch: 0151 loss_train: 1.8183 acc_train: 0.2929 loss_val: 1.8041 acc_val: 0.3500 time: 0.0368s\n",
            "Epoch: 0152 loss_train: 1.8237 acc_train: 0.2929 loss_val: 1.8041 acc_val: 0.3500 time: 0.0364s\n",
            "Epoch: 0153 loss_train: 1.8117 acc_train: 0.2929 loss_val: 1.8039 acc_val: 0.3500 time: 0.0376s\n",
            "Epoch: 0154 loss_train: 1.8078 acc_train: 0.2929 loss_val: 1.8036 acc_val: 0.3500 time: 0.0333s\n",
            "Epoch: 0155 loss_train: 1.8208 acc_train: 0.2929 loss_val: 1.8032 acc_val: 0.3500 time: 0.0350s\n",
            "Epoch: 0156 loss_train: 1.8192 acc_train: 0.2929 loss_val: 1.8028 acc_val: 0.3500 time: 0.0384s\n",
            "Epoch: 0157 loss_train: 1.8220 acc_train: 0.2929 loss_val: 1.8025 acc_val: 0.3500 time: 0.0329s\n",
            "Epoch: 0158 loss_train: 1.8272 acc_train: 0.2929 loss_val: 1.8024 acc_val: 0.3500 time: 0.0327s\n",
            "Epoch: 0159 loss_train: 1.8015 acc_train: 0.2929 loss_val: 1.8023 acc_val: 0.3500 time: 0.0396s\n",
            "Epoch: 0160 loss_train: 1.8119 acc_train: 0.2929 loss_val: 1.8023 acc_val: 0.3500 time: 0.0360s\n",
            "Epoch: 0161 loss_train: 1.8242 acc_train: 0.2929 loss_val: 1.8022 acc_val: 0.3500 time: 0.0343s\n",
            "Epoch: 0162 loss_train: 1.8208 acc_train: 0.2929 loss_val: 1.8022 acc_val: 0.3500 time: 0.0383s\n",
            "Epoch: 0163 loss_train: 1.8212 acc_train: 0.2929 loss_val: 1.8022 acc_val: 0.3500 time: 0.0356s\n",
            "Epoch: 0164 loss_train: 1.8160 acc_train: 0.2929 loss_val: 1.8022 acc_val: 0.3500 time: 0.0402s\n",
            "Epoch: 0165 loss_train: 1.8091 acc_train: 0.2929 loss_val: 1.8020 acc_val: 0.3500 time: 0.0361s\n",
            "Epoch: 0166 loss_train: 1.8211 acc_train: 0.2929 loss_val: 1.8019 acc_val: 0.3500 time: 0.0343s\n",
            "Epoch: 0167 loss_train: 1.8245 acc_train: 0.2929 loss_val: 1.8017 acc_val: 0.3500 time: 0.0442s\n",
            "Epoch: 0168 loss_train: 1.8209 acc_train: 0.2929 loss_val: 1.8015 acc_val: 0.3500 time: 0.0380s\n",
            "Epoch: 0169 loss_train: 1.8115 acc_train: 0.2929 loss_val: 1.8014 acc_val: 0.3500 time: 0.0313s\n",
            "Epoch: 0170 loss_train: 1.8099 acc_train: 0.2929 loss_val: 1.8014 acc_val: 0.3500 time: 0.0303s\n",
            "Epoch: 0171 loss_train: 1.8140 acc_train: 0.2929 loss_val: 1.8013 acc_val: 0.3500 time: 0.0313s\n",
            "Epoch: 0172 loss_train: 1.8220 acc_train: 0.2929 loss_val: 1.8014 acc_val: 0.3500 time: 0.0366s\n",
            "Epoch: 0173 loss_train: 1.8129 acc_train: 0.2929 loss_val: 1.8016 acc_val: 0.3500 time: 0.0380s\n",
            "Epoch: 0174 loss_train: 1.8254 acc_train: 0.2929 loss_val: 1.8018 acc_val: 0.3500 time: 0.0378s\n",
            "Epoch: 0175 loss_train: 1.8266 acc_train: 0.2929 loss_val: 1.8019 acc_val: 0.3500 time: 0.0323s\n",
            "Epoch: 0176 loss_train: 1.8228 acc_train: 0.2929 loss_val: 1.8022 acc_val: 0.3500 time: 0.0351s\n",
            "Epoch: 0177 loss_train: 1.8125 acc_train: 0.2929 loss_val: 1.8024 acc_val: 0.3500 time: 0.0426s\n",
            "Epoch: 0178 loss_train: 1.8163 acc_train: 0.2929 loss_val: 1.8027 acc_val: 0.3500 time: 0.0338s\n",
            "Epoch: 0179 loss_train: 1.8259 acc_train: 0.2929 loss_val: 1.8029 acc_val: 0.3500 time: 0.0351s\n",
            "Epoch: 0180 loss_train: 1.8246 acc_train: 0.2929 loss_val: 1.8031 acc_val: 0.3500 time: 0.0391s\n",
            "Epoch: 0181 loss_train: 1.8304 acc_train: 0.2929 loss_val: 1.8034 acc_val: 0.3500 time: 0.0366s\n",
            "Epoch: 0182 loss_train: 1.8194 acc_train: 0.2929 loss_val: 1.8035 acc_val: 0.3500 time: 0.0332s\n",
            "Epoch: 0183 loss_train: 1.8143 acc_train: 0.2929 loss_val: 1.8035 acc_val: 0.3500 time: 0.0380s\n",
            "Epoch: 0184 loss_train: 1.8142 acc_train: 0.2929 loss_val: 1.8034 acc_val: 0.3500 time: 0.0330s\n",
            "Epoch: 0185 loss_train: 1.8323 acc_train: 0.2929 loss_val: 1.8033 acc_val: 0.3500 time: 0.0391s\n",
            "Epoch: 0186 loss_train: 1.8136 acc_train: 0.2929 loss_val: 1.8030 acc_val: 0.3500 time: 0.0398s\n",
            "Epoch: 0187 loss_train: 1.8155 acc_train: 0.2929 loss_val: 1.8029 acc_val: 0.3500 time: 0.0378s\n",
            "Epoch: 0188 loss_train: 1.8247 acc_train: 0.2929 loss_val: 1.8027 acc_val: 0.3500 time: 0.0354s\n",
            "Epoch: 0189 loss_train: 1.8230 acc_train: 0.2929 loss_val: 1.8027 acc_val: 0.3500 time: 0.0400s\n",
            "Epoch: 0190 loss_train: 1.8190 acc_train: 0.2929 loss_val: 1.8025 acc_val: 0.3500 time: 0.0410s\n",
            "Epoch: 0191 loss_train: 1.8206 acc_train: 0.2929 loss_val: 1.8021 acc_val: 0.3500 time: 0.0355s\n",
            "Epoch: 0192 loss_train: 1.8146 acc_train: 0.2929 loss_val: 1.8019 acc_val: 0.3500 time: 0.0412s\n",
            "Epoch: 0193 loss_train: 1.8186 acc_train: 0.2929 loss_val: 1.8017 acc_val: 0.3500 time: 0.0391s\n",
            "Epoch: 0194 loss_train: 1.8201 acc_train: 0.2929 loss_val: 1.8014 acc_val: 0.3500 time: 0.0381s\n",
            "Epoch: 0195 loss_train: 1.8219 acc_train: 0.2929 loss_val: 1.8012 acc_val: 0.3500 time: 0.0321s\n",
            "Epoch: 0196 loss_train: 1.8057 acc_train: 0.2929 loss_val: 1.8008 acc_val: 0.3500 time: 0.0352s\n",
            "Epoch: 0197 loss_train: 1.8168 acc_train: 0.2929 loss_val: 1.8007 acc_val: 0.3500 time: 0.0318s\n",
            "Epoch: 0198 loss_train: 1.8134 acc_train: 0.2929 loss_val: 1.8008 acc_val: 0.3500 time: 0.0357s\n",
            "Epoch: 0199 loss_train: 1.8139 acc_train: 0.2929 loss_val: 1.8009 acc_val: 0.3500 time: 0.0375s\n",
            "Epoch: 0200 loss_train: 1.8251 acc_train: 0.2929 loss_val: 1.8011 acc_val: 0.3500 time: 0.0313s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 7.5720s\n",
            "Test set results: loss= 1.8898 accuracy= 0.3090\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zi-io65aFC4z",
        "outputId": "1d73231a-eb45-4302-c28b-06ffa267afb5"
      },
      "source": [
        "#15-layers\n",
        "%run pygcn/train.py\n",
        "#        self.gc14 = GraphConvolution(nhid, nhid) #Middle Layer\n",
        "#       x = F.relu(self.gc5(x, adj)) #???\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cora dataset...\n",
            "Epoch: 0001 loss_train: 1.9017 acc_train: 0.2000 loss_val: 1.8843 acc_val: 0.1567 time: 0.0446s\n",
            "Epoch: 0002 loss_train: 1.8889 acc_train: 0.2143 loss_val: 1.8708 acc_val: 0.1567 time: 0.0428s\n",
            "Epoch: 0003 loss_train: 1.8700 acc_train: 0.3071 loss_val: 1.8585 acc_val: 0.3500 time: 0.0429s\n",
            "Epoch: 0004 loss_train: 1.8603 acc_train: 0.2714 loss_val: 1.8476 acc_val: 0.3500 time: 0.0400s\n",
            "Epoch: 0005 loss_train: 1.8558 acc_train: 0.2429 loss_val: 1.8379 acc_val: 0.3500 time: 0.0435s\n",
            "Epoch: 0006 loss_train: 1.8547 acc_train: 0.2786 loss_val: 1.8295 acc_val: 0.3500 time: 0.0465s\n",
            "Epoch: 0007 loss_train: 1.8438 acc_train: 0.2786 loss_val: 1.8224 acc_val: 0.3500 time: 0.0422s\n",
            "Epoch: 0008 loss_train: 1.8238 acc_train: 0.3000 loss_val: 1.8164 acc_val: 0.3500 time: 0.0415s\n",
            "Epoch: 0009 loss_train: 1.8349 acc_train: 0.3000 loss_val: 1.8115 acc_val: 0.3500 time: 0.0455s\n",
            "Epoch: 0010 loss_train: 1.8354 acc_train: 0.2929 loss_val: 1.8077 acc_val: 0.3500 time: 0.0464s\n",
            "Epoch: 0011 loss_train: 1.8209 acc_train: 0.2929 loss_val: 1.8048 acc_val: 0.3500 time: 0.0652s\n",
            "Epoch: 0012 loss_train: 1.8229 acc_train: 0.3000 loss_val: 1.8025 acc_val: 0.3500 time: 0.0465s\n",
            "Epoch: 0013 loss_train: 1.8284 acc_train: 0.2857 loss_val: 1.8007 acc_val: 0.3500 time: 0.0520s\n",
            "Epoch: 0014 loss_train: 1.8494 acc_train: 0.2929 loss_val: 1.7995 acc_val: 0.3500 time: 0.0448s\n",
            "Epoch: 0015 loss_train: 1.8269 acc_train: 0.2929 loss_val: 1.7989 acc_val: 0.3500 time: 0.0456s\n",
            "Epoch: 0016 loss_train: 1.8159 acc_train: 0.2929 loss_val: 1.7989 acc_val: 0.3500 time: 0.0442s\n",
            "Epoch: 0017 loss_train: 1.8196 acc_train: 0.3000 loss_val: 1.7992 acc_val: 0.3500 time: 0.0501s\n",
            "Epoch: 0018 loss_train: 1.8297 acc_train: 0.2857 loss_val: 1.8001 acc_val: 0.3500 time: 0.0476s\n",
            "Epoch: 0019 loss_train: 1.8263 acc_train: 0.2929 loss_val: 1.8012 acc_val: 0.3500 time: 0.0406s\n",
            "Epoch: 0020 loss_train: 1.8113 acc_train: 0.2857 loss_val: 1.8026 acc_val: 0.3500 time: 0.0396s\n",
            "Epoch: 0021 loss_train: 1.8089 acc_train: 0.2857 loss_val: 1.8037 acc_val: 0.3500 time: 0.0448s\n",
            "Epoch: 0022 loss_train: 1.8215 acc_train: 0.2929 loss_val: 1.8046 acc_val: 0.3500 time: 0.0488s\n",
            "Epoch: 0023 loss_train: 1.8118 acc_train: 0.3071 loss_val: 1.8052 acc_val: 0.3500 time: 0.0451s\n",
            "Epoch: 0024 loss_train: 1.8214 acc_train: 0.2929 loss_val: 1.8058 acc_val: 0.3500 time: 0.0455s\n",
            "Epoch: 0025 loss_train: 1.8128 acc_train: 0.3143 loss_val: 1.8061 acc_val: 0.3500 time: 0.0397s\n",
            "Epoch: 0026 loss_train: 1.8352 acc_train: 0.2929 loss_val: 1.8062 acc_val: 0.3500 time: 0.0466s\n",
            "Epoch: 0027 loss_train: 1.8281 acc_train: 0.2929 loss_val: 1.8063 acc_val: 0.3500 time: 0.0431s\n",
            "Epoch: 0028 loss_train: 1.8126 acc_train: 0.2929 loss_val: 1.8065 acc_val: 0.3500 time: 0.0462s\n",
            "Epoch: 0029 loss_train: 1.8182 acc_train: 0.2857 loss_val: 1.8067 acc_val: 0.3500 time: 0.0408s\n",
            "Epoch: 0030 loss_train: 1.8405 acc_train: 0.2929 loss_val: 1.8071 acc_val: 0.3500 time: 0.0429s\n",
            "Epoch: 0031 loss_train: 1.8330 acc_train: 0.2929 loss_val: 1.8076 acc_val: 0.3500 time: 0.0453s\n",
            "Epoch: 0032 loss_train: 1.8079 acc_train: 0.2857 loss_val: 1.8080 acc_val: 0.3500 time: 0.0456s\n",
            "Epoch: 0033 loss_train: 1.8224 acc_train: 0.2929 loss_val: 1.8081 acc_val: 0.3500 time: 0.0437s\n",
            "Epoch: 0034 loss_train: 1.8098 acc_train: 0.2857 loss_val: 1.8078 acc_val: 0.3500 time: 0.0405s\n",
            "Epoch: 0035 loss_train: 1.8267 acc_train: 0.3000 loss_val: 1.8073 acc_val: 0.3500 time: 0.0416s\n",
            "Epoch: 0036 loss_train: 1.8137 acc_train: 0.3000 loss_val: 1.8064 acc_val: 0.3500 time: 0.0449s\n",
            "Epoch: 0037 loss_train: 1.8255 acc_train: 0.2929 loss_val: 1.8055 acc_val: 0.3500 time: 0.0435s\n",
            "Epoch: 0038 loss_train: 1.8098 acc_train: 0.2786 loss_val: 1.8045 acc_val: 0.3500 time: 0.0396s\n",
            "Epoch: 0039 loss_train: 1.8171 acc_train: 0.2929 loss_val: 1.8034 acc_val: 0.3500 time: 0.0550s\n",
            "Epoch: 0040 loss_train: 1.8438 acc_train: 0.2857 loss_val: 1.8028 acc_val: 0.3500 time: 0.0473s\n",
            "Epoch: 0041 loss_train: 1.8090 acc_train: 0.2857 loss_val: 1.8022 acc_val: 0.3500 time: 0.0469s\n",
            "Epoch: 0042 loss_train: 1.8293 acc_train: 0.2929 loss_val: 1.8019 acc_val: 0.3500 time: 0.0418s\n",
            "Epoch: 0043 loss_train: 1.8098 acc_train: 0.2929 loss_val: 1.8015 acc_val: 0.3500 time: 0.0401s\n",
            "Epoch: 0044 loss_train: 1.8111 acc_train: 0.2857 loss_val: 1.8010 acc_val: 0.3500 time: 0.0450s\n",
            "Epoch: 0045 loss_train: 1.8171 acc_train: 0.2857 loss_val: 1.8007 acc_val: 0.3500 time: 0.0413s\n",
            "Epoch: 0046 loss_train: 1.8194 acc_train: 0.2929 loss_val: 1.8005 acc_val: 0.3500 time: 0.0455s\n",
            "Epoch: 0047 loss_train: 1.8198 acc_train: 0.2929 loss_val: 1.8003 acc_val: 0.3500 time: 0.0421s\n",
            "Epoch: 0048 loss_train: 1.8217 acc_train: 0.2929 loss_val: 1.8002 acc_val: 0.3500 time: 0.0422s\n",
            "Epoch: 0049 loss_train: 1.8336 acc_train: 0.2929 loss_val: 1.8002 acc_val: 0.3500 time: 0.0469s\n",
            "Epoch: 0050 loss_train: 1.8154 acc_train: 0.2929 loss_val: 1.8004 acc_val: 0.3500 time: 0.0441s\n",
            "Epoch: 0051 loss_train: 1.8383 acc_train: 0.2929 loss_val: 1.8009 acc_val: 0.3500 time: 0.0456s\n",
            "Epoch: 0052 loss_train: 1.8217 acc_train: 0.2929 loss_val: 1.8014 acc_val: 0.3500 time: 0.0431s\n",
            "Epoch: 0053 loss_train: 1.8221 acc_train: 0.2929 loss_val: 1.8020 acc_val: 0.3500 time: 0.0464s\n",
            "Epoch: 0054 loss_train: 1.8252 acc_train: 0.2929 loss_val: 1.8027 acc_val: 0.3500 time: 0.0451s\n",
            "Epoch: 0055 loss_train: 1.8306 acc_train: 0.2929 loss_val: 1.8033 acc_val: 0.3500 time: 0.0419s\n",
            "Epoch: 0056 loss_train: 1.8086 acc_train: 0.2929 loss_val: 1.8037 acc_val: 0.3500 time: 0.0452s\n",
            "Epoch: 0057 loss_train: 1.8130 acc_train: 0.2929 loss_val: 1.8037 acc_val: 0.3500 time: 0.0428s\n",
            "Epoch: 0058 loss_train: 1.8234 acc_train: 0.2786 loss_val: 1.8038 acc_val: 0.3500 time: 0.0466s\n",
            "Epoch: 0059 loss_train: 1.8253 acc_train: 0.2929 loss_val: 1.8039 acc_val: 0.3500 time: 0.0422s\n",
            "Epoch: 0060 loss_train: 1.8294 acc_train: 0.2929 loss_val: 1.8039 acc_val: 0.3500 time: 0.0403s\n",
            "Epoch: 0061 loss_train: 1.8294 acc_train: 0.2929 loss_val: 1.8040 acc_val: 0.3500 time: 0.0498s\n",
            "Epoch: 0062 loss_train: 1.8260 acc_train: 0.3000 loss_val: 1.8042 acc_val: 0.3500 time: 0.0495s\n",
            "Epoch: 0063 loss_train: 1.8182 acc_train: 0.2929 loss_val: 1.8041 acc_val: 0.3500 time: 0.0458s\n",
            "Epoch: 0064 loss_train: 1.8230 acc_train: 0.2929 loss_val: 1.8038 acc_val: 0.3500 time: 0.0422s\n",
            "Epoch: 0065 loss_train: 1.8206 acc_train: 0.2929 loss_val: 1.8034 acc_val: 0.3500 time: 0.0407s\n",
            "Epoch: 0066 loss_train: 1.8101 acc_train: 0.2929 loss_val: 1.8028 acc_val: 0.3500 time: 0.0431s\n",
            "Epoch: 0067 loss_train: 1.8134 acc_train: 0.2929 loss_val: 1.8022 acc_val: 0.3500 time: 0.0407s\n",
            "Epoch: 0068 loss_train: 1.8124 acc_train: 0.2929 loss_val: 1.8017 acc_val: 0.3500 time: 0.0424s\n",
            "Epoch: 0069 loss_train: 1.8267 acc_train: 0.2929 loss_val: 1.8012 acc_val: 0.3500 time: 0.0461s\n",
            "Epoch: 0070 loss_train: 1.8079 acc_train: 0.2929 loss_val: 1.8008 acc_val: 0.3500 time: 0.0392s\n",
            "Epoch: 0071 loss_train: 1.8095 acc_train: 0.2929 loss_val: 1.8005 acc_val: 0.3500 time: 0.0494s\n",
            "Epoch: 0072 loss_train: 1.8200 acc_train: 0.2929 loss_val: 1.8002 acc_val: 0.3500 time: 0.0417s\n",
            "Epoch: 0073 loss_train: 1.8152 acc_train: 0.2929 loss_val: 1.8001 acc_val: 0.3500 time: 0.0415s\n",
            "Epoch: 0074 loss_train: 1.8256 acc_train: 0.2929 loss_val: 1.8002 acc_val: 0.3500 time: 0.0464s\n",
            "Epoch: 0075 loss_train: 1.8118 acc_train: 0.2929 loss_val: 1.8003 acc_val: 0.3500 time: 0.0409s\n",
            "Epoch: 0076 loss_train: 1.8293 acc_train: 0.2929 loss_val: 1.8006 acc_val: 0.3500 time: 0.0479s\n",
            "Epoch: 0077 loss_train: 1.8291 acc_train: 0.2929 loss_val: 1.8010 acc_val: 0.3500 time: 0.0411s\n",
            "Epoch: 0078 loss_train: 1.8117 acc_train: 0.2929 loss_val: 1.8013 acc_val: 0.3500 time: 0.0432s\n",
            "Epoch: 0079 loss_train: 1.8153 acc_train: 0.2929 loss_val: 1.8015 acc_val: 0.3500 time: 0.0433s\n",
            "Epoch: 0080 loss_train: 1.8329 acc_train: 0.2929 loss_val: 1.8019 acc_val: 0.3500 time: 0.0433s\n",
            "Epoch: 0081 loss_train: 1.8254 acc_train: 0.2929 loss_val: 1.8022 acc_val: 0.3500 time: 0.0461s\n",
            "Epoch: 0082 loss_train: 1.8113 acc_train: 0.2929 loss_val: 1.8024 acc_val: 0.3500 time: 0.0442s\n",
            "Epoch: 0083 loss_train: 1.8183 acc_train: 0.2929 loss_val: 1.8026 acc_val: 0.3500 time: 0.0487s\n",
            "Epoch: 0084 loss_train: 1.8167 acc_train: 0.2929 loss_val: 1.8028 acc_val: 0.3500 time: 0.0551s\n",
            "Epoch: 0085 loss_train: 1.8061 acc_train: 0.2929 loss_val: 1.8027 acc_val: 0.3500 time: 0.0456s\n",
            "Epoch: 0086 loss_train: 1.8206 acc_train: 0.2929 loss_val: 1.8027 acc_val: 0.3500 time: 0.0464s\n",
            "Epoch: 0087 loss_train: 1.8218 acc_train: 0.2929 loss_val: 1.8028 acc_val: 0.3500 time: 0.0455s\n",
            "Epoch: 0088 loss_train: 1.8097 acc_train: 0.2929 loss_val: 1.8026 acc_val: 0.3500 time: 0.0496s\n",
            "Epoch: 0089 loss_train: 1.8080 acc_train: 0.2929 loss_val: 1.8025 acc_val: 0.3500 time: 0.0393s\n",
            "Epoch: 0090 loss_train: 1.8201 acc_train: 0.2929 loss_val: 1.8025 acc_val: 0.3500 time: 0.0416s\n",
            "Epoch: 0091 loss_train: 1.8266 acc_train: 0.2929 loss_val: 1.8025 acc_val: 0.3500 time: 0.0504s\n",
            "Epoch: 0092 loss_train: 1.8009 acc_train: 0.2929 loss_val: 1.8025 acc_val: 0.3500 time: 0.0411s\n",
            "Epoch: 0093 loss_train: 1.8147 acc_train: 0.2929 loss_val: 1.8024 acc_val: 0.3500 time: 0.0422s\n",
            "Epoch: 0094 loss_train: 1.8127 acc_train: 0.2929 loss_val: 1.8022 acc_val: 0.3500 time: 0.0412s\n",
            "Epoch: 0095 loss_train: 1.8232 acc_train: 0.2929 loss_val: 1.8022 acc_val: 0.3500 time: 0.0405s\n",
            "Epoch: 0096 loss_train: 1.8119 acc_train: 0.2929 loss_val: 1.8021 acc_val: 0.3500 time: 0.0450s\n",
            "Epoch: 0097 loss_train: 1.8193 acc_train: 0.2929 loss_val: 1.8020 acc_val: 0.3500 time: 0.0423s\n",
            "Epoch: 0098 loss_train: 1.8401 acc_train: 0.2929 loss_val: 1.8020 acc_val: 0.3500 time: 0.0512s\n",
            "Epoch: 0099 loss_train: 1.8117 acc_train: 0.2929 loss_val: 1.8020 acc_val: 0.3500 time: 0.0423s\n",
            "Epoch: 0100 loss_train: 1.8195 acc_train: 0.2929 loss_val: 1.8020 acc_val: 0.3500 time: 0.0454s\n",
            "Epoch: 0101 loss_train: 1.8265 acc_train: 0.2929 loss_val: 1.8020 acc_val: 0.3500 time: 0.0496s\n",
            "Epoch: 0102 loss_train: 1.8147 acc_train: 0.2929 loss_val: 1.8019 acc_val: 0.3500 time: 0.0419s\n",
            "Epoch: 0103 loss_train: 1.8117 acc_train: 0.2929 loss_val: 1.8018 acc_val: 0.3500 time: 0.0474s\n",
            "Epoch: 0104 loss_train: 1.8154 acc_train: 0.2929 loss_val: 1.8016 acc_val: 0.3500 time: 0.0423s\n",
            "Epoch: 0105 loss_train: 1.8089 acc_train: 0.2929 loss_val: 1.8014 acc_val: 0.3500 time: 0.0454s\n",
            "Epoch: 0106 loss_train: 1.8109 acc_train: 0.2929 loss_val: 1.8011 acc_val: 0.3500 time: 0.0559s\n",
            "Epoch: 0107 loss_train: 1.8189 acc_train: 0.2929 loss_val: 1.8009 acc_val: 0.3500 time: 0.0469s\n",
            "Epoch: 0108 loss_train: 1.8163 acc_train: 0.2929 loss_val: 1.8007 acc_val: 0.3500 time: 0.0482s\n",
            "Epoch: 0109 loss_train: 1.8090 acc_train: 0.2929 loss_val: 1.8005 acc_val: 0.3500 time: 0.0471s\n",
            "Epoch: 0110 loss_train: 1.8270 acc_train: 0.2929 loss_val: 1.8006 acc_val: 0.3500 time: 0.0495s\n",
            "Epoch: 0111 loss_train: 1.8059 acc_train: 0.2929 loss_val: 1.8006 acc_val: 0.3500 time: 0.0560s\n",
            "Epoch: 0112 loss_train: 1.8304 acc_train: 0.2929 loss_val: 1.8009 acc_val: 0.3500 time: 0.0570s\n",
            "Epoch: 0113 loss_train: 1.8328 acc_train: 0.2929 loss_val: 1.8013 acc_val: 0.3500 time: 0.0533s\n",
            "Epoch: 0114 loss_train: 1.8085 acc_train: 0.2929 loss_val: 1.8018 acc_val: 0.3500 time: 0.0582s\n",
            "Epoch: 0115 loss_train: 1.8227 acc_train: 0.2929 loss_val: 1.8022 acc_val: 0.3500 time: 0.0582s\n",
            "Epoch: 0116 loss_train: 1.8178 acc_train: 0.2929 loss_val: 1.8026 acc_val: 0.3500 time: 0.0561s\n",
            "Epoch: 0117 loss_train: 1.8188 acc_train: 0.2929 loss_val: 1.8030 acc_val: 0.3500 time: 0.0622s\n",
            "Epoch: 0118 loss_train: 1.8173 acc_train: 0.2929 loss_val: 1.8034 acc_val: 0.3500 time: 0.0589s\n",
            "Epoch: 0119 loss_train: 1.8047 acc_train: 0.2929 loss_val: 1.8035 acc_val: 0.3500 time: 0.0656s\n",
            "Epoch: 0120 loss_train: 1.8153 acc_train: 0.2929 loss_val: 1.8035 acc_val: 0.3500 time: 0.0634s\n",
            "Epoch: 0121 loss_train: 1.8035 acc_train: 0.2929 loss_val: 1.8034 acc_val: 0.3500 time: 0.0620s\n",
            "Epoch: 0122 loss_train: 1.8007 acc_train: 0.2929 loss_val: 1.8031 acc_val: 0.3500 time: 0.0611s\n",
            "Epoch: 0123 loss_train: 1.8172 acc_train: 0.2929 loss_val: 1.8028 acc_val: 0.3500 time: 0.0732s\n",
            "Epoch: 0124 loss_train: 1.8158 acc_train: 0.2929 loss_val: 1.8025 acc_val: 0.3500 time: 0.0658s\n",
            "Epoch: 0125 loss_train: 1.8138 acc_train: 0.2929 loss_val: 1.8022 acc_val: 0.3500 time: 0.0665s\n",
            "Epoch: 0126 loss_train: 1.8217 acc_train: 0.2929 loss_val: 1.8021 acc_val: 0.3500 time: 0.0690s\n",
            "Epoch: 0127 loss_train: 1.8120 acc_train: 0.2929 loss_val: 1.8021 acc_val: 0.3500 time: 0.0700s\n",
            "Epoch: 0128 loss_train: 1.8102 acc_train: 0.2929 loss_val: 1.8019 acc_val: 0.3500 time: 0.0683s\n",
            "Epoch: 0129 loss_train: 1.8000 acc_train: 0.2929 loss_val: 1.8016 acc_val: 0.3500 time: 0.0692s\n",
            "Epoch: 0130 loss_train: 1.8238 acc_train: 0.2929 loss_val: 1.8015 acc_val: 0.3500 time: 0.0690s\n",
            "Epoch: 0131 loss_train: 1.8129 acc_train: 0.2929 loss_val: 1.8013 acc_val: 0.3500 time: 0.0718s\n",
            "Epoch: 0132 loss_train: 1.8120 acc_train: 0.2929 loss_val: 1.8012 acc_val: 0.3500 time: 0.0749s\n",
            "Epoch: 0133 loss_train: 1.8114 acc_train: 0.2929 loss_val: 1.8011 acc_val: 0.3500 time: 0.0701s\n",
            "Epoch: 0134 loss_train: 1.8252 acc_train: 0.2929 loss_val: 1.8011 acc_val: 0.3500 time: 0.0798s\n",
            "Epoch: 0135 loss_train: 1.8200 acc_train: 0.2929 loss_val: 1.8012 acc_val: 0.3500 time: 0.0747s\n",
            "Epoch: 0136 loss_train: 1.8094 acc_train: 0.2929 loss_val: 1.8012 acc_val: 0.3500 time: 0.0735s\n",
            "Epoch: 0137 loss_train: 1.8257 acc_train: 0.2929 loss_val: 1.8012 acc_val: 0.3500 time: 0.0778s\n",
            "Epoch: 0138 loss_train: 1.8127 acc_train: 0.2929 loss_val: 1.8013 acc_val: 0.3500 time: 0.0767s\n",
            "Epoch: 0139 loss_train: 1.8152 acc_train: 0.2929 loss_val: 1.8015 acc_val: 0.3500 time: 0.0760s\n",
            "Epoch: 0140 loss_train: 1.8168 acc_train: 0.2929 loss_val: 1.8018 acc_val: 0.3500 time: 0.0770s\n",
            "Epoch: 0141 loss_train: 1.8152 acc_train: 0.2929 loss_val: 1.8020 acc_val: 0.3500 time: 0.0781s\n",
            "Epoch: 0142 loss_train: 1.8247 acc_train: 0.2929 loss_val: 1.8024 acc_val: 0.3500 time: 0.0755s\n",
            "Epoch: 0143 loss_train: 1.8249 acc_train: 0.2929 loss_val: 1.8027 acc_val: 0.3500 time: 0.0653s\n",
            "Epoch: 0144 loss_train: 1.8276 acc_train: 0.2929 loss_val: 1.8031 acc_val: 0.3500 time: 0.0700s\n",
            "Epoch: 0145 loss_train: 1.8227 acc_train: 0.2857 loss_val: 1.8035 acc_val: 0.3500 time: 0.0671s\n",
            "Epoch: 0146 loss_train: 1.8241 acc_train: 0.2929 loss_val: 1.8040 acc_val: 0.3500 time: 0.0647s\n",
            "Epoch: 0147 loss_train: 1.8202 acc_train: 0.2929 loss_val: 1.8044 acc_val: 0.3500 time: 0.0580s\n",
            "Epoch: 0148 loss_train: 1.8178 acc_train: 0.2929 loss_val: 1.8047 acc_val: 0.3500 time: 0.0698s\n",
            "Epoch: 0149 loss_train: 1.8196 acc_train: 0.2929 loss_val: 1.8048 acc_val: 0.3500 time: 0.0648s\n",
            "Epoch: 0150 loss_train: 1.8215 acc_train: 0.2929 loss_val: 1.8051 acc_val: 0.3500 time: 0.0607s\n",
            "Epoch: 0151 loss_train: 1.7979 acc_train: 0.2929 loss_val: 1.8048 acc_val: 0.3500 time: 0.0657s\n",
            "Epoch: 0152 loss_train: 1.8268 acc_train: 0.2929 loss_val: 1.8046 acc_val: 0.3500 time: 0.0672s\n",
            "Epoch: 0153 loss_train: 1.8097 acc_train: 0.2929 loss_val: 1.8042 acc_val: 0.3500 time: 0.0647s\n",
            "Epoch: 0154 loss_train: 1.8241 acc_train: 0.2929 loss_val: 1.8037 acc_val: 0.3500 time: 0.0670s\n",
            "Epoch: 0155 loss_train: 1.8268 acc_train: 0.2929 loss_val: 1.8034 acc_val: 0.3500 time: 0.0730s\n",
            "Epoch: 0156 loss_train: 1.8023 acc_train: 0.2929 loss_val: 1.8028 acc_val: 0.3500 time: 0.0703s\n",
            "Epoch: 0157 loss_train: 1.8220 acc_train: 0.2929 loss_val: 1.8023 acc_val: 0.3500 time: 0.0715s\n",
            "Epoch: 0158 loss_train: 1.8079 acc_train: 0.2929 loss_val: 1.8017 acc_val: 0.3500 time: 0.0752s\n",
            "Epoch: 0159 loss_train: 1.8136 acc_train: 0.2929 loss_val: 1.8013 acc_val: 0.3500 time: 0.0747s\n",
            "Epoch: 0160 loss_train: 1.8288 acc_train: 0.2929 loss_val: 1.8011 acc_val: 0.3500 time: 0.0738s\n",
            "Epoch: 0161 loss_train: 1.8170 acc_train: 0.2929 loss_val: 1.8009 acc_val: 0.3500 time: 0.0697s\n",
            "Epoch: 0162 loss_train: 1.8117 acc_train: 0.2929 loss_val: 1.8008 acc_val: 0.3500 time: 0.0645s\n",
            "Epoch: 0163 loss_train: 1.8177 acc_train: 0.2929 loss_val: 1.8008 acc_val: 0.3500 time: 0.0672s\n",
            "Epoch: 0164 loss_train: 1.8187 acc_train: 0.2929 loss_val: 1.8009 acc_val: 0.3500 time: 0.0616s\n",
            "Epoch: 0165 loss_train: 1.8229 acc_train: 0.2929 loss_val: 1.8010 acc_val: 0.3500 time: 0.0712s\n",
            "Epoch: 0166 loss_train: 1.8001 acc_train: 0.2929 loss_val: 1.8010 acc_val: 0.3500 time: 0.0765s\n",
            "Epoch: 0167 loss_train: 1.8135 acc_train: 0.2929 loss_val: 1.8009 acc_val: 0.3500 time: 0.0682s\n",
            "Epoch: 0168 loss_train: 1.8202 acc_train: 0.2929 loss_val: 1.8010 acc_val: 0.3500 time: 0.0734s\n",
            "Epoch: 0169 loss_train: 1.8055 acc_train: 0.2929 loss_val: 1.8010 acc_val: 0.3500 time: 0.0713s\n",
            "Epoch: 0170 loss_train: 1.8210 acc_train: 0.2929 loss_val: 1.8011 acc_val: 0.3500 time: 0.0780s\n",
            "Epoch: 0171 loss_train: 1.8136 acc_train: 0.2929 loss_val: 1.8012 acc_val: 0.3500 time: 0.0744s\n",
            "Epoch: 0172 loss_train: 1.8071 acc_train: 0.2929 loss_val: 1.8013 acc_val: 0.3500 time: 0.0685s\n",
            "Epoch: 0173 loss_train: 1.8176 acc_train: 0.2929 loss_val: 1.8016 acc_val: 0.3500 time: 0.0699s\n",
            "Epoch: 0174 loss_train: 1.8245 acc_train: 0.2929 loss_val: 1.8020 acc_val: 0.3500 time: 0.0740s\n",
            "Epoch: 0175 loss_train: 1.8062 acc_train: 0.2929 loss_val: 1.8023 acc_val: 0.3500 time: 0.0689s\n",
            "Epoch: 0176 loss_train: 1.8109 acc_train: 0.2929 loss_val: 1.8026 acc_val: 0.3500 time: 0.0754s\n",
            "Epoch: 0177 loss_train: 1.8077 acc_train: 0.2929 loss_val: 1.8028 acc_val: 0.3500 time: 0.0698s\n",
            "Epoch: 0178 loss_train: 1.8008 acc_train: 0.2929 loss_val: 1.8028 acc_val: 0.3500 time: 0.0665s\n",
            "Epoch: 0179 loss_train: 1.8186 acc_train: 0.2929 loss_val: 1.8028 acc_val: 0.3500 time: 0.0769s\n",
            "Epoch: 0180 loss_train: 1.8234 acc_train: 0.2929 loss_val: 1.8029 acc_val: 0.3500 time: 0.0730s\n",
            "Epoch: 0181 loss_train: 1.8341 acc_train: 0.2929 loss_val: 1.8031 acc_val: 0.3500 time: 0.0718s\n",
            "Epoch: 0182 loss_train: 1.8131 acc_train: 0.2929 loss_val: 1.8033 acc_val: 0.3500 time: 0.0686s\n",
            "Epoch: 0183 loss_train: 1.8219 acc_train: 0.2929 loss_val: 1.8035 acc_val: 0.3500 time: 0.0665s\n",
            "Epoch: 0184 loss_train: 1.8218 acc_train: 0.2929 loss_val: 1.8037 acc_val: 0.3500 time: 0.0637s\n",
            "Epoch: 0185 loss_train: 1.8010 acc_train: 0.2929 loss_val: 1.8037 acc_val: 0.3500 time: 0.0688s\n",
            "Epoch: 0186 loss_train: 1.8035 acc_train: 0.2929 loss_val: 1.8037 acc_val: 0.3500 time: 0.0602s\n",
            "Epoch: 0187 loss_train: 1.8169 acc_train: 0.2929 loss_val: 1.8038 acc_val: 0.3500 time: 0.0651s\n",
            "Epoch: 0188 loss_train: 1.8293 acc_train: 0.2929 loss_val: 1.8039 acc_val: 0.3500 time: 0.0605s\n",
            "Epoch: 0189 loss_train: 1.8156 acc_train: 0.2929 loss_val: 1.8038 acc_val: 0.3500 time: 0.0620s\n",
            "Epoch: 0190 loss_train: 1.8242 acc_train: 0.2929 loss_val: 1.8036 acc_val: 0.3500 time: 0.0620s\n",
            "Epoch: 0191 loss_train: 1.8093 acc_train: 0.2929 loss_val: 1.8033 acc_val: 0.3500 time: 0.0689s\n",
            "Epoch: 0192 loss_train: 1.8139 acc_train: 0.2929 loss_val: 1.8029 acc_val: 0.3500 time: 0.0630s\n",
            "Epoch: 0193 loss_train: 1.8235 acc_train: 0.2929 loss_val: 1.8026 acc_val: 0.3500 time: 0.0635s\n",
            "Epoch: 0194 loss_train: 1.8179 acc_train: 0.2929 loss_val: 1.8024 acc_val: 0.3500 time: 0.0585s\n",
            "Epoch: 0195 loss_train: 1.8173 acc_train: 0.2929 loss_val: 1.8022 acc_val: 0.3500 time: 0.0726s\n",
            "Epoch: 0196 loss_train: 1.8158 acc_train: 0.2929 loss_val: 1.8020 acc_val: 0.3500 time: 0.0653s\n",
            "Epoch: 0197 loss_train: 1.8133 acc_train: 0.2929 loss_val: 1.8018 acc_val: 0.3500 time: 0.0652s\n",
            "Epoch: 0198 loss_train: 1.8153 acc_train: 0.2929 loss_val: 1.8017 acc_val: 0.3500 time: 0.0595s\n",
            "Epoch: 0199 loss_train: 1.8153 acc_train: 0.2929 loss_val: 1.8018 acc_val: 0.3500 time: 0.0652s\n",
            "Epoch: 0200 loss_train: 1.8062 acc_train: 0.2929 loss_val: 1.8017 acc_val: 0.3500 time: 0.0642s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 11.3254s\n",
            "Test set results: loss= 1.8870 accuracy= 0.3090\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3uGDer-G53R",
        "outputId": "0ee8ebcf-4578-424b-9336-350635534aa6"
      },
      "source": [
        "#(1.6) Based on DeepGCN, replace GC layer with residual learning layer. Compare the performance with (1.5)\n",
        "#  def forward(self, x, adj):\n",
        "#         x = F.relu(self.gc1(x, adj))\n",
        "#         i = x\n",
        "#         x = F.dropout(x, self.dropout, training=self.training)\n",
        "#         x = i+x\n",
        "#         x = self.gc2(x, adj)\n",
        "\n",
        "#         return F.log_softmax(x, dim=1)\n",
        "%run pygcn/train.py\n",
        "#Observation: Accuracy marely increases using residual layer. Doesn't help oversmoothing problem. Although one residual layer is better that 1 GCN middle layer."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cora dataset...\n",
            "Epoch: 0001 loss_train: 1.9707 acc_train: 0.0857 loss_val: 1.9606 acc_val: 0.1633 time: 0.1165s\n",
            "Epoch: 0002 loss_train: 1.9418 acc_train: 0.1929 loss_val: 1.9387 acc_val: 0.1567 time: 0.0109s\n",
            "Epoch: 0003 loss_train: 1.9252 acc_train: 0.2000 loss_val: 1.9184 acc_val: 0.1567 time: 0.0117s\n",
            "Epoch: 0004 loss_train: 1.9111 acc_train: 0.2000 loss_val: 1.8991 acc_val: 0.1567 time: 0.0104s\n",
            "Epoch: 0005 loss_train: 1.8928 acc_train: 0.2071 loss_val: 1.8804 acc_val: 0.1567 time: 0.0130s\n",
            "Epoch: 0006 loss_train: 1.8556 acc_train: 0.2286 loss_val: 1.8626 acc_val: 0.1600 time: 0.0108s\n",
            "Epoch: 0007 loss_train: 1.8295 acc_train: 0.2714 loss_val: 1.8456 acc_val: 0.1700 time: 0.0093s\n",
            "Epoch: 0008 loss_train: 1.8265 acc_train: 0.3357 loss_val: 1.8296 acc_val: 0.3800 time: 0.0109s\n",
            "Epoch: 0009 loss_train: 1.8087 acc_train: 0.3929 loss_val: 1.8143 acc_val: 0.4367 time: 0.0102s\n",
            "Epoch: 0010 loss_train: 1.7998 acc_train: 0.3929 loss_val: 1.7998 acc_val: 0.4433 time: 0.0084s\n",
            "Epoch: 0011 loss_train: 1.7858 acc_train: 0.4214 loss_val: 1.7859 acc_val: 0.4433 time: 0.0107s\n",
            "Epoch: 0012 loss_train: 1.7593 acc_train: 0.4214 loss_val: 1.7722 acc_val: 0.3967 time: 0.0097s\n",
            "Epoch: 0013 loss_train: 1.7577 acc_train: 0.4071 loss_val: 1.7585 acc_val: 0.3700 time: 0.0079s\n",
            "Epoch: 0014 loss_train: 1.7401 acc_train: 0.4000 loss_val: 1.7451 acc_val: 0.3700 time: 0.0109s\n",
            "Epoch: 0015 loss_train: 1.7303 acc_train: 0.3857 loss_val: 1.7321 acc_val: 0.3633 time: 0.0108s\n",
            "Epoch: 0016 loss_train: 1.7267 acc_train: 0.3500 loss_val: 1.7202 acc_val: 0.3567 time: 0.0103s\n",
            "Epoch: 0017 loss_train: 1.7058 acc_train: 0.3786 loss_val: 1.7091 acc_val: 0.3533 time: 0.0110s\n",
            "Epoch: 0018 loss_train: 1.6659 acc_train: 0.3786 loss_val: 1.6980 acc_val: 0.3533 time: 0.0121s\n",
            "Epoch: 0019 loss_train: 1.6631 acc_train: 0.3357 loss_val: 1.6870 acc_val: 0.3533 time: 0.0170s\n",
            "Epoch: 0020 loss_train: 1.6444 acc_train: 0.3786 loss_val: 1.6759 acc_val: 0.3533 time: 0.0111s\n",
            "Epoch: 0021 loss_train: 1.6280 acc_train: 0.3429 loss_val: 1.6645 acc_val: 0.3500 time: 0.0110s\n",
            "Epoch: 0022 loss_train: 1.6202 acc_train: 0.3500 loss_val: 1.6527 acc_val: 0.3500 time: 0.0109s\n",
            "Epoch: 0023 loss_train: 1.5994 acc_train: 0.3357 loss_val: 1.6406 acc_val: 0.3600 time: 0.0105s\n",
            "Epoch: 0024 loss_train: 1.5869 acc_train: 0.3643 loss_val: 1.6282 acc_val: 0.3633 time: 0.0101s\n",
            "Epoch: 0025 loss_train: 1.5700 acc_train: 0.4143 loss_val: 1.6154 acc_val: 0.3667 time: 0.0110s\n",
            "Epoch: 0026 loss_train: 1.5165 acc_train: 0.4357 loss_val: 1.6020 acc_val: 0.3667 time: 0.0110s\n",
            "Epoch: 0027 loss_train: 1.5279 acc_train: 0.4071 loss_val: 1.5878 acc_val: 0.3800 time: 0.0112s\n",
            "Epoch: 0028 loss_train: 1.5110 acc_train: 0.4786 loss_val: 1.5729 acc_val: 0.4033 time: 0.0122s\n",
            "Epoch: 0029 loss_train: 1.4681 acc_train: 0.5143 loss_val: 1.5572 acc_val: 0.4300 time: 0.0108s\n",
            "Epoch: 0030 loss_train: 1.4434 acc_train: 0.5000 loss_val: 1.5409 acc_val: 0.4533 time: 0.0110s\n",
            "Epoch: 0031 loss_train: 1.4341 acc_train: 0.5286 loss_val: 1.5238 acc_val: 0.5100 time: 0.0109s\n",
            "Epoch: 0032 loss_train: 1.4010 acc_train: 0.5429 loss_val: 1.5060 acc_val: 0.5267 time: 0.0116s\n",
            "Epoch: 0033 loss_train: 1.3877 acc_train: 0.5429 loss_val: 1.4875 acc_val: 0.5567 time: 0.0108s\n",
            "Epoch: 0034 loss_train: 1.3682 acc_train: 0.6071 loss_val: 1.4687 acc_val: 0.5667 time: 0.0101s\n",
            "Epoch: 0035 loss_train: 1.3230 acc_train: 0.6286 loss_val: 1.4492 acc_val: 0.5900 time: 0.0114s\n",
            "Epoch: 0036 loss_train: 1.3367 acc_train: 0.6500 loss_val: 1.4293 acc_val: 0.6033 time: 0.0111s\n",
            "Epoch: 0037 loss_train: 1.2792 acc_train: 0.6571 loss_val: 1.4091 acc_val: 0.6067 time: 0.0150s\n",
            "Epoch: 0038 loss_train: 1.2648 acc_train: 0.6857 loss_val: 1.3885 acc_val: 0.6233 time: 0.0109s\n",
            "Epoch: 0039 loss_train: 1.2226 acc_train: 0.7071 loss_val: 1.3671 acc_val: 0.6433 time: 0.0096s\n",
            "Epoch: 0040 loss_train: 1.1987 acc_train: 0.7000 loss_val: 1.3446 acc_val: 0.6433 time: 0.0114s\n",
            "Epoch: 0041 loss_train: 1.1592 acc_train: 0.7214 loss_val: 1.3222 acc_val: 0.6500 time: 0.0133s\n",
            "Epoch: 0042 loss_train: 1.1180 acc_train: 0.7143 loss_val: 1.2995 acc_val: 0.6500 time: 0.0118s\n",
            "Epoch: 0043 loss_train: 1.1150 acc_train: 0.6857 loss_val: 1.2773 acc_val: 0.6500 time: 0.0119s\n",
            "Epoch: 0044 loss_train: 1.0845 acc_train: 0.6786 loss_val: 1.2551 acc_val: 0.6567 time: 0.0111s\n",
            "Epoch: 0045 loss_train: 1.0338 acc_train: 0.7214 loss_val: 1.2332 acc_val: 0.6633 time: 0.0109s\n",
            "Epoch: 0046 loss_train: 1.0226 acc_train: 0.7214 loss_val: 1.2116 acc_val: 0.6733 time: 0.0097s\n",
            "Epoch: 0047 loss_train: 1.0009 acc_train: 0.7429 loss_val: 1.1898 acc_val: 0.6900 time: 0.0157s\n",
            "Epoch: 0048 loss_train: 0.9570 acc_train: 0.7357 loss_val: 1.1684 acc_val: 0.6967 time: 0.0101s\n",
            "Epoch: 0049 loss_train: 0.9404 acc_train: 0.7429 loss_val: 1.1473 acc_val: 0.7033 time: 0.0109s\n",
            "Epoch: 0050 loss_train: 0.9095 acc_train: 0.7857 loss_val: 1.1267 acc_val: 0.7067 time: 0.0114s\n",
            "Epoch: 0051 loss_train: 0.8979 acc_train: 0.7714 loss_val: 1.1067 acc_val: 0.7100 time: 0.0108s\n",
            "Epoch: 0052 loss_train: 0.8841 acc_train: 0.7714 loss_val: 1.0874 acc_val: 0.7167 time: 0.0109s\n",
            "Epoch: 0053 loss_train: 0.8269 acc_train: 0.8143 loss_val: 1.0688 acc_val: 0.7233 time: 0.0111s\n",
            "Epoch: 0054 loss_train: 0.8144 acc_train: 0.8143 loss_val: 1.0508 acc_val: 0.7300 time: 0.0167s\n",
            "Epoch: 0055 loss_train: 0.7930 acc_train: 0.8429 loss_val: 1.0326 acc_val: 0.7433 time: 0.0115s\n",
            "Epoch: 0056 loss_train: 0.7699 acc_train: 0.8571 loss_val: 1.0138 acc_val: 0.7567 time: 0.0112s\n",
            "Epoch: 0057 loss_train: 0.7535 acc_train: 0.8571 loss_val: 0.9949 acc_val: 0.7567 time: 0.0110s\n",
            "Epoch: 0058 loss_train: 0.7353 acc_train: 0.8429 loss_val: 0.9766 acc_val: 0.7567 time: 0.0097s\n",
            "Epoch: 0059 loss_train: 0.7008 acc_train: 0.8786 loss_val: 0.9591 acc_val: 0.7600 time: 0.0109s\n",
            "Epoch: 0060 loss_train: 0.6819 acc_train: 0.9000 loss_val: 0.9427 acc_val: 0.7700 time: 0.0108s\n",
            "Epoch: 0061 loss_train: 0.6785 acc_train: 0.8714 loss_val: 0.9274 acc_val: 0.7867 time: 0.0108s\n",
            "Epoch: 0062 loss_train: 0.6804 acc_train: 0.8929 loss_val: 0.9132 acc_val: 0.7900 time: 0.0108s\n",
            "Epoch: 0063 loss_train: 0.6442 acc_train: 0.8929 loss_val: 0.8997 acc_val: 0.7933 time: 0.0104s\n",
            "Epoch: 0064 loss_train: 0.6410 acc_train: 0.9000 loss_val: 0.8870 acc_val: 0.7933 time: 0.0108s\n",
            "Epoch: 0065 loss_train: 0.5925 acc_train: 0.9071 loss_val: 0.8745 acc_val: 0.7967 time: 0.0112s\n",
            "Epoch: 0066 loss_train: 0.5934 acc_train: 0.8929 loss_val: 0.8625 acc_val: 0.8033 time: 0.0106s\n",
            "Epoch: 0067 loss_train: 0.5702 acc_train: 0.9000 loss_val: 0.8514 acc_val: 0.8033 time: 0.0099s\n",
            "Epoch: 0068 loss_train: 0.5476 acc_train: 0.9286 loss_val: 0.8413 acc_val: 0.8033 time: 0.0103s\n",
            "Epoch: 0069 loss_train: 0.5565 acc_train: 0.9143 loss_val: 0.8316 acc_val: 0.8033 time: 0.0077s\n",
            "Epoch: 0070 loss_train: 0.5126 acc_train: 0.9500 loss_val: 0.8229 acc_val: 0.8100 time: 0.0112s\n",
            "Epoch: 0071 loss_train: 0.4931 acc_train: 0.9286 loss_val: 0.8139 acc_val: 0.8067 time: 0.0106s\n",
            "Epoch: 0072 loss_train: 0.5163 acc_train: 0.9286 loss_val: 0.8041 acc_val: 0.8100 time: 0.0146s\n",
            "Epoch: 0073 loss_train: 0.4869 acc_train: 0.9357 loss_val: 0.7946 acc_val: 0.8133 time: 0.0096s\n",
            "Epoch: 0074 loss_train: 0.4659 acc_train: 0.9571 loss_val: 0.7845 acc_val: 0.8200 time: 0.0106s\n",
            "Epoch: 0075 loss_train: 0.4469 acc_train: 0.9714 loss_val: 0.7752 acc_val: 0.8333 time: 0.0112s\n",
            "Epoch: 0076 loss_train: 0.4724 acc_train: 0.9500 loss_val: 0.7670 acc_val: 0.8333 time: 0.0111s\n",
            "Epoch: 0077 loss_train: 0.4426 acc_train: 0.9357 loss_val: 0.7599 acc_val: 0.8333 time: 0.0103s\n",
            "Epoch: 0078 loss_train: 0.4188 acc_train: 0.9571 loss_val: 0.7538 acc_val: 0.8400 time: 0.0237s\n",
            "Epoch: 0079 loss_train: 0.4509 acc_train: 0.9500 loss_val: 0.7491 acc_val: 0.8433 time: 0.0109s\n",
            "Epoch: 0080 loss_train: 0.4012 acc_train: 0.9643 loss_val: 0.7452 acc_val: 0.8333 time: 0.0177s\n",
            "Epoch: 0081 loss_train: 0.4274 acc_train: 0.9286 loss_val: 0.7417 acc_val: 0.8267 time: 0.0107s\n",
            "Epoch: 0082 loss_train: 0.3964 acc_train: 0.9571 loss_val: 0.7375 acc_val: 0.8233 time: 0.0097s\n",
            "Epoch: 0083 loss_train: 0.4011 acc_train: 0.9500 loss_val: 0.7328 acc_val: 0.8233 time: 0.0100s\n",
            "Epoch: 0084 loss_train: 0.4157 acc_train: 0.9429 loss_val: 0.7276 acc_val: 0.8233 time: 0.0109s\n",
            "Epoch: 0085 loss_train: 0.3971 acc_train: 0.9571 loss_val: 0.7219 acc_val: 0.8300 time: 0.0105s\n",
            "Epoch: 0086 loss_train: 0.3934 acc_train: 0.9357 loss_val: 0.7160 acc_val: 0.8300 time: 0.0107s\n",
            "Epoch: 0087 loss_train: 0.4039 acc_train: 0.9500 loss_val: 0.7110 acc_val: 0.8333 time: 0.0109s\n",
            "Epoch: 0088 loss_train: 0.3822 acc_train: 0.9857 loss_val: 0.7071 acc_val: 0.8367 time: 0.0142s\n",
            "Epoch: 0089 loss_train: 0.3727 acc_train: 0.9643 loss_val: 0.7033 acc_val: 0.8400 time: 0.0110s\n",
            "Epoch: 0090 loss_train: 0.3747 acc_train: 0.9500 loss_val: 0.7001 acc_val: 0.8367 time: 0.0110s\n",
            "Epoch: 0091 loss_train: 0.3368 acc_train: 0.9714 loss_val: 0.6969 acc_val: 0.8267 time: 0.0099s\n",
            "Epoch: 0092 loss_train: 0.3362 acc_train: 0.9643 loss_val: 0.6935 acc_val: 0.8300 time: 0.0111s\n",
            "Epoch: 0093 loss_train: 0.3345 acc_train: 0.9714 loss_val: 0.6912 acc_val: 0.8233 time: 0.0102s\n",
            "Epoch: 0094 loss_train: 0.3401 acc_train: 0.9643 loss_val: 0.6905 acc_val: 0.8200 time: 0.0108s\n",
            "Epoch: 0095 loss_train: 0.3658 acc_train: 0.9571 loss_val: 0.6898 acc_val: 0.8167 time: 0.0111s\n",
            "Epoch: 0096 loss_train: 0.3540 acc_train: 0.9571 loss_val: 0.6882 acc_val: 0.8133 time: 0.0113s\n",
            "Epoch: 0097 loss_train: 0.3116 acc_train: 0.9857 loss_val: 0.6859 acc_val: 0.8100 time: 0.0099s\n",
            "Epoch: 0098 loss_train: 0.3379 acc_train: 0.9714 loss_val: 0.6832 acc_val: 0.8100 time: 0.0107s\n",
            "Epoch: 0099 loss_train: 0.3286 acc_train: 0.9786 loss_val: 0.6814 acc_val: 0.8133 time: 0.0107s\n",
            "Epoch: 0100 loss_train: 0.3151 acc_train: 0.9714 loss_val: 0.6799 acc_val: 0.8233 time: 0.0108s\n",
            "Epoch: 0101 loss_train: 0.3297 acc_train: 0.9643 loss_val: 0.6774 acc_val: 0.8233 time: 0.0104s\n",
            "Epoch: 0102 loss_train: 0.2933 acc_train: 0.9714 loss_val: 0.6758 acc_val: 0.8267 time: 0.0110s\n",
            "Epoch: 0103 loss_train: 0.3251 acc_train: 0.9643 loss_val: 0.6736 acc_val: 0.8167 time: 0.0112s\n",
            "Epoch: 0104 loss_train: 0.3108 acc_train: 0.9714 loss_val: 0.6727 acc_val: 0.8167 time: 0.0110s\n",
            "Epoch: 0105 loss_train: 0.3019 acc_train: 0.9857 loss_val: 0.6706 acc_val: 0.8167 time: 0.0113s\n",
            "Epoch: 0106 loss_train: 0.2984 acc_train: 0.9857 loss_val: 0.6704 acc_val: 0.8167 time: 0.0150s\n",
            "Epoch: 0107 loss_train: 0.3086 acc_train: 0.9571 loss_val: 0.6692 acc_val: 0.8133 time: 0.0104s\n",
            "Epoch: 0108 loss_train: 0.2668 acc_train: 0.9857 loss_val: 0.6662 acc_val: 0.8133 time: 0.0115s\n",
            "Epoch: 0109 loss_train: 0.2680 acc_train: 0.9857 loss_val: 0.6642 acc_val: 0.8133 time: 0.0108s\n",
            "Epoch: 0110 loss_train: 0.2929 acc_train: 0.9786 loss_val: 0.6629 acc_val: 0.8100 time: 0.0123s\n",
            "Epoch: 0111 loss_train: 0.2797 acc_train: 0.9643 loss_val: 0.6602 acc_val: 0.8133 time: 0.0111s\n",
            "Epoch: 0112 loss_train: 0.2676 acc_train: 0.9857 loss_val: 0.6589 acc_val: 0.8133 time: 0.0138s\n",
            "Epoch: 0113 loss_train: 0.2778 acc_train: 0.9857 loss_val: 0.6591 acc_val: 0.8167 time: 0.0113s\n",
            "Epoch: 0114 loss_train: 0.2860 acc_train: 0.9857 loss_val: 0.6596 acc_val: 0.8200 time: 0.0137s\n",
            "Epoch: 0115 loss_train: 0.2895 acc_train: 0.9714 loss_val: 0.6611 acc_val: 0.8233 time: 0.0102s\n",
            "Epoch: 0116 loss_train: 0.2666 acc_train: 0.9786 loss_val: 0.6623 acc_val: 0.8200 time: 0.0100s\n",
            "Epoch: 0117 loss_train: 0.2800 acc_train: 0.9786 loss_val: 0.6612 acc_val: 0.8233 time: 0.0094s\n",
            "Epoch: 0118 loss_train: 0.2670 acc_train: 0.9929 loss_val: 0.6589 acc_val: 0.8233 time: 0.0109s\n",
            "Epoch: 0119 loss_train: 0.2910 acc_train: 0.9714 loss_val: 0.6539 acc_val: 0.8200 time: 0.0094s\n",
            "Epoch: 0120 loss_train: 0.2678 acc_train: 0.9929 loss_val: 0.6482 acc_val: 0.8200 time: 0.0110s\n",
            "Epoch: 0121 loss_train: 0.2678 acc_train: 0.9857 loss_val: 0.6434 acc_val: 0.8233 time: 0.0106s\n",
            "Epoch: 0122 loss_train: 0.2425 acc_train: 0.9786 loss_val: 0.6405 acc_val: 0.8233 time: 0.0111s\n",
            "Epoch: 0123 loss_train: 0.2555 acc_train: 0.9857 loss_val: 0.6379 acc_val: 0.8167 time: 0.0124s\n",
            "Epoch: 0124 loss_train: 0.2601 acc_train: 0.9714 loss_val: 0.6372 acc_val: 0.8167 time: 0.0134s\n",
            "Epoch: 0125 loss_train: 0.2378 acc_train: 0.9929 loss_val: 0.6378 acc_val: 0.8200 time: 0.0092s\n",
            "Epoch: 0126 loss_train: 0.2419 acc_train: 0.9929 loss_val: 0.6388 acc_val: 0.8200 time: 0.0112s\n",
            "Epoch: 0127 loss_train: 0.2440 acc_train: 0.9786 loss_val: 0.6404 acc_val: 0.8133 time: 0.0108s\n",
            "Epoch: 0128 loss_train: 0.2403 acc_train: 0.9857 loss_val: 0.6404 acc_val: 0.8167 time: 0.0080s\n",
            "Epoch: 0129 loss_train: 0.2270 acc_train: 0.9857 loss_val: 0.6381 acc_val: 0.8200 time: 0.0103s\n",
            "Epoch: 0130 loss_train: 0.2384 acc_train: 0.9857 loss_val: 0.6335 acc_val: 0.8200 time: 0.0091s\n",
            "Epoch: 0131 loss_train: 0.2422 acc_train: 0.9643 loss_val: 0.6299 acc_val: 0.8267 time: 0.0087s\n",
            "Epoch: 0132 loss_train: 0.2362 acc_train: 0.9786 loss_val: 0.6276 acc_val: 0.8300 time: 0.0112s\n",
            "Epoch: 0133 loss_train: 0.2503 acc_train: 0.9643 loss_val: 0.6270 acc_val: 0.8300 time: 0.0109s\n",
            "Epoch: 0134 loss_train: 0.2397 acc_train: 0.9786 loss_val: 0.6278 acc_val: 0.8333 time: 0.0103s\n",
            "Epoch: 0135 loss_train: 0.2269 acc_train: 0.9929 loss_val: 0.6302 acc_val: 0.8300 time: 0.0110s\n",
            "Epoch: 0136 loss_train: 0.2450 acc_train: 0.9714 loss_val: 0.6328 acc_val: 0.8233 time: 0.0103s\n",
            "Epoch: 0137 loss_train: 0.2101 acc_train: 0.9929 loss_val: 0.6347 acc_val: 0.8233 time: 0.0094s\n",
            "Epoch: 0138 loss_train: 0.2385 acc_train: 0.9857 loss_val: 0.6372 acc_val: 0.8233 time: 0.0113s\n",
            "Epoch: 0139 loss_train: 0.2178 acc_train: 0.9929 loss_val: 0.6370 acc_val: 0.8200 time: 0.0116s\n",
            "Epoch: 0140 loss_train: 0.2169 acc_train: 0.9857 loss_val: 0.6356 acc_val: 0.8200 time: 0.0104s\n",
            "Epoch: 0141 loss_train: 0.2018 acc_train: 0.9857 loss_val: 0.6325 acc_val: 0.8200 time: 0.0092s\n",
            "Epoch: 0142 loss_train: 0.2270 acc_train: 0.9857 loss_val: 0.6286 acc_val: 0.8200 time: 0.0137s\n",
            "Epoch: 0143 loss_train: 0.2187 acc_train: 1.0000 loss_val: 0.6240 acc_val: 0.8200 time: 0.0109s\n",
            "Epoch: 0144 loss_train: 0.2571 acc_train: 0.9643 loss_val: 0.6223 acc_val: 0.8200 time: 0.0110s\n",
            "Epoch: 0145 loss_train: 0.2357 acc_train: 0.9857 loss_val: 0.6225 acc_val: 0.8200 time: 0.0109s\n",
            "Epoch: 0146 loss_train: 0.2216 acc_train: 0.9929 loss_val: 0.6248 acc_val: 0.8233 time: 0.0084s\n",
            "Epoch: 0147 loss_train: 0.2133 acc_train: 0.9857 loss_val: 0.6268 acc_val: 0.8200 time: 0.0111s\n",
            "Epoch: 0148 loss_train: 0.1991 acc_train: 0.9786 loss_val: 0.6286 acc_val: 0.8200 time: 0.0110s\n",
            "Epoch: 0149 loss_train: 0.2266 acc_train: 0.9714 loss_val: 0.6307 acc_val: 0.8200 time: 0.0105s\n",
            "Epoch: 0150 loss_train: 0.2072 acc_train: 0.9929 loss_val: 0.6324 acc_val: 0.8233 time: 0.0110s\n",
            "Epoch: 0151 loss_train: 0.1974 acc_train: 0.9857 loss_val: 0.6324 acc_val: 0.8133 time: 0.0109s\n",
            "Epoch: 0152 loss_train: 0.2112 acc_train: 0.9857 loss_val: 0.6296 acc_val: 0.8167 time: 0.0108s\n",
            "Epoch: 0153 loss_train: 0.2240 acc_train: 0.9857 loss_val: 0.6290 acc_val: 0.8200 time: 0.0107s\n",
            "Epoch: 0154 loss_train: 0.2124 acc_train: 0.9929 loss_val: 0.6296 acc_val: 0.8200 time: 0.0083s\n",
            "Epoch: 0155 loss_train: 0.2069 acc_train: 0.9857 loss_val: 0.6300 acc_val: 0.8167 time: 0.0118s\n",
            "Epoch: 0156 loss_train: 0.1895 acc_train: 1.0000 loss_val: 0.6290 acc_val: 0.8167 time: 0.0105s\n",
            "Epoch: 0157 loss_train: 0.2073 acc_train: 0.9857 loss_val: 0.6264 acc_val: 0.8200 time: 0.0112s\n",
            "Epoch: 0158 loss_train: 0.2072 acc_train: 0.9786 loss_val: 0.6253 acc_val: 0.8167 time: 0.0100s\n",
            "Epoch: 0159 loss_train: 0.2214 acc_train: 0.9786 loss_val: 0.6208 acc_val: 0.8200 time: 0.0109s\n",
            "Epoch: 0160 loss_train: 0.2220 acc_train: 0.9714 loss_val: 0.6185 acc_val: 0.8200 time: 0.0150s\n",
            "Epoch: 0161 loss_train: 0.1927 acc_train: 0.9929 loss_val: 0.6179 acc_val: 0.8200 time: 0.0105s\n",
            "Epoch: 0162 loss_train: 0.1923 acc_train: 0.9857 loss_val: 0.6173 acc_val: 0.8167 time: 0.0106s\n",
            "Epoch: 0163 loss_train: 0.1793 acc_train: 1.0000 loss_val: 0.6166 acc_val: 0.8167 time: 0.0161s\n",
            "Epoch: 0164 loss_train: 0.1959 acc_train: 0.9929 loss_val: 0.6170 acc_val: 0.8200 time: 0.0114s\n",
            "Epoch: 0165 loss_train: 0.2042 acc_train: 0.9786 loss_val: 0.6161 acc_val: 0.8267 time: 0.0109s\n",
            "Epoch: 0166 loss_train: 0.1832 acc_train: 0.9786 loss_val: 0.6168 acc_val: 0.8267 time: 0.0084s\n",
            "Epoch: 0167 loss_train: 0.1945 acc_train: 0.9857 loss_val: 0.6166 acc_val: 0.8267 time: 0.0111s\n",
            "Epoch: 0168 loss_train: 0.1830 acc_train: 1.0000 loss_val: 0.6168 acc_val: 0.8267 time: 0.0108s\n",
            "Epoch: 0169 loss_train: 0.1956 acc_train: 0.9929 loss_val: 0.6174 acc_val: 0.8267 time: 0.0110s\n",
            "Epoch: 0170 loss_train: 0.1993 acc_train: 0.9786 loss_val: 0.6180 acc_val: 0.8233 time: 0.0083s\n",
            "Epoch: 0171 loss_train: 0.1947 acc_train: 0.9786 loss_val: 0.6194 acc_val: 0.8167 time: 0.0110s\n",
            "Epoch: 0172 loss_train: 0.1797 acc_train: 0.9929 loss_val: 0.6205 acc_val: 0.8133 time: 0.0100s\n",
            "Epoch: 0173 loss_train: 0.1884 acc_train: 0.9857 loss_val: 0.6195 acc_val: 0.8133 time: 0.0117s\n",
            "Epoch: 0174 loss_train: 0.1931 acc_train: 0.9929 loss_val: 0.6188 acc_val: 0.8133 time: 0.0094s\n",
            "Epoch: 0175 loss_train: 0.1845 acc_train: 0.9714 loss_val: 0.6175 acc_val: 0.8167 time: 0.0109s\n",
            "Epoch: 0176 loss_train: 0.1875 acc_train: 0.9929 loss_val: 0.6185 acc_val: 0.8167 time: 0.0110s\n",
            "Epoch: 0177 loss_train: 0.1754 acc_train: 0.9929 loss_val: 0.6189 acc_val: 0.8167 time: 0.0112s\n",
            "Epoch: 0178 loss_train: 0.1793 acc_train: 0.9929 loss_val: 0.6199 acc_val: 0.8133 time: 0.0137s\n",
            "Epoch: 0179 loss_train: 0.1992 acc_train: 0.9929 loss_val: 0.6196 acc_val: 0.8133 time: 0.0111s\n",
            "Epoch: 0180 loss_train: 0.1795 acc_train: 0.9857 loss_val: 0.6204 acc_val: 0.8133 time: 0.0097s\n",
            "Epoch: 0181 loss_train: 0.1725 acc_train: 0.9786 loss_val: 0.6220 acc_val: 0.8133 time: 0.0109s\n",
            "Epoch: 0182 loss_train: 0.1998 acc_train: 0.9786 loss_val: 0.6243 acc_val: 0.8133 time: 0.0147s\n",
            "Epoch: 0183 loss_train: 0.1727 acc_train: 0.9857 loss_val: 0.6256 acc_val: 0.8167 time: 0.0120s\n",
            "Epoch: 0184 loss_train: 0.1852 acc_train: 0.9786 loss_val: 0.6235 acc_val: 0.8200 time: 0.0101s\n",
            "Epoch: 0185 loss_train: 0.1871 acc_train: 0.9857 loss_val: 0.6218 acc_val: 0.8200 time: 0.0115s\n",
            "Epoch: 0186 loss_train: 0.1783 acc_train: 0.9786 loss_val: 0.6217 acc_val: 0.8133 time: 0.0116s\n",
            "Epoch: 0187 loss_train: 0.2003 acc_train: 0.9786 loss_val: 0.6240 acc_val: 0.8133 time: 0.0109s\n",
            "Epoch: 0188 loss_train: 0.1707 acc_train: 0.9929 loss_val: 0.6259 acc_val: 0.8167 time: 0.0079s\n",
            "Epoch: 0189 loss_train: 0.1806 acc_train: 0.9857 loss_val: 0.6264 acc_val: 0.8167 time: 0.0109s\n",
            "Epoch: 0190 loss_train: 0.1771 acc_train: 0.9929 loss_val: 0.6251 acc_val: 0.8167 time: 0.0100s\n",
            "Epoch: 0191 loss_train: 0.1801 acc_train: 0.9929 loss_val: 0.6209 acc_val: 0.8167 time: 0.0144s\n",
            "Epoch: 0192 loss_train: 0.1823 acc_train: 0.9929 loss_val: 0.6173 acc_val: 0.8133 time: 0.0113s\n",
            "Epoch: 0193 loss_train: 0.1853 acc_train: 0.9714 loss_val: 0.6169 acc_val: 0.8167 time: 0.0111s\n",
            "Epoch: 0194 loss_train: 0.1623 acc_train: 1.0000 loss_val: 0.6150 acc_val: 0.8167 time: 0.0081s\n",
            "Epoch: 0195 loss_train: 0.1743 acc_train: 0.9929 loss_val: 0.6128 acc_val: 0.8233 time: 0.0124s\n",
            "Epoch: 0196 loss_train: 0.1662 acc_train: 0.9857 loss_val: 0.6143 acc_val: 0.8267 time: 0.0131s\n",
            "Epoch: 0197 loss_train: 0.1566 acc_train: 1.0000 loss_val: 0.6156 acc_val: 0.8267 time: 0.0099s\n",
            "Epoch: 0198 loss_train: 0.1874 acc_train: 0.9786 loss_val: 0.6161 acc_val: 0.8200 time: 0.0107s\n",
            "Epoch: 0199 loss_train: 0.1590 acc_train: 0.9929 loss_val: 0.6161 acc_val: 0.8167 time: 0.0126s\n",
            "Epoch: 0200 loss_train: 0.1658 acc_train: 0.9714 loss_val: 0.6166 acc_val: 0.8100 time: 0.0114s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 2.4936s\n",
            "Test set results: loss= 0.5976 accuracy= 0.8330\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRUeqU_kNZMF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}